{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "44350it [19:55, 37.09it/s]\n",
      "11088it [06:03, 30.53it/s]\n",
      "100%|██████████| 462/462 [00:08<00:00, 55.04it/s]\n",
      "100%|██████████| 116/116 [00:00<00:00, 148.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], Train Loss : [0.51093] Val Loss : [0.33744] Val AUC : [0.93560]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 462/462 [00:07<00:00, 62.87it/s]\n",
      "100%|██████████| 116/116 [00:00<00:00, 153.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2], Train Loss : [0.31068] Val Loss : [0.28448] Val AUC : [0.95397]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 462/462 [00:07<00:00, 61.34it/s]\n",
      "100%|██████████| 116/116 [00:00<00:00, 174.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3], Train Loss : [0.26747] Val Loss : [0.27039] Val AUC : [0.96240]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 462/462 [00:07<00:00, 59.64it/s]\n",
      "100%|██████████| 116/116 [00:00<00:00, 135.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4], Train Loss : [0.23842] Val Loss : [0.22569] Val AUC : [0.96931]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 462/462 [00:08<00:00, 57.14it/s]\n",
      "100%|██████████| 116/116 [00:00<00:00, 172.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5], Train Loss : [0.21854] Val Loss : [0.20937] Val AUC : [0.97350]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50000it [32:38, 25.52it/s]\n",
      "100%|██████████| 521/521 [00:01<00:00, 364.89it/s]\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torchmetrics\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "class Config:\n",
    "    SR = 32000\n",
    "    N_MFCC = 13\n",
    "    # Dataset\n",
    "    ROOT_FOLDER = './'\n",
    "    # Training\n",
    "    N_CLASSES = 2\n",
    "    BATCH_SIZE = 96\n",
    "    N_EPOCHS = 5\n",
    "    LR = 3e-4\n",
    "    # Others\n",
    "    SEED = 42\n",
    "    \n",
    "CONFIG = Config()\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CONFIG.SEED) # Seed 고정\n",
    "\n",
    "df = pd.read_csv('./train.csv')\n",
    "train, val, _, _ = train_test_split(df, df['label'], test_size=0.2, random_state=CONFIG.SEED)\n",
    "\n",
    "def get_mfcc_feature(df, train_mode=True):\n",
    "    features = []\n",
    "    labels = []\n",
    "    for _, row in tqdm(df.iterrows()):\n",
    "        # librosa패키지를 사용하여 wav 파일 load\n",
    "        y, sr = librosa.load(row['path'], sr=CONFIG.SR)\n",
    "        \n",
    "        # librosa패키지를 사용하여 mfcc 추출\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=CONFIG.N_MFCC)\n",
    "        mfcc = np.mean(mfcc.T, axis=0)\n",
    "        features.append(mfcc)\n",
    "\n",
    "        if train_mode:\n",
    "            label = row['label']\n",
    "            label_vector = np.zeros(CONFIG.N_CLASSES, dtype=float)\n",
    "            label_vector[0 if label == 'fake' else 1] = 1\n",
    "            labels.append(label_vector)\n",
    "\n",
    "    if train_mode:\n",
    "        return features, labels\n",
    "    return features\n",
    "\n",
    "train_mfcc, train_labels = get_mfcc_feature(train, True)\n",
    "val_mfcc, val_labels = get_mfcc_feature(val, True)\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, mfcc, label):\n",
    "        self.mfcc = mfcc\n",
    "        self.label = label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mfcc)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.label is not None:\n",
    "            return self.mfcc[index], self.label[index]\n",
    "        return self.mfcc[index]\n",
    "\n",
    "train_dataset = CustomDataset(train_mfcc, train_labels)\n",
    "val_dataset = CustomDataset(val_mfcc, val_labels)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=CONFIG.BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=CONFIG.BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim=CONFIG.N_MFCC, hidden_dim=128, output_dim=CONFIG.N_CLASSES):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def train(model, optimizer, train_loader, val_loader, device):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss().to(device)\n",
    "    \n",
    "    best_val_score = 0\n",
    "    best_model = None\n",
    "    \n",
    "    for epoch in range(1, CONFIG.N_EPOCHS+1):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        for features, labels in tqdm(iter(train_loader)):\n",
    "            features = features.float().to(device)\n",
    "            labels = labels.float().to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model(features)\n",
    "            loss = criterion(output, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss.append(loss.item())\n",
    "                    \n",
    "        _val_loss, _val_score = validation(model, criterion, val_loader, device)\n",
    "        _train_loss = np.mean(train_loss)\n",
    "        print(f'Epoch [{epoch}], Train Loss : [{_train_loss:.5f}] Val Loss : [{_val_loss:.5f}] Val AUC : [{_val_score:.5f}]')\n",
    "            \n",
    "        if best_val_score < _val_score:\n",
    "            best_val_score = _val_score\n",
    "            best_model = model\n",
    "    \n",
    "    return best_model\n",
    "\n",
    "def multiLabel_AUC(y_true, y_scores):\n",
    "    auc_scores = []\n",
    "    for i in range(y_true.shape[1]):\n",
    "        auc = roc_auc_score(y_true[:, i], y_scores[:, i])\n",
    "        auc_scores.append(auc)\n",
    "    mean_auc_score = np.mean(auc_scores)\n",
    "    return mean_auc_score\n",
    "    \n",
    "def validation(model, criterion, val_loader, device):\n",
    "    model.eval()\n",
    "    val_loss, all_labels, all_probs = [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for features, labels in tqdm(iter(val_loader)):\n",
    "            features = features.float().to(device)\n",
    "            labels = labels.float().to(device)\n",
    "            \n",
    "            probs = model(features)\n",
    "            \n",
    "            loss = criterion(probs, labels)\n",
    "\n",
    "            val_loss.append(loss.item())\n",
    "\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "            all_probs.append(probs.cpu().numpy())\n",
    "        \n",
    "        _val_loss = np.mean(val_loss)\n",
    "\n",
    "        all_labels = np.concatenate(all_labels, axis=0)\n",
    "        all_probs = np.concatenate(all_probs, axis=0)\n",
    "        \n",
    "        # Calculate AUC score\n",
    "        auc_score = multiLabel_AUC(all_labels, all_probs)\n",
    "    \n",
    "    return _val_loss, auc_score\n",
    "\n",
    "model = MLP()\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr = CONFIG.LR)\n",
    "\n",
    "infer_model = train(model, optimizer, train_loader, val_loader, device)\n",
    "\n",
    "test = pd.read_csv('./test.csv')\n",
    "test_mfcc = get_mfcc_feature(test, False)\n",
    "test_dataset = CustomDataset(test_mfcc, None)\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=CONFIG.BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "def inference(model, test_loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for features in tqdm(iter(test_loader)):\n",
    "            features = features.float().to(device)\n",
    "            \n",
    "            probs = model(features)\n",
    "\n",
    "            probs  = probs.cpu().detach().numpy()\n",
    "            predictions += probs.tolist()\n",
    "    return predictions\n",
    "\n",
    "preds = inference(infer_model, test_loader, device)\n",
    "\n",
    "submit = pd.read_csv('./sample_submission.csv')\n",
    "submit.iloc[:, 1:] = preds\n",
    "submit.head()\n",
    "\n",
    "submit.to_csv('./baseline_submit.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "44350it [19:00, 38.90it/s]\n",
      "11088it [04:42, 39.22it/s]\n",
      "100%|██████████| 462/462 [00:03<00:00, 123.43it/s]\n",
      "100%|██████████| 116/116 [00:00<00:00, 228.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], Train Loss : [0.51093] Val Loss : [0.33744] Val AUC : [0.93560]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 462/462 [00:04<00:00, 103.87it/s]\n",
      "100%|██████████| 116/116 [00:00<00:00, 429.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2], Train Loss : [0.31068] Val Loss : [0.28448] Val AUC : [0.95397]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 462/462 [00:04<00:00, 115.29it/s]\n",
      "100%|██████████| 116/116 [00:00<00:00, 311.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3], Train Loss : [0.26747] Val Loss : [0.27039] Val AUC : [0.96240]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 462/462 [00:04<00:00, 97.37it/s] \n",
      "100%|██████████| 116/116 [00:00<00:00, 303.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4], Train Loss : [0.23842] Val Loss : [0.22569] Val AUC : [0.96931]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 462/462 [00:04<00:00, 96.44it/s] \n",
      "100%|██████████| 116/116 [00:00<00:00, 192.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5], Train Loss : [0.21854] Val Loss : [0.20937] Val AUC : [0.97350]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 116/116 [00:00<00:00, 247.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Combined Score: [0.03366]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50000it [31:51, 26.16it/s]\n",
      "100%|██████████| 521/521 [00:04<00:00, 130.00it/s]\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torchmetrics\n",
    "import os\n",
    "import warnings\n",
    "from sklearn.metrics import mean_squared_error, roc_auc_score\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "class Config:\n",
    "    SR = 32000\n",
    "    N_MFCC = 13\n",
    "    ROOT_FOLDER = './'\n",
    "    N_CLASSES = 2\n",
    "    BATCH_SIZE = 96\n",
    "    N_EPOCHS = 5\n",
    "    LR = 3e-4\n",
    "    SEED = 42\n",
    "    \n",
    "CONFIG = Config()\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CONFIG.SEED) # Seed 고정\n",
    "\n",
    "df = pd.read_csv('./train.csv')\n",
    "train, val, _, _ = train_test_split(df, df['label'], test_size=0.2, random_state=CONFIG.SEED)\n",
    "\n",
    "def get_mfcc_feature(df, train_mode=True):\n",
    "    features = []\n",
    "    labels = []\n",
    "    for _, row in tqdm(df.iterrows()):\n",
    "        y, sr = librosa.load(row['path'], sr=CONFIG.SR)\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=CONFIG.N_MFCC)\n",
    "        mfcc = np.mean(mfcc.T, axis=0)\n",
    "        features.append(mfcc)\n",
    "\n",
    "        if train_mode:\n",
    "            label = row['label']\n",
    "            label_vector = np.zeros(CONFIG.N_CLASSES, dtype=float)\n",
    "            label_vector[0 if label == 'fake' else 1] = 1\n",
    "            labels.append(label_vector)\n",
    "\n",
    "    if train_mode:\n",
    "        return features, labels\n",
    "    return features\n",
    "\n",
    "train_mfcc, train_labels = get_mfcc_feature(train, True)\n",
    "val_mfcc, val_labels = get_mfcc_feature(val, True)\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, mfcc, label):\n",
    "        self.mfcc = mfcc\n",
    "        self.label = label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mfcc)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.label is not None:\n",
    "            return self.mfcc[index], self.label[index]\n",
    "        return self.mfcc[index]\n",
    "\n",
    "train_dataset = CustomDataset(train_mfcc, train_labels)\n",
    "val_dataset = CustomDataset(val_mfcc, val_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=False)\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim=CONFIG.N_MFCC, hidden_dim=128, output_dim=CONFIG.N_CLASSES):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "def train(model, optimizer, train_loader, val_loader, device):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss().to(device)\n",
    "    \n",
    "    best_val_score = 0\n",
    "    best_model = None\n",
    "    \n",
    "    for epoch in range(1, CONFIG.N_EPOCHS+1):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        for features, labels in tqdm(iter(train_loader)):\n",
    "            features = features.float().to(device)\n",
    "            labels = labels.float().to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model(features)\n",
    "            loss = criterion(output, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss.append(loss.item())\n",
    "                    \n",
    "        _val_loss, _val_score = validation(model, criterion, val_loader, device)\n",
    "        _train_loss = np.mean(train_loss)\n",
    "        print(f'Epoch [{epoch}], Train Loss : [{_train_loss:.5f}] Val Loss : [{_val_loss:.5f}] Val AUC : [{_val_score:.5f}]')\n",
    "            \n",
    "        if best_val_score < _val_score:\n",
    "            best_val_score = _val_score\n",
    "            best_model = model\n",
    "    \n",
    "    return best_model\n",
    "\n",
    "def multiLabel_AUC(y_true, y_scores):\n",
    "    auc_scores = []\n",
    "    for i in range(y_true.shape[1]):\n",
    "        auc = roc_auc_score(y_true[:, i], y_scores[:, i])\n",
    "        auc_scores.append(auc)\n",
    "    mean_auc_score = np.mean(auc_scores)\n",
    "    return mean_auc_score\n",
    "    \n",
    "def validation(model, criterion, val_loader, device):\n",
    "    model.eval()\n",
    "    val_loss, all_labels, all_probs = [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for features, labels in tqdm(iter(val_loader)):\n",
    "            features = features.float().to(device)\n",
    "            labels = labels.float().to(device)\n",
    "            \n",
    "            probs = model(features)\n",
    "            \n",
    "            loss = criterion(probs, labels)\n",
    "\n",
    "            val_loss.append(loss.item())\n",
    "\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "            all_probs.append(probs.cpu().numpy())\n",
    "        \n",
    "        _val_loss = np.mean(val_loss)\n",
    "\n",
    "        all_labels = np.concatenate(all_labels, axis=0)\n",
    "        all_probs = np.concatenate(all_probs, axis=0)\n",
    "        \n",
    "        auc_score = multiLabel_AUC(all_labels, all_probs)\n",
    "    \n",
    "    return _val_loss, auc_score\n",
    "\n",
    "# 평가 산식 함수 정의\n",
    "def expected_calibration_error(y_true, y_prob, n_bins=10):\n",
    "    prob_true, prob_pred = calibration_curve(y_true, y_prob, n_bins=n_bins, strategy='uniform')\n",
    "    bin_totals = np.histogram(y_prob, bins=np.linspace(0, 1, n_bins + 1), density=False)[0]\n",
    "    non_empty_bins = bin_totals > 0\n",
    "    bin_weights = bin_totals / len(y_prob)\n",
    "    bin_weights = bin_weights[non_empty_bins]\n",
    "    prob_true = prob_true[:len(bin_weights)]\n",
    "    prob_pred = prob_pred[:len(bin_weights)]\n",
    "    ece = np.sum(bin_weights * np.abs(prob_true - prob_pred))\n",
    "    return ece\n",
    "\n",
    "def auc_brier_ece(answer_df, submission_df):\n",
    "    if submission_df.isnull().values.any():\n",
    "        raise ValueError(\"The submission dataframe contains missing values.\")\n",
    "\n",
    "    if len(answer_df.columns) != len(submission_df.columns) or not all(answer_df.columns == submission_df.columns):\n",
    "        raise ValueError(\"The columns of the answer and submission dataframes do not match.\")\n",
    "\n",
    "    submission_df = submission_df[submission_df.index.isin(answer_df.index)]\n",
    "    submission_df.index = range(submission_df.shape[0])\n",
    "    \n",
    "    auc_scores = []\n",
    "    for column in answer_df.columns:\n",
    "        y_true = answer_df[column]\n",
    "        y_scores = submission_df[column]\n",
    "        auc = roc_auc_score(y_true, y_scores)\n",
    "        auc_scores.append(auc)\n",
    "\n",
    "    mean_auc = np.mean(auc_scores)\n",
    "\n",
    "    brier_scores = []\n",
    "    ece_scores = []\n",
    "    \n",
    "    for column in answer_df.columns:\n",
    "        y_true = answer_df[column].values\n",
    "        y_prob = submission_df[column].values\n",
    "        \n",
    "        brier = mean_squared_error(y_true, y_prob)\n",
    "        brier_scores.append(brier)\n",
    "        \n",
    "        ece = expected_calibration_error(y_true, y_prob)\n",
    "        ece_scores.append(ece)\n",
    "    \n",
    "    mean_brier = np.mean(brier_scores)\n",
    "    mean_ece = np.mean(ece_scores)\n",
    "    \n",
    "    combined_score = 0.5 * (1 - mean_auc) + 0.25 * mean_brier + 0.25 * mean_ece\n",
    "    \n",
    "    return combined_score\n",
    "\n",
    "model = MLP()\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr = CONFIG.LR)\n",
    "\n",
    "infer_model = train(model, optimizer, train_loader, val_loader, device)\n",
    "\n",
    "# 최종 모델 평가\n",
    "val_probs = []\n",
    "with torch.no_grad():\n",
    "    for features, labels in tqdm(iter(val_loader)):\n",
    "        features = features.float().to(device)\n",
    "        probs = infer_model(features)\n",
    "        val_probs.append(probs.cpu().numpy())\n",
    "\n",
    "val_probs = np.concatenate(val_probs, axis=0)\n",
    "val_labels = np.array(val_labels)\n",
    "answer_df = pd.DataFrame(val_labels, columns=['fake', 'real'])\n",
    "submission_df = pd.DataFrame(val_probs, columns=['fake', 'real'])\n",
    "combined_score = auc_brier_ece(answer_df, submission_df)\n",
    "print(f'Final Combined Score: [{combined_score:.5f}]')\n",
    "\n",
    "test = pd.read_csv('./test.csv')\n",
    "test_mfcc = get_mfcc_feature(test, False)\n",
    "test_dataset = CustomDataset(test_mfcc, None)\n",
    "test_loader = DataLoader(test_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=False)\n",
    "\n",
    "def inference(model, test_loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for features in tqdm(iter(test_loader)):\n",
    "            features = features.float().to(device)\n",
    "            \n",
    "            probs = model(features)\n",
    "\n",
    "            probs  = probs.cpu().detach().numpy()\n",
    "            predictions += probs.tolist()\n",
    "    return predictions\n",
    "\n",
    "preds = inference(infer_model, test_loader, device)\n",
    "\n",
    "submit = pd.read_csv('./sample_submission.csv')\n",
    "submit.iloc[:, 1:] = preds\n",
    "submit.head()\n",
    "\n",
    "submit.to_csv('./baseline_submit2.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import warnings\n",
    "from sklearn.metrics import mean_squared_error, roc_auc_score\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "class Config:\n",
    "    SR = 32000\n",
    "    N_MFCC = 13\n",
    "    ROOT_FOLDER = './'\n",
    "    N_CLASSES = 2\n",
    "    BATCH_SIZE = 96\n",
    "    N_EPOCHS = 5\n",
    "    LR = 3e-4\n",
    "    SEED = 42\n",
    "    \n",
    "CONFIG = Config()\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CONFIG.SEED)\n",
    "\n",
    "df = pd.read_csv('./train.csv')\n",
    "train, val, _, _ = train_test_split(df, df['label'], test_size=0.2, random_state=CONFIG.SEED)\n",
    "\n",
    "def get_mfcc_feature(df, train_mode=True):\n",
    "    features = []\n",
    "    labels = []\n",
    "    for _, row in tqdm(df.iterrows()):\n",
    "        y, sr = librosa.load(row['path'], sr=CONFIG.SR)\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=CONFIG.N_MFCC)\n",
    "        mfcc = np.mean(mfcc.T, axis=0)\n",
    "        features.append(mfcc)\n",
    "\n",
    "        if train_mode:\n",
    "            label = row['label']\n",
    "            label_vector = np.zeros(CONFIG.N_CLASSES, dtype=float)\n",
    "            label_vector[0 if label == 'fake' else 1] = 1\n",
    "            labels.append(label_vector)\n",
    "\n",
    "    if train_mode:\n",
    "        return features, labels\n",
    "    return features\n",
    "\n",
    "train_mfcc, train_labels = get_mfcc_feature(train, True)\n",
    "val_mfcc, val_labels = get_mfcc_feature(val, True)\n",
    "\n",
    "# 데이터 불균형 해결 (SMOTE 사용)\n",
    "smote = SMOTE(random_state=CONFIG.SEED)\n",
    "X_resampled, y_resampled = smote.fit_resample(np.array(train_mfcc), np.array(train_labels))\n",
    "y_resampled = torch.tensor(y_resampled).float()\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, mfcc, label):\n",
    "        self.mfcc = mfcc\n",
    "        self.label = label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mfcc)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.label is not None:\n",
    "            return self.mfcc[index], self.label[index]\n",
    "        return self.mfcc[index]\n",
    "\n",
    "train_dataset = CustomDataset(X_resampled, y_resampled)\n",
    "val_dataset = CustomDataset(np.array(val_mfcc), np.array(val_labels))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=False)\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim=CONFIG.N_MFCC, hidden_dim=128, output_dim=CONFIG.N_CLASSES):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "def train(model, optimizer, train_loader, val_loader, device):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss().to(device)\n",
    "    \n",
    "    best_val_score = 0\n",
    "    best_model = None\n",
    "    \n",
    "    for epoch in range(1, CONFIG.N_EPOCHS+1):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        for features, labels in tqdm(iter(train_loader)):\n",
    "            features = features.float().to(device)\n",
    "            labels = labels.float().to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model(features)\n",
    "            loss = criterion(output, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss.append(loss.item())\n",
    "                    \n",
    "        _val_loss, _val_score = validation(model, criterion, val_loader, device)\n",
    "        _train_loss = np.mean(train_loss)\n",
    "        print(f'Epoch [{epoch}], Train Loss : [{_train_loss:.5f}] Val Loss : [{_val_loss:.5f}] Val AUC : [{_val_score:.5f}]')\n",
    "            \n",
    "        if best_val_score < _val_score:\n",
    "            best_val_score = _val_score\n",
    "            best_model = model\n",
    "    \n",
    "    return best_model\n",
    "\n",
    "def multiLabel_AUC(y_true, y_scores):\n",
    "    auc_scores = []\n",
    "    for i in range(y_true.shape[1]):\n",
    "        auc = roc_auc_score(y_true[:, i], y_scores[:, i])\n",
    "        auc_scores.append(auc)\n",
    "    mean_auc_score = np.mean(auc_scores)\n",
    "    return mean_auc_score\n",
    "    \n",
    "def validation(model, criterion, val_loader, device):\n",
    "    model.eval()\n",
    "    val_loss, all_labels, all_probs = [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for features, labels in tqdm(iter(val_loader)):\n",
    "            features = features.float().to(device)\n",
    "            labels = labels.float().to(device)\n",
    "            \n",
    "            probs = model(features)\n",
    "            \n",
    "            loss = criterion(probs, labels)\n",
    "\n",
    "            val_loss.append(loss.item())\n",
    "\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "            all_probs.append(probs.cpu().numpy())\n",
    "        \n",
    "        _val_loss = np.mean(val_loss)\n",
    "\n",
    "        all_labels = np.concatenate(all_labels, axis=0)\n",
    "        all_probs = np.concatenate(all_probs, axis=0)\n",
    "        \n",
    "        auc_score = multiLabel_AUC(all_labels, all_probs)\n",
    "    \n",
    "    return _val_loss, auc_score\n",
    "\n",
    "# 평가 산식 함수 정의\n",
    "def expected_calibration_error(y_true, y_prob, n_bins=10):\n",
    "    prob_true, prob_pred = calibration_curve(y_true, y_prob, n_bins=n_bins, strategy='uniform')\n",
    "    bin_totals = np.histogram(y_prob, bins=np.linspace(0, 1, n_bins + 1), density=False)[0]\n",
    "    non_empty_bins = bin_totals > 0\n",
    "    bin_weights = bin_totals / len(y_prob)\n",
    "    bin_weights = bin_weights[non_empty_bins]\n",
    "    prob_true = prob_true[:len(bin_weights)]\n",
    "    prob_pred = prob_pred[:len(bin_weights)]\n",
    "    ece = np.sum(bin_weights * np.abs(prob_true - prob_pred))\n",
    "    return ece\n",
    "\n",
    "def auc_brier_ece(answer_df, submission_df):\n",
    "    if submission_df.isnull().values.any():\n",
    "        raise ValueError(\"The submission dataframe contains missing values.\")\n",
    "\n",
    "    if len(answer_df.columns) != len(submission_df.columns) or not all(answer_df.columns == submission_df.columns):\n",
    "        raise ValueError(\"The columns of the answer and submission dataframes do not match.\")\n",
    "\n",
    "    auc_scores = []\n",
    "    for column in answer_df.columns:\n",
    "        y_true = answer_df[column]\n",
    "        y_scores = submission_df[column]\n",
    "        auc = roc_auc_score(y_true, y_scores)\n",
    "        auc_scores.append(auc)\n",
    "\n",
    "    mean_auc = np.mean(auc_scores)\n",
    "\n",
    "    brier_scores = []\n",
    "    ece_scores = []\n",
    "    \n",
    "    for column in answer_df.columns:\n",
    "        y_true = answer_df[column].values\n",
    "        y_prob = submission_df[column].values\n",
    "        \n",
    "        brier = mean_squared_error(y_true, y_prob)\n",
    "        brier_scores.append(brier)\n",
    "        \n",
    "        ece = expected_calibration_error(y_true, y_prob)\n",
    "        ece_scores.append(ece)\n",
    "    \n",
    "    mean_brier = np.mean(brier_scores)\n",
    "    mean_ece = np.mean(ece_scores)\n",
    "    \n",
    "    combined_score = 0.5 * (1 - mean_auc) + 0.25 * mean_brier + 0.25 * mean_ece\n",
    "    \n",
    "    return combined_score\n",
    "\n",
    "# Unlabeled 데이터에 대해 pseudo-labeling\n",
    "def get_mfcc_feature_from_files(file_paths):\n",
    "    features = []\n",
    "    for file_path in tqdm(file_paths):\n",
    "        y, sr = librosa.load(file_path, sr=CONFIG.SR)\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=CONFIG.N_MFCC)\n",
    "        mfcc = np.mean(mfcc.T, axis=0)\n",
    "        features.append(mfcc)\n",
    "    return features\n",
    "\n",
    "unlabeled_path = './unlabeled_data'\n",
    "unlabeled_files = [os.path.join(unlabeled_path, f) for f in os.listdir(unlabeled_path) if f.endswith('.ogg')]\n",
    "unlabeled_features = get_mfcc_feature_from_files(unlabeled_files)\n",
    "\n",
    "unlabeled_dataset = CustomDataset(np.array(unlabeled_features), None)\n",
    "unlabeled_loader = DataLoader(unlabeled_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=False)\n",
    "\n",
    "def pseudo_labeling(model, loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    pseudo_labels = []\n",
    "    with torch.no_grad():\n",
    "        for features in tqdm(iter(loader)):\n",
    "            features = features.float().to(device)\n",
    "            probs = model(features)\n",
    "            pseudo_labels.append(probs.cpu().detach().numpy())\n",
    "    return np.concatenate(pseudo_labels, axis=0)\n",
    "\n",
    "# Pseudo-label 생성\n",
    "pseudo_labels = pseudo_labeling(infer_model, unlabeled_loader, device)\n",
    "pseudo_labels = (pseudo_labels > 0.5).astype(int)  # 임계값 0.5를 사용하여 pseudo-label 생성\n",
    "\n",
    "# Pseudo-labeled 데이터를 학습 데이터에 추가\n",
    "pseudo_labeled_dataset = CustomDataset(np.array(unlabeled_features), torch.tensor(pseudo_labels).float())\n",
    "train_dataset_combined = torch.utils.data.ConcatDataset([train_dataset, pseudo_labeled_dataset])\n",
    "train_loader_combined = DataLoader(train_dataset_combined, batch_size=CONFIG.BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# 모델을 pseudo-labeled 데이터를 포함하여 재학습\n",
    "model = MLP()\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr = CONFIG.LR)\n",
    "infer_model = train(model, optimizer, train_loader_combined, val_loader, device)\n",
    "\n",
    "# 최종 모델 평가\n",
    "val_probs = []\n",
    "with torch.no_grad():\n",
    "    for features, labels in tqdm(iter(val_loader)):\n",
    "        features = features.float().to(device)\n",
    "        probs = infer_model(features)\n",
    "        val_probs.append(probs.cpu().numpy())\n",
    "\n",
    "val_probs = np.concatenate(val_probs, axis=0)\n",
    "val_labels = np.array(val_labels)\n",
    "answer_df = pd.DataFrame(val_labels, columns=['fake', 'real'])\n",
    "submission_df = pd.DataFrame(val_probs, columns=['fake', 'real'])\n",
    "combined_score = auc_brier_ece(answer_df, submission_df)\n",
    "print(f'Final Combined Score: [{combined_score:.5f}]')\n",
    "\n",
    "test = pd.read_csv('./test.csv')\n",
    "test_mfcc = get_mfcc_feature(test, False)\n",
    "test_dataset = CustomDataset(np.array(test_mfcc), None)\n",
    "test_loader = DataLoader(test_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=False)\n",
    "\n",
    "def inference(model, test_loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for features in tqdm(iter(test_loader)):\n",
    "            features = features.float().to(device)\n",
    "            \n",
    "            probs = model(features)\n",
    "\n",
    "            probs  = probs.cpu().detach().numpy()\n",
    "            predictions += probs.tolist()\n",
    "    return predictions\n",
    "\n",
    "preds = inference(infer_model, test_loader, device)\n",
    "\n",
    "submit = pd.read_csv('./sample_submission.csv')\n",
    "submit.iloc[:, 1:] = preds\n",
    "submit.head()\n",
    "\n",
    "submit.to_csv('./baseline_submit3.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
