{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label ['fake' 'real']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "55438it [23:41, 39.00it/s]\n",
      "100%|██████████| 464/464 [00:06<00:00, 67.40it/s]\n",
      "100%|██████████| 116/116 [00:00<00:00, 290.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], Train Loss : [0.94270] Val Loss : [0.67075] Val AUC : [0.73635] Combined Score: [0.22962]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 464/464 [00:07<00:00, 65.42it/s]\n",
      "100%|██████████| 116/116 [00:00<00:00, 244.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2], Train Loss : [0.65205] Val Loss : [0.59030] Val AUC : [0.78640] Combined Score: [0.18217]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 464/464 [00:07<00:00, 65.23it/s]\n",
      "100%|██████████| 116/116 [00:00<00:00, 259.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3], Train Loss : [0.59321] Val Loss : [0.51249] Val AUC : [0.84822] Combined Score: [0.13870]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 464/464 [00:06<00:00, 68.75it/s]\n",
      "100%|██████████| 116/116 [00:00<00:00, 294.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4], Train Loss : [0.52001] Val Loss : [0.42587] Val AUC : [0.90458] Combined Score: [0.10156]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 464/464 [00:07<00:00, 63.32it/s]\n",
      "100%|██████████| 116/116 [00:00<00:00, 291.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5], Train Loss : [0.45694] Val Loss : [0.38388] Val AUC : [0.92793] Combined Score: [0.08886]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1264/1264 [00:36<00:00, 34.22it/s]\n",
      "100%|██████████| 14/14 [00:00<00:00, 244.94it/s]\n",
      "100%|██████████| 477/477 [00:06<00:00, 69.34it/s]\n",
      "100%|██████████| 116/116 [00:00<00:00, 264.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], Train Loss : [0.41048] Val Loss : [0.34715] Val AUC : [0.93900] Combined Score: [0.07658]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 477/477 [00:07<00:00, 66.06it/s]\n",
      "100%|██████████| 116/116 [00:00<00:00, 288.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2], Train Loss : [0.37720] Val Loss : [0.30946] Val AUC : [0.94859] Combined Score: [0.06571]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 477/477 [00:07<00:00, 64.94it/s]\n",
      "100%|██████████| 116/116 [00:00<00:00, 259.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3], Train Loss : [0.35390] Val Loss : [0.29682] Val AUC : [0.95573] Combined Score: [0.06273]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 477/477 [00:06<00:00, 70.21it/s]\n",
      "100%|██████████| 116/116 [00:00<00:00, 284.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4], Train Loss : [0.33200] Val Loss : [0.27937] Val AUC : [0.96103] Combined Score: [0.05734]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 477/477 [00:07<00:00, 64.20it/s]\n",
      "100%|██████████| 116/116 [00:00<00:00, 250.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5], Train Loss : [0.31640] Val Loss : [0.25648] Val AUC : [0.96589] Combined Score: [0.05019]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50000it [30:15, 27.54it/s]\n",
      "100%|██████████| 521/521 [00:02<00:00, 177.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Combined Score before test: 0.42355\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import librosa\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import warnings\n",
    "from sklearn.metrics import roc_auc_score, mean_squared_error\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "class Config:\n",
    "    SR = 32000\n",
    "    N_MFCC = 13\n",
    "    ROOT_FOLDER = './'\n",
    "    N_CLASSES = 2\n",
    "    BATCH_SIZE = 96\n",
    "    N_EPOCHS = 5\n",
    "    LR = 3e-4\n",
    "    SEED = 42\n",
    "\n",
    "CONFIG = Config()\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CONFIG.SEED)\n",
    "\n",
    "# 데이터 로드 및 전처리\n",
    "df = pd.read_csv('./train.csv')\n",
    "\n",
    "def label_encoder(column):\n",
    "    le = LabelEncoder().fit(column)\n",
    "    print(column.name, le.classes_)\n",
    "    return le.transform(column)\n",
    "\n",
    "df['class'] = label_encoder(df['label'])\n",
    "\n",
    "# MFCC 특징 추출\n",
    "def get_mfcc_feature(df, train_mode=True):\n",
    "    features = []\n",
    "    labels = []\n",
    "    for _, row in tqdm(df.iterrows()):\n",
    "        y, sr = librosa.load(row['path'], sr=CONFIG.SR)\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=CONFIG.N_MFCC)\n",
    "        mfcc = np.mean(mfcc.T, axis=0)\n",
    "        features.append(mfcc)\n",
    "\n",
    "        if train_mode:\n",
    "            label = row['class']\n",
    "            labels.append(label)\n",
    "\n",
    "    return features, labels\n",
    "\n",
    "features, labels = get_mfcc_feature(df, True)\n",
    "feature_df = pd.DataFrame({'features': features, 'class': labels})\n",
    "\n",
    "X = np.array(feature_df['features'].tolist())\n",
    "y = np.array(feature_df['class'].tolist())\n",
    "\n",
    "# 데이터 불균형 해결\n",
    "ros = RandomOverSampler(random_state=CONFIG.SEED)\n",
    "X_resampled, y_resampled = ros.fit_resample(X, y)\n",
    "y_resampled = torch.tensor(y_resampled).long()  # 정수형으로 변환\n",
    "y_resampled = torch.nn.functional.one_hot(y_resampled, num_classes=CONFIG.N_CLASSES).float()\n",
    "\n",
    "# 데이터 분할\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=CONFIG.SEED)\n",
    "\n",
    "# PyTorch Dataset 클래스 정의\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, mfcc, label):\n",
    "        self.mfcc = mfcc\n",
    "        self.label = label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mfcc)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.mfcc[index], self.label[index]\n",
    "\n",
    "train_dataset = CustomDataset(X_train, y_train)\n",
    "val_dataset = CustomDataset(X_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# MLP 모델 정의\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim=CONFIG.N_MFCC, hidden_dims=[128, 256, 128], output_dim=CONFIG.N_CLASSES):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dims[0])\n",
    "        self.fc2 = nn.Linear(hidden_dims[0], hidden_dims[1])\n",
    "        self.fc3 = nn.Linear(hidden_dims[1], hidden_dims[2])\n",
    "        self.fc4 = nn.Linear(hidden_dims[2], output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "# 평가 함수 정의\n",
    "def expected_calibration_error(y_true, y_prob, n_bins=10):\n",
    "    prob_true, prob_pred = calibration_curve(y_true, y_prob, n_bins=n_bins, strategy='uniform')\n",
    "    bin_totals = np.histogram(y_prob, bins=np.linspace(0, 1, n_bins + 1), density=False)[0]\n",
    "    non_empty_bins = bin_totals > 0\n",
    "    bin_weights = bin_totals / len(y_prob)\n",
    "    bin_weights = bin_weights[non_empty_bins]\n",
    "    prob_true = prob_true[:len(bin_weights)]\n",
    "    prob_pred = prob_pred[:len(bin_weights)]\n",
    "    ece = np.sum(bin_weights * np.abs(prob_true - prob_pred))\n",
    "    return ece\n",
    "\n",
    "def auc_brier_ece(answer_df, submission_df):\n",
    "    # Check for missing values in submission_df\n",
    "    if submission_df.isnull().values.any():\n",
    "        raise ValueError(\"The submission dataframe contains missing values.\")\n",
    "\n",
    "    # Check if the number and names of columns are the same in both dataframes\n",
    "    if len(answer_df.columns) != len(submission_df.columns) or not all(answer_df.columns == submission_df.columns):\n",
    "        raise ValueError(\"The columns of the answer and submission dataframes do not match.\")\n",
    "        \n",
    "    submission_df = submission_df[submission_df.index.isin(answer_df.index)]\n",
    "    submission_df.index = range(submission_df.shape[0])\n",
    "    \n",
    "    # Calculate AUC for each class\n",
    "    auc_scores = []\n",
    "    for column in answer_df.columns:\n",
    "        y_true = answer_df[column]\n",
    "        y_scores = submission_df[column]\n",
    "        auc = roc_auc_score(y_true, y_scores)\n",
    "        auc_scores.append(auc)\n",
    "\n",
    "    # Calculate mean AUC\n",
    "    mean_auc = np.mean(auc_scores)\n",
    "\n",
    "    brier_scores = []\n",
    "    ece_scores = []\n",
    "    \n",
    "    # Calculate Brier Score and ECE for each class\n",
    "    for column in answer_df.columns:\n",
    "        y_true = answer_df[column].values\n",
    "        y_prob = submission_df[column].values\n",
    "        \n",
    "        # Brier Score\n",
    "        brier = mean_squared_error(y_true, y_prob)\n",
    "        brier_scores.append(brier)\n",
    "        \n",
    "        # ECE\n",
    "        ece = expected_calibration_error(y_true, y_prob)\n",
    "        ece_scores.append(ece)\n",
    "    \n",
    "    # Calculate mean Brier Score and mean ECE\n",
    "    mean_brier = np.mean(brier_scores)\n",
    "    mean_ece = np.mean(ece_scores)\n",
    "    \n",
    "    # Calculate combined score\n",
    "    combined_score = 0.5 * (1 - mean_auc) + 0.25 * mean_brier + 0.25 * mean_ece\n",
    "    \n",
    "    return combined_score\n",
    "\n",
    "# 모델 학습 함수 정의\n",
    "def train(model, optimizer, train_loader, val_loader, device):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss().to(device)\n",
    "    \n",
    "    best_val_score = 0\n",
    "    best_model = None\n",
    "    val_labels = y_val.cpu().numpy()\n",
    "    \n",
    "    for epoch in range(1, CONFIG.N_EPOCHS+1):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        for features, labels in tqdm(iter(train_loader)):\n",
    "            features = features.float().to(device)\n",
    "            labels = labels.float().to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model(features)\n",
    "            loss = criterion(output, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss.append(loss.item())\n",
    "                    \n",
    "        _val_loss, _val_score, val_outputs = validation(model, criterion, val_loader, device)\n",
    "        _train_loss = np.mean(train_loss)\n",
    "        \n",
    "        combined_score = auc_brier_ece(pd.DataFrame(val_labels), pd.DataFrame(val_outputs))\n",
    "        \n",
    "        print(f'Epoch [{epoch}], Train Loss : [{_train_loss:.5f}] Val Loss : [{_val_loss:.5f}] Val AUC : [{_val_score:.5f}] Combined Score: [{combined_score:.5f}]')\n",
    "            \n",
    "        if best_val_score < _val_score:\n",
    "            best_val_score = _val_score\n",
    "            best_model = model\n",
    "    \n",
    "    return best_model\n",
    "\n",
    "def multiLabel_AUC(y_true, y_scores):\n",
    "    auc_scores = []\n",
    "    for i in range(y_true.shape[1]):\n",
    "        auc = roc_auc_score(y_true[:, i], y_scores[:, i])\n",
    "        auc_scores.append(auc)\n",
    "    mean_auc_score = np.mean(auc_scores)\n",
    "    return mean_auc_score\n",
    "    \n",
    "def validation(model, criterion, val_loader, device):\n",
    "    model.eval()\n",
    "    val_loss, all_labels, all_probs = [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for features, labels in tqdm(iter(val_loader)):\n",
    "            features = features.float().to(device)\n",
    "            labels = labels.float().to(device)\n",
    "            \n",
    "            probs = model(features)\n",
    "            \n",
    "            loss = criterion(probs, labels)\n",
    "\n",
    "            val_loss.append(loss.item())\n",
    "\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "            all_probs.append(probs.cpu().numpy())\n",
    "        \n",
    "        _val_loss = np.mean(val_loss)\n",
    "\n",
    "        all_labels = np.concatenate(all_labels, axis=0)\n",
    "        all_probs = np.concatenate(all_probs, axis=0)\n",
    "        \n",
    "        auc_score = multiLabel_AUC(all_labels, all_probs)\n",
    "    \n",
    "    return _val_loss, auc_score, all_probs\n",
    "\n",
    "model = MLP()\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr = CONFIG.LR)\n",
    "\n",
    "infer_model = train(model, optimizer, train_loader, val_loader, device)\n",
    "\n",
    "# Unlabeled 데이터에 대해 pseudo-labeling\n",
    "unlabeled_path = './unlabeled_data'\n",
    "unlabeled_files = [os.path.join(unlabeled_path, f) for f in os.listdir(unlabeled_path) if f.endswith('.ogg')]\n",
    "\n",
    "def get_mfcc_feature_from_files(file_paths):\n",
    "    features = []\n",
    "    for file_path in tqdm(file_paths):\n",
    "        y, sr = librosa.load(file_path, sr=CONFIG.SR)\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=CONFIG.N_MFCC)\n",
    "        mfcc = np.mean(mfcc.T, axis=0)\n",
    "        features.append(mfcc)\n",
    "    return features\n",
    "\n",
    "unlabeled_features = get_mfcc_feature_from_files(unlabeled_files)\n",
    "unlabeled_features = np.array(unlabeled_features)\n",
    "unlabeled_dataset = CustomDataset(unlabeled_features, torch.zeros((len(unlabeled_features), CONFIG.N_CLASSES)))\n",
    "unlabeled_loader = DataLoader(unlabeled_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=False)\n",
    "\n",
    "def pseudo_labeling(model, loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    pseudo_labels = []\n",
    "    with torch.no_grad():\n",
    "        for features, _ in tqdm(iter(loader)):\n",
    "            features = features.float().to(device)\n",
    "            probs = model(features)\n",
    "            pseudo_labels.append(probs.cpu().detach().numpy())\n",
    "    return np.concatenate(pseudo_labels, axis=0)\n",
    "\n",
    "pseudo_labels = pseudo_labeling(infer_model, unlabeled_loader, device)\n",
    "pseudo_labels = (pseudo_labels > 0.5).astype(int)  # 임계값 0.5를 사용하여 pseudo-label 생성\n",
    "\n",
    "# Pseudo-labeled 데이터를 학습 데이터에 추가\n",
    "pseudo_labeled_dataset = CustomDataset(unlabeled_features, torch.tensor(pseudo_labels).float())\n",
    "train_dataset_combined = torch.utils.data.ConcatDataset([train_dataset, pseudo_labeled_dataset])\n",
    "train_loader_combined = DataLoader(train_dataset_combined, batch_size=CONFIG.BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# 모델을 pseudo-labeled 데이터를 포함하여 재학습\n",
    "infer_model = train(model, optimizer, train_loader_combined, val_loader, device)\n",
    "\n",
    "# 테스트 데이터 예측\n",
    "test = pd.read_csv('./test.csv')\n",
    "test_features, _ = get_mfcc_feature(test, False)\n",
    "test_features = np.array(test_features)\n",
    "test_dataset = CustomDataset(test_features, torch.zeros((len(test_features), CONFIG.N_CLASSES)))\n",
    "test_loader = DataLoader(test_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=False)\n",
    "\n",
    "def inference(model, test_loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for features, _ in tqdm(iter(test_loader)):\n",
    "            features = features.float().to(device)\n",
    "            probs = model(features)\n",
    "            probs  = probs.cpu().detach().numpy()\n",
    "            predictions += probs.tolist()\n",
    "    return predictions\n",
    "\n",
    "preds = inference(infer_model, test_loader, device)\n",
    "\n",
    "submit = pd.read_csv('./sample_submission.csv')\n",
    "submit.iloc[:, 1:] = preds\n",
    "submit.head()\n",
    "\n",
    "combined_score_before_test = auc_brier_ece(pd.DataFrame(y_val.cpu().numpy()), pd.DataFrame(preds))\n",
    "print(f'Final Combined Score before test: {combined_score_before_test:.5f}')\n",
    "\n",
    "submit.to_csv('./wy_origin.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label ['fake' 'real']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "55438it [28:55, 31.95it/s]\n",
      "100%|██████████| 696/696 [00:11<00:00, 62.21it/s]\n",
      "100%|██████████| 174/174 [00:00<00:00, 274.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], Train Loss : [0.87780] Val Loss : [0.65412] Val AUC : [0.74489] Combined Score: [0.21925]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 696/696 [00:11<00:00, 58.07it/s]\n",
      "100%|██████████| 174/174 [00:00<00:00, 320.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2], Train Loss : [0.63537] Val Loss : [0.56339] Val AUC : [0.81182] Combined Score: [0.16542]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 696/696 [00:12<00:00, 57.16it/s]\n",
      "100%|██████████| 174/174 [00:00<00:00, 285.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3], Train Loss : [0.55121] Val Loss : [0.44588] Val AUC : [0.88725] Combined Score: [0.10770]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 696/696 [00:12<00:00, 56.73it/s]\n",
      "100%|██████████| 174/174 [00:00<00:00, 299.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4], Train Loss : [0.46634] Val Loss : [0.37568] Val AUC : [0.92543] Combined Score: [0.08540]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 696/696 [00:12<00:00, 56.30it/s]\n",
      "100%|██████████| 174/174 [00:00<00:00, 290.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5], Train Loss : [0.41052] Val Loss : [0.33201] Val AUC : [0.94245] Combined Score: [0.07279]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 696/696 [00:11<00:00, 59.04it/s]\n",
      "100%|██████████| 174/174 [00:00<00:00, 303.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6], Train Loss : [0.37403] Val Loss : [0.30222] Val AUC : [0.95167] Combined Score: [0.06280]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 696/696 [00:12<00:00, 57.01it/s]\n",
      "100%|██████████| 174/174 [00:00<00:00, 283.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7], Train Loss : [0.34988] Val Loss : [0.28586] Val AUC : [0.95751] Combined Score: [0.05857]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 696/696 [00:11<00:00, 59.21it/s]\n",
      "100%|██████████| 174/174 [00:00<00:00, 306.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8], Train Loss : [0.32653] Val Loss : [0.25715] Val AUC : [0.96418] Combined Score: [0.04965]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 696/696 [00:11<00:00, 58.83it/s]\n",
      "100%|██████████| 174/174 [00:00<00:00, 265.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9], Train Loss : [0.31062] Val Loss : [0.24741] Val AUC : [0.96846] Combined Score: [0.04870]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 696/696 [00:11<00:00, 58.82it/s]\n",
      "100%|██████████| 174/174 [00:00<00:00, 338.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10], Train Loss : [0.29382] Val Loss : [0.23366] Val AUC : [0.97143] Combined Score: [0.04462]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1264/1264 [00:44<00:00, 28.68it/s]\n",
      "100%|██████████| 20/20 [00:00<00:00, 322.53it/s]\n",
      "100%|██████████| 716/716 [00:11<00:00, 59.69it/s]\n",
      "100%|██████████| 174/174 [00:00<00:00, 317.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], Train Loss : [0.27828] Val Loss : [0.22648] Val AUC : [0.97458] Combined Score: [0.04370]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 716/716 [00:12<00:00, 58.04it/s]\n",
      "100%|██████████| 174/174 [00:00<00:00, 320.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2], Train Loss : [0.26739] Val Loss : [0.22028] Val AUC : [0.97755] Combined Score: [0.04311]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 716/716 [00:12<00:00, 59.29it/s]\n",
      "100%|██████████| 174/174 [00:00<00:00, 328.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3], Train Loss : [0.25693] Val Loss : [0.20918] Val AUC : [0.97926] Combined Score: [0.04036]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 716/716 [00:12<00:00, 59.33it/s]\n",
      "100%|██████████| 174/174 [00:00<00:00, 288.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4], Train Loss : [0.24997] Val Loss : [0.20056] Val AUC : [0.98116] Combined Score: [0.03831]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 716/716 [00:12<00:00, 58.76it/s]\n",
      "100%|██████████| 174/174 [00:00<00:00, 294.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5], Train Loss : [0.24129] Val Loss : [0.19078] Val AUC : [0.98282] Combined Score: [0.03564]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 716/716 [00:12<00:00, 58.57it/s]\n",
      "100%|██████████| 174/174 [00:00<00:00, 261.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6], Train Loss : [0.23482] Val Loss : [0.18649] Val AUC : [0.98319] Combined Score: [0.03370]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 716/716 [00:12<00:00, 57.23it/s]\n",
      "100%|██████████| 174/174 [00:00<00:00, 284.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7], Train Loss : [0.22982] Val Loss : [0.18193] Val AUC : [0.98460] Combined Score: [0.03361]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 716/716 [00:12<00:00, 59.22it/s]\n",
      "100%|██████████| 174/174 [00:00<00:00, 287.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8], Train Loss : [0.22343] Val Loss : [0.17411] Val AUC : [0.98517] Combined Score: [0.03117]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 716/716 [00:12<00:00, 59.50it/s]\n",
      "100%|██████████| 174/174 [00:00<00:00, 315.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9], Train Loss : [0.21432] Val Loss : [0.17081] Val AUC : [0.98645] Combined Score: [0.03060]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 716/716 [00:12<00:00, 59.04it/s]\n",
      "100%|██████████| 174/174 [00:00<00:00, 326.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10], Train Loss : [0.21156] Val Loss : [0.17021] Val AUC : [0.98680] Combined Score: [0.03117]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50000it [26:21, 31.61it/s]\n",
      "100%|██████████| 782/782 [00:02<00:00, 368.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Combined Score before test: 0.43873\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import librosa\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import warnings\n",
    "from sklearn.metrics import roc_auc_score, mean_squared_error\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "class Config:\n",
    "    SR = 32000\n",
    "    N_MFCC = 13\n",
    "    ROOT_FOLDER = './'\n",
    "    N_CLASSES = 2\n",
    "    BATCH_SIZE = 64\n",
    "    N_EPOCHS = 10\n",
    "    LR = 3e-4\n",
    "    SEED = 42\n",
    "\n",
    "CONFIG = Config()\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CONFIG.SEED)\n",
    "\n",
    "# 데이터 로드 및 전처리\n",
    "df = pd.read_csv('./train.csv')\n",
    "\n",
    "def label_encoder(column):\n",
    "    le = LabelEncoder().fit(column)\n",
    "    print(column.name, le.classes_)\n",
    "    return le.transform(column)\n",
    "\n",
    "df['class'] = label_encoder(df['label'])\n",
    "\n",
    "# MFCC 특징 추출\n",
    "def get_mfcc_feature(df, train_mode=True):\n",
    "    features = []\n",
    "    labels = []\n",
    "    for _, row in tqdm(df.iterrows()):\n",
    "        y, sr = librosa.load(row['path'], sr=CONFIG.SR)\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=CONFIG.N_MFCC)\n",
    "        mfcc = np.mean(mfcc.T, axis=0)\n",
    "        features.append(mfcc)\n",
    "\n",
    "        if train_mode:\n",
    "            label = row['class']\n",
    "            labels.append(label)\n",
    "\n",
    "    return features, labels\n",
    "\n",
    "features, labels = get_mfcc_feature(df, True)\n",
    "feature_df = pd.DataFrame({'features': features, 'class': labels})\n",
    "\n",
    "X = np.array(feature_df['features'].tolist())\n",
    "y = np.array(feature_df['class'].tolist())\n",
    "\n",
    "# 데이터 불균형 해결\n",
    "ros = RandomOverSampler(random_state=CONFIG.SEED)\n",
    "X_resampled, y_resampled = ros.fit_resample(X, y)\n",
    "y_resampled = torch.tensor(y_resampled).long()  # 정수형으로 변환\n",
    "y_resampled = torch.nn.functional.one_hot(y_resampled, num_classes=CONFIG.N_CLASSES).float()\n",
    "\n",
    "# 데이터 분할\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=CONFIG.SEED)\n",
    "\n",
    "# PyTorch Dataset 클래스 정의\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, mfcc, label):\n",
    "        self.mfcc = mfcc\n",
    "        self.label = label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mfcc)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.mfcc[index], self.label[index]\n",
    "\n",
    "train_dataset = CustomDataset(X_train, y_train)\n",
    "val_dataset = CustomDataset(X_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# MLP 모델 정의\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim=CONFIG.N_MFCC, hidden_dims=[128, 256, 128], output_dim=CONFIG.N_CLASSES):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dims[0])\n",
    "        self.fc2 = nn.Linear(hidden_dims[0], hidden_dims[1])\n",
    "        self.fc3 = nn.Linear(hidden_dims[1], hidden_dims[2])\n",
    "        self.fc4 = nn.Linear(hidden_dims[2], output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "# 평가 함수 정의\n",
    "def expected_calibration_error(y_true, y_prob, n_bins=10):\n",
    "    prob_true, prob_pred = calibration_curve(y_true, y_prob, n_bins=n_bins, strategy='uniform')\n",
    "    bin_totals = np.histogram(y_prob, bins=np.linspace(0, 1, n_bins + 1), density=False)[0]\n",
    "    non_empty_bins = bin_totals > 0\n",
    "    bin_weights = bin_totals / len(y_prob)\n",
    "    bin_weights = bin_weights[non_empty_bins]\n",
    "    prob_true = prob_true[:len(bin_weights)]\n",
    "    prob_pred = prob_pred[:len(bin_weights)]\n",
    "    ece = np.sum(bin_weights * np.abs(prob_true - prob_pred))\n",
    "    return ece\n",
    "\n",
    "def auc_brier_ece(answer_df, submission_df):\n",
    "    # Check for missing values in submission_df\n",
    "    if submission_df.isnull().values.any():\n",
    "        raise ValueError(\"The submission dataframe contains missing values.\")\n",
    "\n",
    "    # Check if the number and names of columns are the same in both dataframes\n",
    "    if len(answer_df.columns) != len(submission_df.columns) or not all(answer_df.columns == submission_df.columns):\n",
    "        raise ValueError(\"The columns of the answer and submission dataframes do not match.\")\n",
    "        \n",
    "    submission_df = submission_df[submission_df.index.isin(answer_df.index)]\n",
    "    submission_df.index = range(submission_df.shape[0])\n",
    "    \n",
    "    # Calculate AUC for each class\n",
    "    auc_scores = []\n",
    "    for column in answer_df.columns:\n",
    "        y_true = answer_df[column]\n",
    "        y_scores = submission_df[column]\n",
    "        auc = roc_auc_score(y_true, y_scores)\n",
    "        auc_scores.append(auc)\n",
    "\n",
    "    # Calculate mean AUC\n",
    "    mean_auc = np.mean(auc_scores)\n",
    "\n",
    "    brier_scores = []\n",
    "    ece_scores = []\n",
    "    \n",
    "    # Calculate Brier Score and ECE for each class\n",
    "    for column in answer_df.columns:\n",
    "        y_true = answer_df[column].values\n",
    "        y_prob = submission_df[column].values\n",
    "        \n",
    "        # Brier Score\n",
    "        brier = mean_squared_error(y_true, y_prob)\n",
    "        brier_scores.append(brier)\n",
    "        \n",
    "        # ECE\n",
    "        ece = expected_calibration_error(y_true, y_prob)\n",
    "        ece_scores.append(ece)\n",
    "    \n",
    "    # Calculate mean Brier Score and mean ECE\n",
    "    mean_brier = np.mean(brier_scores)\n",
    "    mean_ece = np.mean(ece_scores)\n",
    "    \n",
    "    # Calculate combined score\n",
    "    combined_score = 0.5 * (1 - mean_auc) + 0.25 * mean_brier + 0.25 * mean_ece\n",
    "    \n",
    "    return combined_score\n",
    "\n",
    "# 모델 학습 함수 정의\n",
    "def train(model, optimizer, train_loader, val_loader, device):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss().to(device)\n",
    "    \n",
    "    best_val_score = 0\n",
    "    best_model = None\n",
    "    val_labels = y_val.cpu().numpy()\n",
    "    \n",
    "    for epoch in range(1, CONFIG.N_EPOCHS+1):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        for features, labels in tqdm(iter(train_loader)):\n",
    "            features = features.float().to(device)\n",
    "            labels = labels.float().to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model(features)\n",
    "            loss = criterion(output, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss.append(loss.item())\n",
    "                    \n",
    "        _val_loss, _val_score, val_outputs = validation(model, criterion, val_loader, device)\n",
    "        _train_loss = np.mean(train_loss)\n",
    "        \n",
    "        combined_score = auc_brier_ece(pd.DataFrame(val_labels), pd.DataFrame(val_outputs))\n",
    "        \n",
    "        print(f'Epoch [{epoch}], Train Loss : [{_train_loss:.5f}] Val Loss : [{_val_loss:.5f}] Val AUC : [{_val_score:.5f}] Combined Score: [{combined_score:.5f}]')\n",
    "            \n",
    "        if best_val_score < _val_score:\n",
    "            best_val_score = _val_score\n",
    "            best_model = model\n",
    "    \n",
    "    return best_model\n",
    "\n",
    "def multiLabel_AUC(y_true, y_scores):\n",
    "    auc_scores = []\n",
    "    for i in range(y_true.shape[1]):\n",
    "        auc = roc_auc_score(y_true[:, i], y_scores[:, i])\n",
    "        auc_scores.append(auc)\n",
    "    mean_auc_score = np.mean(auc_scores)\n",
    "    return mean_auc_score\n",
    "    \n",
    "def validation(model, criterion, val_loader, device):\n",
    "    model.eval()\n",
    "    val_loss, all_labels, all_probs = [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for features, labels in tqdm(iter(val_loader)):\n",
    "            features = features.float().to(device)\n",
    "            labels = labels.float().to(device)\n",
    "            \n",
    "            probs = model(features)\n",
    "            \n",
    "            loss = criterion(probs, labels)\n",
    "\n",
    "            val_loss.append(loss.item())\n",
    "\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "            all_probs.append(probs.cpu().numpy())\n",
    "        \n",
    "        _val_loss = np.mean(val_loss)\n",
    "\n",
    "        all_labels = np.concatenate(all_labels, axis=0)\n",
    "        all_probs = np.concatenate(all_probs, axis=0)\n",
    "        \n",
    "        auc_score = multiLabel_AUC(all_labels, all_probs)\n",
    "    \n",
    "    return _val_loss, auc_score, all_probs\n",
    "\n",
    "model = MLP()\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr = CONFIG.LR)\n",
    "\n",
    "infer_model = train(model, optimizer, train_loader, val_loader, device)\n",
    "\n",
    "# Unlabeled 데이터에 대해 pseudo-labeling\n",
    "unlabeled_path = './unlabeled_data'\n",
    "unlabeled_files = [os.path.join(unlabeled_path, f) for f in os.listdir(unlabeled_path) if f.endswith('.ogg')]\n",
    "\n",
    "def get_mfcc_feature_from_files(file_paths):\n",
    "    features = []\n",
    "    for file_path in tqdm(file_paths):\n",
    "        y, sr = librosa.load(file_path, sr=CONFIG.SR)\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=CONFIG.N_MFCC)\n",
    "        mfcc = np.mean(mfcc.T, axis=0)\n",
    "        features.append(mfcc)\n",
    "    return features\n",
    "\n",
    "unlabeled_features = get_mfcc_feature_from_files(unlabeled_files)\n",
    "unlabeled_features = np.array(unlabeled_features)\n",
    "unlabeled_dataset = CustomDataset(unlabeled_features, torch.zeros((len(unlabeled_features), CONFIG.N_CLASSES)))\n",
    "unlabeled_loader = DataLoader(unlabeled_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=False)\n",
    "\n",
    "def pseudo_labeling(model, loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    pseudo_labels = []\n",
    "    with torch.no_grad():\n",
    "        for features, _ in tqdm(iter(loader)):\n",
    "            features = features.float().to(device)\n",
    "            probs = model(features)\n",
    "            pseudo_labels.append(probs.cpu().detach().numpy())\n",
    "    return np.concatenate(pseudo_labels, axis=0)\n",
    "\n",
    "pseudo_labels = pseudo_labeling(infer_model, unlabeled_loader, device)\n",
    "pseudo_labels = (pseudo_labels > 0.5).astype(int)  # 임계값 0.5를 사용하여 pseudo-label 생성\n",
    "\n",
    "# Pseudo-labeled 데이터를 학습 데이터에 추가\n",
    "pseudo_labeled_dataset = CustomDataset(unlabeled_features, torch.tensor(pseudo_labels).float())\n",
    "train_dataset_combined = torch.utils.data.ConcatDataset([train_dataset, pseudo_labeled_dataset])\n",
    "train_loader_combined = DataLoader(train_dataset_combined, batch_size=CONFIG.BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# 모델을 pseudo-labeled 데이터를 포함하여 재학습\n",
    "infer_model = train(model, optimizer, train_loader_combined, val_loader, device)\n",
    "\n",
    "# 테스트 데이터 예측\n",
    "test = pd.read_csv('./test.csv')\n",
    "test_features, _ = get_mfcc_feature(test, False)\n",
    "test_features = np.array(test_features)\n",
    "test_dataset = CustomDataset(test_features, torch.zeros((len(test_features), CONFIG.N_CLASSES)))\n",
    "test_loader = DataLoader(test_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=False)\n",
    "\n",
    "def inference(model, test_loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for features, _ in tqdm(iter(test_loader)):\n",
    "            features = features.float().to(device)\n",
    "            probs = model(features)\n",
    "            probs  = probs.cpu().detach().numpy()\n",
    "            predictions += probs.tolist()\n",
    "    return predictions\n",
    "\n",
    "preds = inference(infer_model, test_loader, device)\n",
    "\n",
    "submit = pd.read_csv('./sample_submission.csv')\n",
    "submit.iloc[:, 1:] = preds\n",
    "submit.head()\n",
    "\n",
    "combined_score_before_test = auc_brier_ece(pd.DataFrame(y_val.cpu().numpy()), pd.DataFrame(preds))\n",
    "print(f'Final Combined Score before test: {combined_score_before_test:.5f}')\n",
    "\n",
    "submit.to_csv('./wy_origin2.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label ['fake' 'real']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "55438it [24:16, 38.07it/s]\n",
      "100%|██████████| 696/696 [00:09<00:00, 70.02it/s]\n",
      "100%|██████████| 174/174 [00:00<00:00, 334.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], Train Loss : [13.76120] Val Loss : [0.68369] Val AUC : [0.68417] Combined Score: [0.24231]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 696/696 [00:09<00:00, 70.22it/s]\n",
      "100%|██████████| 174/174 [00:00<00:00, 382.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2], Train Loss : [0.76569] Val Loss : [0.67165] Val AUC : [0.77531] Combined Score: [0.22073]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 696/696 [00:10<00:00, 65.38it/s]\n",
      "100%|██████████| 174/174 [00:00<00:00, 296.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3], Train Loss : [0.65150] Val Loss : [0.58006] Val AUC : [0.81407] Combined Score: [0.17394]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 696/696 [00:11<00:00, 60.57it/s]\n",
      "100%|██████████| 174/174 [00:00<00:00, 379.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4], Train Loss : [0.58384] Val Loss : [0.47576] Val AUC : [0.88562] Combined Score: [0.12220]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 696/696 [00:11<00:00, 62.40it/s]\n",
      "100%|██████████| 174/174 [00:00<00:00, 339.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5], Train Loss : [0.49719] Val Loss : [0.38398] Val AUC : [0.92688] Combined Score: [0.08830]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 696/696 [00:10<00:00, 63.88it/s]\n",
      "100%|██████████| 174/174 [00:00<00:00, 307.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6], Train Loss : [0.43591] Val Loss : [0.34149] Val AUC : [0.94184] Combined Score: [0.07572]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 696/696 [00:10<00:00, 63.76it/s]\n",
      "100%|██████████| 174/174 [00:00<00:00, 352.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7], Train Loss : [0.39083] Val Loss : [0.31077] Val AUC : [0.94933] Combined Score: [0.06575]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 696/696 [00:10<00:00, 64.92it/s]\n",
      "100%|██████████| 174/174 [00:00<00:00, 348.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8], Train Loss : [0.36146] Val Loss : [0.29272] Val AUC : [0.95634] Combined Score: [0.06228]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 696/696 [00:11<00:00, 60.24it/s]\n",
      "100%|██████████| 174/174 [00:00<00:00, 280.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9], Train Loss : [0.34131] Val Loss : [0.28096] Val AUC : [0.95814] Combined Score: [0.05738]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 696/696 [00:10<00:00, 64.20it/s]\n",
      "100%|██████████| 174/174 [00:00<00:00, 319.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10], Train Loss : [0.32129] Val Loss : [0.26162] Val AUC : [0.96514] Combined Score: [0.05213]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 696/696 [00:11<00:00, 62.94it/s]\n",
      "100%|██████████| 174/174 [00:00<00:00, 346.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11], Train Loss : [0.30790] Val Loss : [0.24941] Val AUC : [0.96847] Combined Score: [0.04938]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 696/696 [00:11<00:00, 60.11it/s]\n",
      "100%|██████████| 174/174 [00:00<00:00, 381.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12], Train Loss : [0.29026] Val Loss : [0.24346] Val AUC : [0.97440] Combined Score: [0.05032]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 696/696 [00:10<00:00, 64.03it/s]\n",
      "100%|██████████| 174/174 [00:00<00:00, 390.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13], Train Loss : [0.28270] Val Loss : [0.23494] Val AUC : [0.97466] Combined Score: [0.04653]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 696/696 [00:10<00:00, 64.39it/s]\n",
      "100%|██████████| 174/174 [00:00<00:00, 306.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14], Train Loss : [0.26548] Val Loss : [0.22550] Val AUC : [0.97814] Combined Score: [0.04516]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 696/696 [00:11<00:00, 61.73it/s]\n",
      "100%|██████████| 174/174 [00:00<00:00, 356.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15], Train Loss : [0.25931] Val Loss : [0.21376] Val AUC : [0.98015] Combined Score: [0.04245]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 696/696 [00:10<00:00, 64.41it/s]\n",
      "100%|██████████| 174/174 [00:00<00:00, 353.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16], Train Loss : [0.24903] Val Loss : [0.21477] Val AUC : [0.98101] Combined Score: [0.04249]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 696/696 [00:10<00:00, 65.69it/s]\n",
      "100%|██████████| 174/174 [00:00<00:00, 357.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17], Train Loss : [0.24253] Val Loss : [0.21053] Val AUC : [0.98187] Combined Score: [0.04202]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 696/696 [00:11<00:00, 62.47it/s]\n",
      "100%|██████████| 174/174 [00:00<00:00, 388.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18], Train Loss : [0.23560] Val Loss : [0.21224] Val AUC : [0.98318] Combined Score: [0.04330]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 696/696 [00:10<00:00, 65.99it/s]\n",
      "100%|██████████| 174/174 [00:00<00:00, 350.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19], Train Loss : [0.23001] Val Loss : [0.20410] Val AUC : [0.98349] Combined Score: [0.03935]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 696/696 [00:10<00:00, 65.49it/s]\n",
      "100%|██████████| 174/174 [00:00<00:00, 368.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20], Train Loss : [0.22663] Val Loss : [0.18932] Val AUC : [0.98558] Combined Score: [0.03675]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1264/1264 [00:37<00:00, 33.74it/s]\n",
      "100%|██████████| 20/20 [00:00<00:00, 333.33it/s]\n",
      "100%|██████████| 716/716 [00:11<00:00, 61.62it/s]\n",
      "100%|██████████| 174/174 [00:00<00:00, 350.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], Train Loss : [0.21994] Val Loss : [0.19883] Val AUC : [0.98386] Combined Score: [0.03820]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 716/716 [00:11<00:00, 63.71it/s]\n",
      "100%|██████████| 174/174 [00:00<00:00, 358.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2], Train Loss : [0.21446] Val Loss : [0.18575] Val AUC : [0.98705] Combined Score: [0.03661]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 716/716 [00:11<00:00, 60.29it/s]\n",
      "100%|██████████| 174/174 [00:00<00:00, 348.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3], Train Loss : [0.21128] Val Loss : [0.18172] Val AUC : [0.98725] Combined Score: [0.03538]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 716/716 [00:11<00:00, 63.26it/s]\n",
      "100%|██████████| 174/174 [00:00<00:00, 358.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4], Train Loss : [0.20394] Val Loss : [0.19185] Val AUC : [0.98587] Combined Score: [0.03767]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 716/716 [00:11<00:00, 62.08it/s]\n",
      "100%|██████████| 174/174 [00:00<00:00, 370.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5], Train Loss : [0.20456] Val Loss : [0.18776] Val AUC : [0.98770] Combined Score: [0.03714]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 716/716 [00:12<00:00, 58.93it/s]\n",
      "100%|██████████| 174/174 [00:00<00:00, 390.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6], Train Loss : [0.20011] Val Loss : [0.17681] Val AUC : [0.98797] Combined Score: [0.03449]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 716/716 [00:11<00:00, 62.88it/s]\n",
      "100%|██████████| 174/174 [00:00<00:00, 287.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7], Train Loss : [0.19806] Val Loss : [0.17749] Val AUC : [0.98845] Combined Score: [0.03418]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 716/716 [00:11<00:00, 62.37it/s]\n",
      "100%|██████████| 174/174 [00:00<00:00, 323.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8], Train Loss : [0.19395] Val Loss : [0.17574] Val AUC : [0.98730] Combined Score: [0.03308]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 716/716 [00:11<00:00, 60.88it/s]\n",
      "100%|██████████| 174/174 [00:00<00:00, 387.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9], Train Loss : [0.18994] Val Loss : [0.16444] Val AUC : [0.98917] Combined Score: [0.03118]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 716/716 [00:11<00:00, 62.78it/s]\n",
      "100%|██████████| 174/174 [00:00<00:00, 388.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10], Train Loss : [0.19236] Val Loss : [0.17345] Val AUC : [0.98833] Combined Score: [0.03333]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 716/716 [00:11<00:00, 62.61it/s]\n",
      "100%|██████████| 174/174 [00:00<00:00, 331.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11], Train Loss : [0.18844] Val Loss : [0.16507] Val AUC : [0.98999] Combined Score: [0.03195]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 716/716 [00:11<00:00, 61.75it/s]\n",
      "100%|██████████| 174/174 [00:00<00:00, 320.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12], Train Loss : [0.18367] Val Loss : [0.15581] Val AUC : [0.98969] Combined Score: [0.02859]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 716/716 [00:11<00:00, 63.76it/s]\n",
      "100%|██████████| 174/174 [00:00<00:00, 371.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13], Train Loss : [0.18518] Val Loss : [0.15952] Val AUC : [0.98963] Combined Score: [0.03010]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 716/716 [00:11<00:00, 61.59it/s]\n",
      "100%|██████████| 174/174 [00:00<00:00, 305.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14], Train Loss : [0.18593] Val Loss : [0.15872] Val AUC : [0.98986] Combined Score: [0.03057]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 716/716 [00:10<00:00, 65.17it/s]\n",
      "100%|██████████| 174/174 [00:00<00:00, 351.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15], Train Loss : [0.17915] Val Loss : [0.15433] Val AUC : [0.99024] Combined Score: [0.02840]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 716/716 [00:11<00:00, 64.90it/s]\n",
      "100%|██████████| 174/174 [00:00<00:00, 368.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16], Train Loss : [0.17972] Val Loss : [0.15008] Val AUC : [0.99096] Combined Score: [0.02739]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 716/716 [00:11<00:00, 61.88it/s]\n",
      "100%|██████████| 174/174 [00:00<00:00, 377.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17], Train Loss : [0.17757] Val Loss : [0.14832] Val AUC : [0.99075] Combined Score: [0.02664]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 716/716 [00:11<00:00, 64.69it/s]\n",
      "100%|██████████| 174/174 [00:00<00:00, 368.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18], Train Loss : [0.17854] Val Loss : [0.15655] Val AUC : [0.99022] Combined Score: [0.02873]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 716/716 [00:11<00:00, 63.84it/s]\n",
      "100%|██████████| 174/174 [00:00<00:00, 347.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19], Train Loss : [0.17488] Val Loss : [0.14960] Val AUC : [0.99080] Combined Score: [0.02729]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 716/716 [00:11<00:00, 62.14it/s]\n",
      "100%|██████████| 174/174 [00:00<00:00, 379.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20], Train Loss : [0.17354] Val Loss : [0.15479] Val AUC : [0.99093] Combined Score: [0.02924]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50000it [24:46, 33.64it/s]\n",
      "100%|██████████| 782/782 [00:02<00:00, 355.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Combined Score before test: 0.43518\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import librosa\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import warnings\n",
    "from sklearn.metrics import roc_auc_score, mean_squared_error\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "class Config:\n",
    "    SR = 32000\n",
    "    N_MFCC = 13\n",
    "    ROOT_FOLDER = './'\n",
    "    N_CLASSES = 2\n",
    "    BATCH_SIZE = 64\n",
    "    N_EPOCHS = 20\n",
    "    LR = 3e-4\n",
    "    SEED = 42\n",
    "    DROPOUT_RATE = 0.5\n",
    "    HIDDEN_DIMS = [128, 256, 128]\n",
    "    INITIALIZATION = 'xavier'\n",
    "    ACTIVATION_FUNCTION = 'relu'\n",
    "\n",
    "CONFIG = Config()\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CONFIG.SEED)\n",
    "\n",
    "# 데이터 로드 및 전처리\n",
    "df = pd.read_csv('./train.csv')\n",
    "\n",
    "def label_encoder(column):\n",
    "    le = LabelEncoder().fit(column)\n",
    "    print(column.name, le.classes_)\n",
    "    return le.transform(column)\n",
    "\n",
    "df['class'] = label_encoder(df['label'])\n",
    "\n",
    "# MFCC 특징 추출\n",
    "def get_mfcc_feature(df, train_mode=True):\n",
    "    features = []\n",
    "    labels = []\n",
    "    for _, row in tqdm(df.iterrows()):\n",
    "        y, sr = librosa.load(row['path'], sr=CONFIG.SR)\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=CONFIG.N_MFCC)\n",
    "        mfcc = np.mean(mfcc.T, axis=0)\n",
    "        features.append(mfcc)\n",
    "\n",
    "        if train_mode:\n",
    "            label = row['class']\n",
    "            labels.append(label)\n",
    "\n",
    "    return features, labels\n",
    "\n",
    "features, labels = get_mfcc_feature(df, True)\n",
    "feature_df = pd.DataFrame({'features': features, 'class': labels})\n",
    "\n",
    "X = np.array(feature_df['features'].tolist())\n",
    "y = np.array(feature_df['class'].tolist())\n",
    "\n",
    "# 데이터 불균형 해결\n",
    "ros = RandomOverSampler(random_state=CONFIG.SEED)\n",
    "X_resampled, y_resampled = ros.fit_resample(X, y)\n",
    "y_resampled = torch.tensor(y_resampled).long()  # 정수형으로 변환\n",
    "y_resampled = torch.nn.functional.one_hot(y_resampled, num_classes=CONFIG.N_CLASSES).float()\n",
    "\n",
    "# 데이터 분할\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=CONFIG.SEED)\n",
    "\n",
    "# PyTorch Dataset 클래스 정의\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, mfcc, label):\n",
    "        self.mfcc = mfcc\n",
    "        self.label = label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mfcc)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.mfcc[index], self.label[index]\n",
    "\n",
    "train_dataset = CustomDataset(X_train, y_train)\n",
    "val_dataset = CustomDataset(X_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# MLP 모델 정의\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim=CONFIG.N_MFCC, hidden_dims=CONFIG.HIDDEN_DIMS, output_dim=CONFIG.N_CLASSES):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dims[0])\n",
    "        self.fc2 = nn.Linear(hidden_dims[0], hidden_dims[1])\n",
    "        self.fc3 = nn.Linear(hidden_dims[1], hidden_dims[2])\n",
    "        self.fc4 = nn.Linear(hidden_dims[2], output_dim)\n",
    "        \n",
    "        if CONFIG.INITIALIZATION == 'xavier':\n",
    "            nn.init.xavier_uniform_(self.fc1.weight)\n",
    "            nn.init.xavier_uniform_(self.fc2.weight)\n",
    "            nn.init.xavier_uniform_(self.fc3.weight)\n",
    "            nn.init.xavier_uniform_(self.fc4.weight)\n",
    "        \n",
    "        self.activation = nn.ReLU() if CONFIG.ACTIVATION_FUNCTION == 'relu' else nn.Tanh()\n",
    "        self.dropout = nn.Dropout(CONFIG.DROPOUT_RATE)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.activation(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "# 평가 함수 정의\n",
    "def expected_calibration_error(y_true, y_prob, n_bins=10):\n",
    "    prob_true, prob_pred = calibration_curve(y_true, y_prob, n_bins=n_bins, strategy='uniform')\n",
    "    bin_totals = np.histogram(y_prob, bins=np.linspace(0, 1, n_bins + 1), density=False)[0]\n",
    "    non_empty_bins = bin_totals > 0\n",
    "    bin_weights = bin_totals / len(y_prob)\n",
    "    bin_weights = bin_weights[non_empty_bins]\n",
    "    prob_true = prob_true[:len(bin_weights)]\n",
    "    prob_pred = prob_pred[:len(bin_weights)]\n",
    "    ece = np.sum(bin_weights * np.abs(prob_true - prob_pred))\n",
    "    return ece\n",
    "\n",
    "def auc_brier_ece(answer_df, submission_df):\n",
    "    # Check for missing values in submission_df\n",
    "    if submission_df.isnull().values.any():\n",
    "        raise ValueError(\"The submission dataframe contains missing values.\")\n",
    "\n",
    "    # Check if the number and names of columns are the same in both dataframes\n",
    "    if len(answer_df.columns) != len(submission_df.columns) or not all(answer_df.columns == submission_df.columns):\n",
    "        raise ValueError(\"The columns of the answer and submission dataframes do not match.\")\n",
    "        \n",
    "    submission_df = submission_df[submission_df.index.isin(answer_df.index)]\n",
    "    submission_df.index = range(submission_df.shape[0])\n",
    "    \n",
    "    # Calculate AUC for each class\n",
    "    auc_scores = []\n",
    "    for column in answer_df.columns:\n",
    "        y_true = answer_df[column]\n",
    "        y_scores = submission_df[column]\n",
    "        auc = roc_auc_score(y_true, y_scores)\n",
    "        auc_scores.append(auc)\n",
    "\n",
    "    # Calculate mean AUC\n",
    "    mean_auc = np.mean(auc_scores)\n",
    "\n",
    "    brier_scores = []\n",
    "    ece_scores = []\n",
    "    \n",
    "    # Calculate Brier Score and ECE for each class\n",
    "    for column in answer_df.columns:\n",
    "        y_true = answer_df[column].values\n",
    "        y_prob = submission_df[column].values\n",
    "        \n",
    "        # Brier Score\n",
    "        brier = mean_squared_error(y_true, y_prob)\n",
    "        brier_scores.append(brier)\n",
    "        \n",
    "        # ECE\n",
    "        ece = expected_calibration_error(y_true, y_prob)\n",
    "        ece_scores.append(ece)\n",
    "    \n",
    "    # Calculate mean Brier Score and mean ECE\n",
    "    mean_brier = np.mean(brier_scores)\n",
    "    mean_ece = np.mean(ece_scores)\n",
    "    \n",
    "    # Calculate combined score\n",
    "    combined_score = 0.5 * (1 - mean_auc) + 0.25 * mean_brier + 0.25 * mean_ece\n",
    "    \n",
    "    return combined_score\n",
    "\n",
    "# 모델 학습 함수 정의\n",
    "def train(model, optimizer, train_loader, val_loader, device):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss().to(device)\n",
    "    \n",
    "    best_val_score = 0\n",
    "    best_model = None\n",
    "    val_labels = y_val.cpu().numpy()\n",
    "    \n",
    "    for epoch in range(1, CONFIG.N_EPOCHS+1):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        for features, labels in tqdm(iter(train_loader)):\n",
    "            features = features.float().to(device)\n",
    "            labels = labels.float().to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model(features)\n",
    "            loss = criterion(output, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss.append(loss.item())\n",
    "                    \n",
    "        _val_loss, _val_score, val_outputs = validation(model, criterion, val_loader, device)\n",
    "        _train_loss = np.mean(train_loss)\n",
    "        \n",
    "        combined_score = auc_brier_ece(pd.DataFrame(val_labels), pd.DataFrame(val_outputs))\n",
    "        \n",
    "        print(f'Epoch [{epoch}], Train Loss : [{_train_loss:.5f}] Val Loss : [{_val_loss:.5f}] Val AUC : [{_val_score:.5f}] Combined Score: [{combined_score:.5f}]')\n",
    "            \n",
    "        if best_val_score < _val_score:\n",
    "            best_val_score = _val_score\n",
    "            best_model = model\n",
    "    \n",
    "    return best_model\n",
    "\n",
    "def multiLabel_AUC(y_true, y_scores):\n",
    "    auc_scores = []\n",
    "    for i in range(y_true.shape[1]):\n",
    "        auc = roc_auc_score(y_true[:, i], y_scores[:, i])\n",
    "        auc_scores.append(auc)\n",
    "    mean_auc_score = np.mean(auc_scores)\n",
    "    return mean_auc_score\n",
    "    \n",
    "def validation(model, criterion, val_loader, device):\n",
    "    model.eval()\n",
    "    val_loss, all_labels, all_probs = [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for features, labels in tqdm(iter(val_loader)):\n",
    "            features = features.float().to(device)\n",
    "            labels = labels.float().to(device)\n",
    "            \n",
    "            probs = model(features)\n",
    "            \n",
    "            loss = criterion(probs, labels)\n",
    "\n",
    "            val_loss.append(loss.item())\n",
    "\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "            all_probs.append(probs.cpu().numpy())\n",
    "        \n",
    "        _val_loss = np.mean(val_loss)\n",
    "\n",
    "        all_labels = np.concatenate(all_labels, axis=0)\n",
    "        all_probs = np.concatenate(all_probs, axis=0)\n",
    "        \n",
    "        auc_score = multiLabel_AUC(all_labels, all_probs)\n",
    "    \n",
    "    return _val_loss, auc_score, all_probs\n",
    "\n",
    "model = MLP()\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr = CONFIG.LR)\n",
    "\n",
    "infer_model = train(model, optimizer, train_loader, val_loader, device)\n",
    "\n",
    "# Unlabeled 데이터에 대해 pseudo-labeling\n",
    "unlabeled_path = './unlabeled_data'\n",
    "unlabeled_files = [os.path.join(unlabeled_path, f) for f in os.listdir(unlabeled_path) if f.endswith('.ogg')]\n",
    "\n",
    "def get_mfcc_feature_from_files(file_paths):\n",
    "    features = []\n",
    "    for file_path in tqdm(file_paths):\n",
    "        y, sr = librosa.load(file_path, sr=CONFIG.SR)\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=CONFIG.N_MFCC)\n",
    "        mfcc = np.mean(mfcc.T, axis=0)\n",
    "        features.append(mfcc)\n",
    "    return features\n",
    "\n",
    "unlabeled_features = get_mfcc_feature_from_files(unlabeled_files)\n",
    "unlabeled_features = np.array(unlabeled_features)\n",
    "unlabeled_dataset = CustomDataset(unlabeled_features, torch.zeros((len(unlabeled_features), CONFIG.N_CLASSES)))\n",
    "unlabeled_loader = DataLoader(unlabeled_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=False)\n",
    "\n",
    "def pseudo_labeling(model, loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    pseudo_labels = []\n",
    "    with torch.no_grad():\n",
    "        for features, _ in tqdm(iter(loader)):\n",
    "            features = features.float().to(device)\n",
    "            probs = model(features)\n",
    "            pseudo_labels.append(probs.cpu().detach().numpy())\n",
    "    return np.concatenate(pseudo_labels, axis=0)\n",
    "\n",
    "pseudo_labels = pseudo_labeling(infer_model, unlabeled_loader, device)\n",
    "pseudo_labels = (pseudo_labels > 0.5).astype(int)  # 임계값 0.5를 사용하여 pseudo-label 생성\n",
    "\n",
    "# Pseudo-labeled 데이터를 학습 데이터에 추가\n",
    "pseudo_labeled_dataset = CustomDataset(unlabeled_features, torch.tensor(pseudo_labels).float())\n",
    "train_dataset_combined = torch.utils.data.ConcatDataset([train_dataset, pseudo_labeled_dataset])\n",
    "train_loader_combined = DataLoader(train_dataset_combined, batch_size=CONFIG.BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# 모델을 pseudo-labeled 데이터를 포함하여 재학습\n",
    "infer_model = train(model, optimizer, train_loader_combined, val_loader, device)\n",
    "\n",
    "# 테스트 데이터 예측\n",
    "test = pd.read_csv('./test.csv')\n",
    "test_features, _ = get_mfcc_feature(test, False)\n",
    "test_features = np.array(test_features)\n",
    "test_dataset = CustomDataset(test_features, torch.zeros((len(test_features), CONFIG.N_CLASSES)))\n",
    "test_loader = DataLoader(test_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=False)\n",
    "\n",
    "def inference(model, test_loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for features, _ in tqdm(iter(test_loader)):\n",
    "            features = features.float().to(device)\n",
    "            probs = model(features)\n",
    "            probs  = probs.cpu().detach().numpy()\n",
    "            predictions += probs.tolist()\n",
    "    return predictions\n",
    "\n",
    "preds = inference(infer_model, test_loader, device)\n",
    "\n",
    "submit = pd.read_csv('./sample_submission.csv')\n",
    "submit.iloc[:, 1:] = preds\n",
    "submit.head()\n",
    "\n",
    "combined_score_before_test = auc_brier_ece(pd.DataFrame(y_val.cpu().numpy()), pd.DataFrame(preds))\n",
    "print(f'Final Combined Score before test: {combined_score_before_test:.5f}')\n",
    "\n",
    "submit.to_csv('./wy_origin3.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label ['fake' 'real']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "55438it [26:19, 35.11it/s]\n",
      "100%|██████████| 696/696 [00:12<00:00, 55.29it/s]\n",
      "100%|██████████| 174/174 [00:00<00:00, 205.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], Train Loss : [13.76120] Val Loss : [0.68369] Val AUC : [0.68417] Combined Score: [0.24231]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 696/696 [00:12<00:00, 54.83it/s]\n",
      "100%|██████████| 174/174 [00:00<00:00, 222.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2], Train Loss : [0.76569] Val Loss : [0.67165] Val AUC : [0.77531] Combined Score: [0.22073]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 696/696 [00:13<00:00, 50.57it/s]\n",
      "100%|██████████| 174/174 [00:00<00:00, 311.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3], Train Loss : [0.65150] Val Loss : [0.58006] Val AUC : [0.81407] Combined Score: [0.17394]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 696/696 [00:14<00:00, 48.94it/s]\n",
      "100%|██████████| 174/174 [00:00<00:00, 311.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4], Train Loss : [0.58384] Val Loss : [0.47576] Val AUC : [0.88562] Combined Score: [0.12220]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 696/696 [00:13<00:00, 50.06it/s]\n",
      "100%|██████████| 174/174 [00:00<00:00, 308.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5], Train Loss : [0.49719] Val Loss : [0.38398] Val AUC : [0.92688] Combined Score: [0.08830]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 696/696 [00:13<00:00, 51.94it/s]\n",
      "100%|██████████| 174/174 [00:00<00:00, 313.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6], Train Loss : [0.43591] Val Loss : [0.34149] Val AUC : [0.94184] Combined Score: [0.07572]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 696/696 [00:13<00:00, 51.71it/s]\n",
      "100%|██████████| 174/174 [00:00<00:00, 320.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7], Train Loss : [0.39083] Val Loss : [0.31077] Val AUC : [0.94933] Combined Score: [0.06575]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 696/696 [00:14<00:00, 46.84it/s]\n",
      "100%|██████████| 174/174 [00:00<00:00, 228.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8], Train Loss : [0.36146] Val Loss : [0.29272] Val AUC : [0.95634] Combined Score: [0.06228]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 696/696 [00:14<00:00, 47.32it/s]\n",
      "100%|██████████| 174/174 [00:00<00:00, 261.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9], Train Loss : [0.34131] Val Loss : [0.28096] Val AUC : [0.95814] Combined Score: [0.05738]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 696/696 [00:13<00:00, 50.43it/s]\n",
      "100%|██████████| 174/174 [00:00<00:00, 290.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10], Train Loss : [0.32129] Val Loss : [0.26162] Val AUC : [0.96514] Combined Score: [0.05213]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 696/696 [00:13<00:00, 50.47it/s]\n",
      "100%|██████████| 174/174 [00:00<00:00, 294.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11], Train Loss : [0.30790] Val Loss : [0.24941] Val AUC : [0.96847] Combined Score: [0.04938]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 696/696 [00:13<00:00, 51.10it/s]\n",
      "100%|██████████| 174/174 [00:00<00:00, 270.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12], Train Loss : [0.29026] Val Loss : [0.24346] Val AUC : [0.97440] Combined Score: [0.05032]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 696/696 [00:13<00:00, 50.60it/s]\n",
      "100%|██████████| 174/174 [00:00<00:00, 293.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13], Train Loss : [0.28270] Val Loss : [0.23494] Val AUC : [0.97466] Combined Score: [0.04653]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 696/696 [00:13<00:00, 51.63it/s]\n",
      "100%|██████████| 174/174 [00:00<00:00, 322.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14], Train Loss : [0.26548] Val Loss : [0.22550] Val AUC : [0.97814] Combined Score: [0.04516]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 696/696 [00:13<00:00, 52.14it/s]\n",
      "100%|██████████| 174/174 [00:00<00:00, 292.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15], Train Loss : [0.25931] Val Loss : [0.21376] Val AUC : [0.98015] Combined Score: [0.04245]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1264/1264 [00:44<00:00, 28.16it/s]\n",
      "100%|██████████| 20/20 [00:00<00:00, 258.00it/s]\n",
      "100%|██████████| 716/716 [00:13<00:00, 51.37it/s]\n",
      "100%|██████████| 174/174 [00:00<00:00, 309.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], Train Loss : [0.24764] Val Loss : [0.21982] Val AUC : [0.98047] Combined Score: [0.04470]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 716/716 [00:14<00:00, 50.10it/s]\n",
      "100%|██████████| 174/174 [00:00<00:00, 307.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2], Train Loss : [0.23730] Val Loss : [0.21682] Val AUC : [0.98137] Combined Score: [0.04413]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 716/716 [00:14<00:00, 51.06it/s]\n",
      "100%|██████████| 174/174 [00:00<00:00, 227.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3], Train Loss : [0.23084] Val Loss : [0.20547] Val AUC : [0.98305] Combined Score: [0.04102]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 716/716 [00:14<00:00, 49.73it/s]\n",
      "100%|██████████| 174/174 [00:00<00:00, 319.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4], Train Loss : [0.22646] Val Loss : [0.20406] Val AUC : [0.98373] Combined Score: [0.04137]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 716/716 [00:13<00:00, 51.15it/s]\n",
      "100%|██████████| 174/174 [00:00<00:00, 346.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5], Train Loss : [0.22016] Val Loss : [0.19438] Val AUC : [0.98326] Combined Score: [0.03664]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 716/716 [00:14<00:00, 49.87it/s]\n",
      "100%|██████████| 174/174 [00:00<00:00, 281.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6], Train Loss : [0.21939] Val Loss : [0.19570] Val AUC : [0.98457] Combined Score: [0.03802]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 716/716 [00:14<00:00, 50.45it/s]\n",
      "100%|██████████| 174/174 [00:00<00:00, 303.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7], Train Loss : [0.21552] Val Loss : [0.20258] Val AUC : [0.98421] Combined Score: [0.04117]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 716/716 [00:14<00:00, 51.14it/s]\n",
      "100%|██████████| 174/174 [00:00<00:00, 321.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8], Train Loss : [0.20859] Val Loss : [0.19084] Val AUC : [0.98494] Combined Score: [0.03673]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 716/716 [00:14<00:00, 50.33it/s]\n",
      "100%|██████████| 174/174 [00:00<00:00, 280.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9], Train Loss : [0.20639] Val Loss : [0.19010] Val AUC : [0.98662] Combined Score: [0.03803]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 716/716 [00:14<00:00, 50.87it/s]\n",
      "100%|██████████| 174/174 [00:00<00:00, 292.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10], Train Loss : [0.20302] Val Loss : [0.18138] Val AUC : [0.98674] Combined Score: [0.03489]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 716/716 [00:11<00:00, 61.47it/s]\n",
      "100%|██████████| 174/174 [00:00<00:00, 334.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11], Train Loss : [0.20021] Val Loss : [0.18377] Val AUC : [0.98687] Combined Score: [0.03554]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 716/716 [00:11<00:00, 62.78it/s]\n",
      "100%|██████████| 174/174 [00:00<00:00, 348.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12], Train Loss : [0.20069] Val Loss : [0.17884] Val AUC : [0.98803] Combined Score: [0.03513]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 716/716 [00:11<00:00, 63.02it/s]\n",
      "100%|██████████| 174/174 [00:00<00:00, 365.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13], Train Loss : [0.19265] Val Loss : [0.17748] Val AUC : [0.98808] Combined Score: [0.03413]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 716/716 [00:11<00:00, 62.87it/s]\n",
      "100%|██████████| 174/174 [00:00<00:00, 361.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14], Train Loss : [0.18810] Val Loss : [0.16575] Val AUC : [0.98888] Combined Score: [0.03114]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 716/716 [00:11<00:00, 63.45it/s]\n",
      "100%|██████████| 174/174 [00:00<00:00, 386.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15], Train Loss : [0.19060] Val Loss : [0.15917] Val AUC : [0.98934] Combined Score: [0.02912]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50000it [24:30, 34.00it/s]\n",
      "100%|██████████| 782/782 [00:01<00:00, 409.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Combined Score before test: 0.43702\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import librosa\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import warnings\n",
    "from sklearn.metrics import roc_auc_score, mean_squared_error\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "class Config:\n",
    "    SR = 32000\n",
    "    N_MFCC = 13\n",
    "    ROOT_FOLDER = './'\n",
    "    N_CLASSES = 2\n",
    "    BATCH_SIZE = 64\n",
    "    N_EPOCHS = 15\n",
    "    LR = 3e-4\n",
    "    SEED = 42\n",
    "    DROPOUT_RATE = 0.5\n",
    "    HIDDEN_DIMS = [128, 256, 128]\n",
    "    INITIALIZATION = 'xavier'\n",
    "    ACTIVATION_FUNCTION = 'relu'\n",
    "\n",
    "CONFIG = Config()\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CONFIG.SEED)\n",
    "\n",
    "# 데이터 로드 및 전처리\n",
    "df = pd.read_csv('./train.csv')\n",
    "\n",
    "def label_encoder(column):\n",
    "    le = LabelEncoder().fit(column)\n",
    "    print(column.name, le.classes_)\n",
    "    return le.transform(column)\n",
    "\n",
    "df['class'] = label_encoder(df['label'])\n",
    "\n",
    "# MFCC 특징 추출\n",
    "def get_mfcc_feature(df, train_mode=True):\n",
    "    features = []\n",
    "    labels = []\n",
    "    for _, row in tqdm(df.iterrows()):\n",
    "        y, sr = librosa.load(row['path'], sr=CONFIG.SR)\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=CONFIG.N_MFCC)\n",
    "        mfcc = np.mean(mfcc.T, axis=0)\n",
    "        features.append(mfcc)\n",
    "\n",
    "        if train_mode:\n",
    "            label = row['class']\n",
    "            labels.append(label)\n",
    "\n",
    "    return features, labels\n",
    "\n",
    "features, labels = get_mfcc_feature(df, True)\n",
    "feature_df = pd.DataFrame({'features': features, 'class': labels})\n",
    "\n",
    "X = np.array(feature_df['features'].tolist())\n",
    "y = np.array(feature_df['class'].tolist())\n",
    "\n",
    "# 데이터 불균형 해결\n",
    "ros = RandomOverSampler(random_state=CONFIG.SEED)\n",
    "X_resampled, y_resampled = ros.fit_resample(X, y)\n",
    "y_resampled = torch.tensor(y_resampled).long()  # 정수형으로 변환\n",
    "y_resampled = torch.nn.functional.one_hot(y_resampled, num_classes=CONFIG.N_CLASSES).float()\n",
    "\n",
    "# 데이터 분할\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=CONFIG.SEED)\n",
    "\n",
    "# PyTorch Dataset 클래스 정의\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, mfcc, label):\n",
    "        self.mfcc = mfcc\n",
    "        self.label = label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mfcc)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.mfcc[index], self.label[index]\n",
    "\n",
    "train_dataset = CustomDataset(X_train, y_train)\n",
    "val_dataset = CustomDataset(X_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# MLP 모델 정의\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim=CONFIG.N_MFCC, hidden_dims=CONFIG.HIDDEN_DIMS, output_dim=CONFIG.N_CLASSES):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dims[0])\n",
    "        self.fc2 = nn.Linear(hidden_dims[0], hidden_dims[1])\n",
    "        self.fc3 = nn.Linear(hidden_dims[1], hidden_dims[2])\n",
    "        self.fc4 = nn.Linear(hidden_dims[2], output_dim)\n",
    "        \n",
    "        if CONFIG.INITIALIZATION == 'xavier':\n",
    "            nn.init.xavier_uniform_(self.fc1.weight)\n",
    "            nn.init.xavier_uniform_(self.fc2.weight)\n",
    "            nn.init.xavier_uniform_(self.fc3.weight)\n",
    "            nn.init.xavier_uniform_(self.fc4.weight)\n",
    "        \n",
    "        self.activation = nn.ReLU() if CONFIG.ACTIVATION_FUNCTION == 'relu' else nn.Tanh()\n",
    "        self.dropout = nn.Dropout(CONFIG.DROPOUT_RATE)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.activation(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "# 평가 함수 정의\n",
    "def expected_calibration_error(y_true, y_prob, n_bins=10):\n",
    "    prob_true, prob_pred = calibration_curve(y_true, y_prob, n_bins=n_bins, strategy='uniform')\n",
    "    bin_totals = np.histogram(y_prob, bins=np.linspace(0, 1, n_bins + 1), density=False)[0]\n",
    "    non_empty_bins = bin_totals > 0\n",
    "    bin_weights = bin_totals / len(y_prob)\n",
    "    bin_weights = bin_weights[non_empty_bins]\n",
    "    prob_true = prob_true[:len(bin_weights)]\n",
    "    prob_pred = prob_pred[:len(bin_weights)]\n",
    "    ece = np.sum(bin_weights * np.abs(prob_true - prob_pred))\n",
    "    return ece\n",
    "\n",
    "def auc_brier_ece(answer_df, submission_df):\n",
    "    # Check for missing values in submission_df\n",
    "    if submission_df.isnull().values.any():\n",
    "        raise ValueError(\"The submission dataframe contains missing values.\")\n",
    "\n",
    "    # Check if the number and names of columns are the same in both dataframes\n",
    "    if len(answer_df.columns) != len(submission_df.columns) or not all(answer_df.columns == submission_df.columns):\n",
    "        raise ValueError(\"The columns of the answer and submission dataframes do not match.\")\n",
    "        \n",
    "    submission_df = submission_df[submission_df.index.isin(answer_df.index)]\n",
    "    submission_df.index = range(submission_df.shape[0])\n",
    "    \n",
    "    # Calculate AUC for each class\n",
    "    auc_scores = []\n",
    "    for column in answer_df.columns:\n",
    "        y_true = answer_df[column]\n",
    "        y_scores = submission_df[column]\n",
    "        auc = roc_auc_score(y_true, y_scores)\n",
    "        auc_scores.append(auc)\n",
    "\n",
    "    # Calculate mean AUC\n",
    "    mean_auc = np.mean(auc_scores)\n",
    "\n",
    "    brier_scores = []\n",
    "    ece_scores = []\n",
    "    \n",
    "    # Calculate Brier Score and ECE for each class\n",
    "    for column in answer_df.columns:\n",
    "        y_true = answer_df[column].values\n",
    "        y_prob = submission_df[column].values\n",
    "        \n",
    "        # Brier Score\n",
    "        brier = mean_squared_error(y_true, y_prob)\n",
    "        brier_scores.append(brier)\n",
    "        \n",
    "        # ECE\n",
    "        ece = expected_calibration_error(y_true, y_prob)\n",
    "        ece_scores.append(ece)\n",
    "    \n",
    "    # Calculate mean Brier Score and mean ECE\n",
    "    mean_brier = np.mean(brier_scores)\n",
    "    mean_ece = np.mean(ece_scores)\n",
    "    \n",
    "    # Calculate combined score\n",
    "    combined_score = 0.5 * (1 - mean_auc) + 0.25 * mean_brier + 0.25 * mean_ece\n",
    "    \n",
    "    return combined_score\n",
    "\n",
    "# 모델 학습 함수 정의\n",
    "def train(model, optimizer, train_loader, val_loader, device):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss().to(device)\n",
    "    \n",
    "    best_val_score = 0\n",
    "    best_model = None\n",
    "    val_labels = y_val.cpu().numpy()\n",
    "    \n",
    "    for epoch in range(1, CONFIG.N_EPOCHS+1):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        for features, labels in tqdm(iter(train_loader)):\n",
    "            features = features.float().to(device)\n",
    "            labels = labels.float().to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model(features)\n",
    "            loss = criterion(output, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss.append(loss.item())\n",
    "                    \n",
    "        _val_loss, _val_score, val_outputs = validation(model, criterion, val_loader, device)\n",
    "        _train_loss = np.mean(train_loss)\n",
    "        \n",
    "        combined_score = auc_brier_ece(pd.DataFrame(val_labels), pd.DataFrame(val_outputs))\n",
    "        \n",
    "        print(f'Epoch [{epoch}], Train Loss : [{_train_loss:.5f}] Val Loss : [{_val_loss:.5f}] Val AUC : [{_val_score:.5f}] Combined Score: [{combined_score:.5f}]')\n",
    "            \n",
    "        if best_val_score < _val_score:\n",
    "            best_val_score = _val_score\n",
    "            best_model = model\n",
    "    \n",
    "    return best_model\n",
    "\n",
    "def multiLabel_AUC(y_true, y_scores):\n",
    "    auc_scores = []\n",
    "    for i in range(y_true.shape[1]):\n",
    "        auc = roc_auc_score(y_true[:, i], y_scores[:, i])\n",
    "        auc_scores.append(auc)\n",
    "    mean_auc_score = np.mean(auc_scores)\n",
    "    return mean_auc_score\n",
    "    \n",
    "def validation(model, criterion, val_loader, device):\n",
    "    model.eval()\n",
    "    val_loss, all_labels, all_probs = [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for features, labels in tqdm(iter(val_loader)):\n",
    "            features = features.float().to(device)\n",
    "            labels = labels.float().to(device)\n",
    "            \n",
    "            probs = model(features)\n",
    "            \n",
    "            loss = criterion(probs, labels)\n",
    "\n",
    "            val_loss.append(loss.item())\n",
    "\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "            all_probs.append(probs.cpu().numpy())\n",
    "        \n",
    "        _val_loss = np.mean(val_loss)\n",
    "\n",
    "        all_labels = np.concatenate(all_labels, axis=0)\n",
    "        all_probs = np.concatenate(all_probs, axis=0)\n",
    "        \n",
    "        auc_score = multiLabel_AUC(all_labels, all_probs)\n",
    "    \n",
    "    return _val_loss, auc_score, all_probs\n",
    "\n",
    "model = MLP()\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr = CONFIG.LR)\n",
    "\n",
    "infer_model = train(model, optimizer, train_loader, val_loader, device)\n",
    "\n",
    "# Unlabeled 데이터에 대해 pseudo-labeling\n",
    "unlabeled_path = './unlabeled_data'\n",
    "unlabeled_files = [os.path.join(unlabeled_path, f) for f in os.listdir(unlabeled_path) if f.endswith('.ogg')]\n",
    "\n",
    "def get_mfcc_feature_from_files(file_paths):\n",
    "    features = []\n",
    "    for file_path in tqdm(file_paths):\n",
    "        y, sr = librosa.load(file_path, sr=CONFIG.SR)\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=CONFIG.N_MFCC)\n",
    "        mfcc = np.mean(mfcc.T, axis=0)\n",
    "        features.append(mfcc)\n",
    "    return features\n",
    "\n",
    "unlabeled_features = get_mfcc_feature_from_files(unlabeled_files)\n",
    "unlabeled_features = np.array(unlabeled_features)\n",
    "unlabeled_dataset = CustomDataset(unlabeled_features, torch.zeros((len(unlabeled_features), CONFIG.N_CLASSES)))\n",
    "unlabeled_loader = DataLoader(unlabeled_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=False)\n",
    "\n",
    "def pseudo_labeling(model, loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    pseudo_labels = []\n",
    "    with torch.no_grad():\n",
    "        for features, _ in tqdm(iter(loader)):\n",
    "            features = features.float().to(device)\n",
    "            probs = model(features)\n",
    "            pseudo_labels.append(probs.cpu().detach().numpy())\n",
    "    return np.concatenate(pseudo_labels, axis=0)\n",
    "\n",
    "pseudo_labels = pseudo_labeling(infer_model, unlabeled_loader, device)\n",
    "pseudo_labels = (pseudo_labels > 0.5).astype(int)  # 임계값 0.5를 사용하여 pseudo-label 생성\n",
    "\n",
    "# Pseudo-labeled 데이터를 학습 데이터에 추가\n",
    "pseudo_labeled_dataset = CustomDataset(unlabeled_features, torch.tensor(pseudo_labels).float())\n",
    "train_dataset_combined = torch.utils.data.ConcatDataset([train_dataset, pseudo_labeled_dataset])\n",
    "train_loader_combined = DataLoader(train_dataset_combined, batch_size=CONFIG.BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# 모델을 pseudo-labeled 데이터를 포함하여 재학습\n",
    "infer_model = train(model, optimizer, train_loader_combined, val_loader, device)\n",
    "\n",
    "# 테스트 데이터 예측\n",
    "test = pd.read_csv('./test.csv')\n",
    "test_features, _ = get_mfcc_feature(test, False)\n",
    "test_features = np.array(test_features)\n",
    "test_dataset = CustomDataset(test_features, torch.zeros((len(test_features), CONFIG.N_CLASSES)))\n",
    "test_loader = DataLoader(test_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=False)\n",
    "\n",
    "def inference(model, test_loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for features, _ in tqdm(iter(test_loader)):\n",
    "            features = features.float().to(device)\n",
    "            probs = model(features)\n",
    "            probs  = probs.cpu().detach().numpy()\n",
    "            predictions += probs.tolist()\n",
    "    return predictions\n",
    "\n",
    "preds = inference(infer_model, test_loader, device)\n",
    "\n",
    "submit = pd.read_csv('./sample_submission.csv')\n",
    "submit.iloc[:, 1:] = preds\n",
    "submit.head()\n",
    "\n",
    "combined_score_before_test = auc_brier_ece(pd.DataFrame(y_val.cpu().numpy()), pd.DataFrame(preds))\n",
    "print(f'Final Combined Score before test: {combined_score_before_test:.5f}')\n",
    "\n",
    "submit.to_csv('./wy_origin3.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label ['fake' 'real']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25082it [4:25:57,  1.57it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 85\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(features)\n\u001b[0;32m     84\u001b[0m \u001b[38;5;66;03m# Train 데이터에서 특징 추출\u001b[39;00m\n\u001b[1;32m---> 85\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[43mget_features_and_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m# 데이터 불균형 해결\u001b[39;00m\n\u001b[0;32m     88\u001b[0m smote \u001b[38;5;241m=\u001b[39m SMOTE(random_state\u001b[38;5;241m=\u001b[39mCONFIG\u001b[38;5;241m.\u001b[39mSEED)\n",
      "Cell \u001b[1;32mIn[1], line 77\u001b[0m, in \u001b[0;36mget_features_and_labels\u001b[1;34m(df, train_mode)\u001b[0m\n\u001b[0;32m     75\u001b[0m file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(CONFIG\u001b[38;5;241m.\u001b[39mROOT_FOLDER, row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m2\u001b[39m:])  \u001b[38;5;66;03m# './train/' 또는 './test/' 제거\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(file_path):\n\u001b[1;32m---> 77\u001b[0m     features\u001b[38;5;241m.\u001b[39mappend(\u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m train_mode:\n\u001b[0;32m     79\u001b[0m         labels\u001b[38;5;241m.\u001b[39mappend(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[1;32mIn[1], line 66\u001b[0m, in \u001b[0;36mextract_features\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m     64\u001b[0m mel \u001b[38;5;241m=\u001b[39m librosa\u001b[38;5;241m.\u001b[39mfeature\u001b[38;5;241m.\u001b[39mmelspectrogram(y\u001b[38;5;241m=\u001b[39my, sr\u001b[38;5;241m=\u001b[39msr)\n\u001b[0;32m     65\u001b[0m contrast \u001b[38;5;241m=\u001b[39m librosa\u001b[38;5;241m.\u001b[39mfeature\u001b[38;5;241m.\u001b[39mspectral_contrast(y\u001b[38;5;241m=\u001b[39my, sr\u001b[38;5;241m=\u001b[39msr)\n\u001b[1;32m---> 66\u001b[0m tonnetz \u001b[38;5;241m=\u001b[39m librosa\u001b[38;5;241m.\u001b[39mfeature\u001b[38;5;241m.\u001b[39mtonnetz(y\u001b[38;5;241m=\u001b[39m\u001b[43mlibrosa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meffects\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mharmonic\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m, sr\u001b[38;5;241m=\u001b[39msr)\n\u001b[0;32m     68\u001b[0m features \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((mfcc, chroma, mel, contrast, tonnetz), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m features\n",
      "File \u001b[1;32mc:\\Users\\moon0\\anaconda3\\Lib\\site-packages\\librosa\\effects.py:238\u001b[0m, in \u001b[0;36mharmonic\u001b[1;34m(y, kernel_size, power, mask, margin, n_fft, hop_length, win_length, window, center, pad_mode)\u001b[0m\n\u001b[0;32m    228\u001b[0m stft \u001b[38;5;241m=\u001b[39m core\u001b[38;5;241m.\u001b[39mstft(\n\u001b[0;32m    229\u001b[0m     y,\n\u001b[0;32m    230\u001b[0m     n_fft\u001b[38;5;241m=\u001b[39mn_fft,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    234\u001b[0m     pad_mode\u001b[38;5;241m=\u001b[39mpad_mode,\n\u001b[0;32m    235\u001b[0m )\n\u001b[0;32m    237\u001b[0m \u001b[38;5;66;03m# Remove percussives\u001b[39;00m\n\u001b[1;32m--> 238\u001b[0m stft_harm \u001b[38;5;241m=\u001b[39m \u001b[43mdecompose\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhpss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpower\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpower\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmargin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmargin\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    242\u001b[0m \u001b[38;5;66;03m# Invert the STFTs\u001b[39;00m\n\u001b[0;32m    243\u001b[0m y_harm \u001b[38;5;241m=\u001b[39m core\u001b[38;5;241m.\u001b[39mistft(\n\u001b[0;32m    244\u001b[0m     stft_harm,\n\u001b[0;32m    245\u001b[0m     dtype\u001b[38;5;241m=\u001b[39my\u001b[38;5;241m.\u001b[39mdtype,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    250\u001b[0m     length\u001b[38;5;241m=\u001b[39my\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],\n\u001b[0;32m    251\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\moon0\\anaconda3\\Lib\\site-packages\\librosa\\decompose.py:390\u001b[0m, in \u001b[0;36mhpss\u001b[1;34m(S, kernel_size, power, mask, margin)\u001b[0m\n\u001b[0;32m    388\u001b[0m \u001b[38;5;66;03m# Compute median filters. Pre-allocation here preserves memory layout.\u001b[39;00m\n\u001b[0;32m    389\u001b[0m harm \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty_like(S)\n\u001b[1;32m--> 390\u001b[0m harm[:] \u001b[38;5;241m=\u001b[39m \u001b[43mmedian_filter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mharm_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreflect\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    392\u001b[0m perc \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty_like(S)\n\u001b[0;32m    393\u001b[0m perc[:] \u001b[38;5;241m=\u001b[39m median_filter(S, size\u001b[38;5;241m=\u001b[39mperc_shape, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreflect\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\moon0\\anaconda3\\Lib\\site-packages\\scipy\\ndimage\\_filters.py:1594\u001b[0m, in \u001b[0;36mmedian_filter\u001b[1;34m(input, size, footprint, output, mode, cval, origin, axes)\u001b[0m\n\u001b[0;32m   1547\u001b[0m \u001b[38;5;129m@_ni_docstrings\u001b[39m\u001b[38;5;241m.\u001b[39mdocfiller\n\u001b[0;32m   1548\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmedian_filter\u001b[39m(\u001b[38;5;28minput\u001b[39m, size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, footprint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1549\u001b[0m                   mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreflect\u001b[39m\u001b[38;5;124m\"\u001b[39m, cval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m, origin\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m*\u001b[39m, axes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1550\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1551\u001b[0m \u001b[38;5;124;03m    Calculate a multidimensional median filter.\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1592\u001b[0m \u001b[38;5;124;03m    >>> plt.show()\u001b[39;00m\n\u001b[0;32m   1593\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1594\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_rank_filter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfootprint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1595\u001b[0m \u001b[43m                        \u001b[49m\u001b[43morigin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmedian\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\moon0\\anaconda3\\Lib\\site-packages\\scipy\\ndimage\\_filters.py:1495\u001b[0m, in \u001b[0;36m_rank_filter\u001b[1;34m(input, rank, size, footprint, output, mode, cval, origin, operation, axes)\u001b[0m\n\u001b[0;32m   1491\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1492\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA sequence of modes is not supported by non-separable rank \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1493\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilters\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1494\u001b[0m mode \u001b[38;5;241m=\u001b[39m _ni_support\u001b[38;5;241m.\u001b[39m_extend_mode_to_code(mode)\n\u001b[1;32m-> 1495\u001b[0m \u001b[43m_nd_image\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrank_filter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfootprint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1496\u001b[0m \u001b[43m                      \u001b[49m\u001b[43morigins\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m temp_needed:\n\u001b[0;32m   1498\u001b[0m     temp[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m] \u001b[38;5;241m=\u001b[39m output\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import librosa\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import warnings\n",
    "from sklearn.metrics import roc_auc_score, mean_squared_error\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "class Config:\n",
    "    SR = 32000\n",
    "    N_MFCC = 13\n",
    "    N_FEATURES = 64\n",
    "    ROOT_FOLDER = './'\n",
    "    N_CLASSES = 2\n",
    "    BATCH_SIZE = 96\n",
    "    N_EPOCHS = 5\n",
    "    LR = 3e-4\n",
    "    SEED = 42\n",
    "    DROPOUT_RATE = 0.5\n",
    "    HIDDEN_DIMS = [128, 256, 128]\n",
    "    INITIALIZATION = 'xavier'\n",
    "    ACTIVATION_FUNCTION = 'relu'\n",
    "\n",
    "CONFIG = Config()\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CONFIG.SEED)\n",
    "\n",
    "# 데이터 로드 및 전처리\n",
    "df = pd.read_csv(os.path.join(CONFIG.ROOT_FOLDER, 'train.csv'))\n",
    "\n",
    "def label_encoder(column):\n",
    "    le = LabelEncoder().fit(column)\n",
    "    print(column.name, le.classes_)\n",
    "    return le.transform(column)\n",
    "\n",
    "df['class'] = label_encoder(df['label'])\n",
    "\n",
    "# 음성 특징 추출 함수\n",
    "def extract_features(file_path):\n",
    "    y, sr = librosa.load(file_path, sr=CONFIG.SR)\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=CONFIG.N_MFCC)\n",
    "    chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "    mel = librosa.feature.melspectrogram(y=y, sr=sr)\n",
    "    contrast = librosa.feature.spectral_contrast(y=y, sr=sr)\n",
    "    tonnetz = librosa.feature.tonnetz(y=librosa.effects.harmonic(y), sr=sr)\n",
    "    \n",
    "    features = np.concatenate((mfcc, chroma, mel, contrast, tonnetz), axis=0)\n",
    "    return features\n",
    "\n",
    "def get_features_and_labels(df, train_mode=True):\n",
    "    features = []\n",
    "    labels = []\n",
    "    for _, row in tqdm(df.iterrows()):\n",
    "        file_path = os.path.join(CONFIG.ROOT_FOLDER, row['path'][2:])  # './train/' 또는 './test/' 제거\n",
    "        if os.path.isfile(file_path):\n",
    "            features.append(extract_features(file_path))\n",
    "            if train_mode:\n",
    "                labels.append(row['class'])\n",
    "    if train_mode:\n",
    "        return np.array(features), np.array(labels)\n",
    "    return np.array(features)\n",
    "\n",
    "# Train 데이터에서 특징 추출\n",
    "X, y = get_features_and_labels(df, True)\n",
    "\n",
    "# 데이터 불균형 해결\n",
    "smote = SMOTE(random_state=CONFIG.SEED)\n",
    "X_resampled, y_resampled = smote.fit_resample(X.reshape(len(X), -1), y)\n",
    "X_resampled = X_resampled.reshape(len(X_resampled), CONFIG.N_FEATURES, -1)\n",
    "y_resampled = torch.tensor(y_resampled).long()  # 정수형으로 변환\n",
    "y_resampled = torch.nn.functional.one_hot(y_resampled, num_classes=CONFIG.N_CLASSES).float()\n",
    "\n",
    "# 데이터 분할\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=CONFIG.SEED)\n",
    "\n",
    "# PyTorch Dataset 클래스 정의\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.features[index], self.labels[index]\n",
    "\n",
    "train_dataset = CustomDataset(X_train, y_train)\n",
    "val_dataset = CustomDataset(X_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# CNN+RNN 모델 정의\n",
    "class CNNRNN(nn.Module):\n",
    "    def __init__(self, input_dim=CONFIG.N_FEATURES, hidden_dims=CONFIG.HIDDEN_DIMS, output_dim=CONFIG.N_CLASSES):\n",
    "        super(CNNRNN, self).__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(CONFIG.DROPOUT_RATE),\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(CONFIG.DROPOUT_RATE)\n",
    "        )\n",
    "        \n",
    "        self.rnn = nn.LSTM(input_size=64*8, hidden_size=hidden_dims[0], num_layers=2, batch_first=True, dropout=CONFIG.DROPOUT_RATE, bidirectional=True)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dims[0]*2, hidden_dims[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(CONFIG.DROPOUT_RATE),\n",
    "            nn.Linear(hidden_dims[1], hidden_dims[2]),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(CONFIG.DROPOUT_RATE),\n",
    "            nn.Linear(hidden_dims[2], output_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = x.unsqueeze(1)  # (batch_size, 1, n_features, time_steps)\n",
    "        x = self.cnn(x)\n",
    "        x = x.view(batch_size, 64*8, -1).permute(0, 2, 1)  # (batch_size, time_steps, 64*8)\n",
    "        x, _ = self.rnn(x)\n",
    "        x = x[:, -1, :]  # 마지막 타임 스텝의 출력\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# 평가 함수 정의\n",
    "def expected_calibration_error(y_true, y_prob, n_bins=10):\n",
    "    prob_true, prob_pred = calibration_curve(y_true, y_prob, n_bins=n_bins, strategy='uniform')\n",
    "    bin_totals = np.histogram(y_prob, bins=np.linspace(0, 1, n_bins + 1), density=False)[0]\n",
    "    non_empty_bins = bin_totals > 0\n",
    "    bin_weights = bin_totals / len(y_prob)\n",
    "    bin_weights = bin_weights[non_empty_bins]\n",
    "    prob_true = prob_true[:len(bin_weights)]\n",
    "    prob_pred = prob_pred[:len(bin_weights)]\n",
    "    ece = np.sum(bin_weights * np.abs(prob_true - prob_pred))\n",
    "    return ece\n",
    "\n",
    "def auc_brier_ece(answer_df, submission_df):\n",
    "    # Check for missing values in submission_df\n",
    "    if submission_df.isnull().values.any():\n",
    "        raise ValueError(\"The submission dataframe contains missing values.\")\n",
    "\n",
    "    # Check if the number and names of columns are the same in both dataframes\n",
    "    if len(answer_df.columns) != len(submission_df.columns) or not all(answer_df.columns == submission_df.columns):\n",
    "        raise ValueError(\"The columns of the answer and submission dataframes do not match.\")\n",
    "        \n",
    "    submission_df = submission_df[submission_df.index.isin(answer_df.index)]\n",
    "    submission_df.index = range(submission_df.shape[0])\n",
    "    \n",
    "    # Calculate AUC for each class\n",
    "    auc_scores = []\n",
    "    for column in answer_df.columns:\n",
    "        y_true = answer_df[column]\n",
    "        y_scores = submission_df[column]\n",
    "        auc = roc_auc_score(y_true, y_scores)\n",
    "        auc_scores.append(auc)\n",
    "\n",
    "    # Calculate mean AUC\n",
    "    mean_auc = np.mean(auc_scores)\n",
    "\n",
    "    brier_scores = []\n",
    "    ece_scores = []\n",
    "    \n",
    "    # Calculate Brier Score and ECE for each class\n",
    "    for column in answer_df.columns:\n",
    "        y_true = answer_df[column].values\n",
    "        y_prob = submission_df[column].values\n",
    "        \n",
    "        # Brier Score\n",
    "        brier = mean_squared_error(y_true, y_prob)\n",
    "        brier_scores.append(brier)\n",
    "        \n",
    "        # ECE\n",
    "        ece = expected_calibration_error(y_true, y_prob)\n",
    "        ece_scores.append(ece)\n",
    "    \n",
    "    # Calculate mean Brier Score and mean ECE\n",
    "    mean_brier = np.mean(brier_scores)\n",
    "    mean_ece = np.mean(ece_scores)\n",
    "    \n",
    "    # Calculate combined score\n",
    "    combined_score = 0.5 * (1 - mean_auc) + 0.25 * mean_brier + 0.25 * mean_ece\n",
    "    \n",
    "    return combined_score\n",
    "\n",
    "# 모델 학습 함수 정의\n",
    "def train(model, optimizer, train_loader, val_loader, device):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss().to(device)\n",
    "    \n",
    "    best_val_score = 0\n",
    "    best_model = None\n",
    "    val_labels = y_val.cpu().numpy()\n",
    "    \n",
    "    for epoch in range(1, CONFIG.N_EPOCHS+1):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        for features, labels in tqdm(iter(train_loader)):\n",
    "            features = features.float().to(device)\n",
    "            labels = labels.float().to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model(features)\n",
    "            loss = criterion(output, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss.append(loss.item())\n",
    "                    \n",
    "        _val_loss, _val_score, val_outputs = validation(model, criterion, val_loader, device)\n",
    "        _train_loss = np.mean(train_loss)\n",
    "        \n",
    "        combined_score = auc_brier_ece(pd.DataFrame(val_labels), pd.DataFrame(val_outputs))\n",
    "        \n",
    "        print(f'Epoch [{epoch}], Train Loss : [{_train_loss:.5f}] Val Loss : [{_val_loss:.5f}] Val AUC : [{_val_score:.5f}] Combined Score: [{combined_score:.5f}]')\n",
    "            \n",
    "        if best_val_score < _val_score:\n",
    "            best_val_score = _val_score\n",
    "            best_model = model\n",
    "    \n",
    "    return best_model\n",
    "\n",
    "def multiLabel_AUC(y_true, y_scores):\n",
    "    auc_scores = []\n",
    "    for i in range(y_true.shape[1]):\n",
    "        auc = roc_auc_score(y_true[:, i], y_scores[:, i])\n",
    "        auc_scores.append(auc)\n",
    "    mean_auc_score = np.mean(auc_scores)\n",
    "    return mean_auc_score\n",
    "    \n",
    "def validation(model, criterion, val_loader, device):\n",
    "    model.eval()\n",
    "    val_loss, all_labels, all_probs = [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for features, labels in tqdm(iter(val_loader)):\n",
    "            features = features.float().to(device)\n",
    "            labels = labels.float().to(device)\n",
    "            \n",
    "            probs = model(features)\n",
    "            \n",
    "            loss = criterion(probs, labels)\n",
    "\n",
    "            val_loss.append(loss.item())\n",
    "\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "            all_probs.append(probs.cpu().numpy())\n",
    "        \n",
    "        _val_loss = np.mean(val_loss)\n",
    "\n",
    "        all_labels = np.concatenate(all_labels, axis=0)\n",
    "        all_probs = np.concatenate(all_probs, axis=0)\n",
    "        \n",
    "        auc_score = multiLabel_AUC(all_labels, all_probs)\n",
    "    \n",
    "    return _val_loss, auc_score, all_probs\n",
    "\n",
    "model = CNNRNN()\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr = CONFIG.LR)\n",
    "\n",
    "infer_model = train(model, optimizer, train_loader, val_loader, device)\n",
    "\n",
    "# Unlabeled 데이터에 대해 pseudo-labeling\n",
    "unlabeled_path = os.path.join(CONFIG.ROOT_FOLDER, 'unlabeled_data')\n",
    "unlabeled_files = [os.path.join(unlabeled_path, f) for f in os.listdir(unlabeled_path) if f.endswith('.ogg')]\n",
    "\n",
    "unlabeled_features = get_features_and_labels(pd.DataFrame({'path': unlabeled_files}), False)\n",
    "unlabeled_dataset = CustomDataset(unlabeled_features, torch.zeros((len(unlabeled_features), CONFIG.N_CLASSES)))\n",
    "unlabeled_loader = DataLoader(unlabeled_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=False)\n",
    "\n",
    "def pseudo_labeling(model, loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    pseudo_labels = []\n",
    "    with torch.no_grad():\n",
    "        for features, _ in tqdm(iter(loader)):\n",
    "            features = features.float().to(device)\n",
    "            probs = model(features)\n",
    "            pseudo_labels.append(probs.cpu().detach().numpy())\n",
    "    return np.concatenate(pseudo_labels, axis=0)\n",
    "\n",
    "pseudo_labels = pseudo_labeling(infer_model, unlabeled_loader, device)\n",
    "pseudo_labels = (pseudo_labels > 0.5).astype(int)  # 임계값 0.5를 사용하여 pseudo-label 생성\n",
    "\n",
    "# Pseudo-labeled 데이터를 학습 데이터에 추가\n",
    "pseudo_labeled_dataset = CustomDataset(unlabeled_features, torch.tensor(pseudo_labels).float())\n",
    "train_dataset_combined = torch.utils.data.ConcatDataset([train_dataset, pseudo_labeled_dataset])\n",
    "train_loader_combined = DataLoader(train_dataset_combined, batch_size=CONFIG.BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# 모델을 pseudo-labeled 데이터를 포함하여 재학습\n",
    "infer_model = train(model, optimizer, train_loader_combined, val_loader, device)\n",
    "\n",
    "# 테스트 데이터 예측\n",
    "test_df = pd.read_csv(os.path.join(CONFIG.ROOT_FOLDER, 'test.csv'))\n",
    "test_features = get_features_and_labels(test_df, False)\n",
    "test_dataset = CustomDataset(test_features, torch.zeros((len(test_features), CONFIG.N_CLASSES)))\n",
    "test_loader = DataLoader(test_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=False)\n",
    "\n",
    "def inference(model, test_loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for features, _ in tqdm(iter(test_loader)):\n",
    "            features = features.float().to(device)\n",
    "            probs = model(features)\n",
    "            probs  = probs.cpu().detach().numpy()\n",
    "            predictions += probs.tolist()\n",
    "    return predictions\n",
    "\n",
    "preds = inference(infer_model, test_loader, device)\n",
    "\n",
    "submit = pd.read_csv(os.path.join(CONFIG.ROOT_FOLDER, 'sample_submission.csv'))\n",
    "submit.iloc[:, 1:] = preds\n",
    "submit.head()\n",
    "\n",
    "combined_score_before_test = auc_brier_ece(pd.DataFrame(y_val.cpu().numpy()), pd.DataFrame(preds))\n",
    "print(f'Final Combined Score before test: {combined_score_before_test:.5f}')\n",
    "\n",
    "submit.to_csv('./wy_origin4.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
