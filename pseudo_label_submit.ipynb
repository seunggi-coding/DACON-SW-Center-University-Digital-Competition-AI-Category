{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "class Config:\n",
    "    SR = 32000\n",
    "    N_MFCC = 13\n",
    "    ROOT_FOLDER = './'\n",
    "    N_CLASSES = 2\n",
    "    BATCH_SIZE = 96\n",
    "    N_EPOCHS = 5\n",
    "    LR = 3e-4\n",
    "    SEED = 42\n",
    "\n",
    "CONFIG = Config()\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CONFIG.SEED)\n",
    "\n",
    "# 데이터 로드 및 전처리\n",
    "df = pd.read_csv('./train.csv')\n",
    "\n",
    "def label_encoder(column):\n",
    "    le = LabelEncoder().fit(column)\n",
    "    print(column.name, le.classes_)\n",
    "    return le.transform(column)\n",
    "\n",
    "df['class'] = label_encoder(df['label'])\n",
    "\n",
    "# MFCC 특징 추출\n",
    "def get_mfcc_feature(df, train_mode=True):\n",
    "    features = []\n",
    "    labels = []\n",
    "    for _, row in tqdm(df.iterrows()):\n",
    "        y, sr = librosa.load(row['path'], sr=CONFIG.SR)\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=CONFIG.N_MFCC)\n",
    "        mfcc = np.mean(mfcc.T, axis=0)\n",
    "        features.append(mfcc)\n",
    "\n",
    "        if train_mode:\n",
    "            label = row['class']\n",
    "            labels.append(label)\n",
    "\n",
    "    return features, labels\n",
    "\n",
    "features, labels = get_mfcc_feature(df, True)\n",
    "feature_df = pd.DataFrame({'features': features, 'class': labels})\n",
    "\n",
    "X = np.array(feature_df['features'].tolist())\n",
    "y = np.array(feature_df['class'].tolist())\n",
    "\n",
    "# 데이터 불균형 해결\n",
    "ros = RandomOverSampler(random_state=CONFIG.SEED)\n",
    "X_resampled, y_resampled = ros.fit_resample(X, y)\n",
    "y_resampled = torch.tensor(y_resampled).long()  # 정수형으로 변환\n",
    "y_resampled = torch.nn.functional.one_hot(y_resampled, num_classes=CONFIG.N_CLASSES).float()\n",
    "\n",
    "# 데이터 분할\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=CONFIG.SEED)\n",
    "\n",
    "# PyTorch Dataset 클래스 정의\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, mfcc, label):\n",
    "        self.mfcc = mfcc\n",
    "        self.label = label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mfcc)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.mfcc[index], self.label[index]\n",
    "\n",
    "train_dataset = CustomDataset(X_train, y_train)\n",
    "val_dataset = CustomDataset(X_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# MLP 모델 정의\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim=CONFIG.N_MFCC, hidden_dims=[128, 256, 128], output_dim=CONFIG.N_CLASSES):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dims[0])\n",
    "        self.fc2 = nn.Linear(hidden_dims[0], hidden_dims[1])\n",
    "        self.fc3 = nn.Linear(hidden_dims[1], hidden_dims[2])\n",
    "        self.fc4 = nn.Linear(hidden_dims[2], output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "# 모델 학습 함수 정의\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def train(model, optimizer, train_loader, val_loader, device):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss().to(device)\n",
    "    \n",
    "    best_val_score = 0\n",
    "    best_model = None\n",
    "    \n",
    "    for epoch in range(1, CONFIG.N_EPOCHS+1):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        for features, labels in tqdm(iter(train_loader)):\n",
    "            features = features.float().to(device)\n",
    "            labels = labels.float().to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model(features)\n",
    "            loss = criterion(output, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss.append(loss.item())\n",
    "                    \n",
    "        _val_loss, _val_score = validation(model, criterion, val_loader, device)\n",
    "        _train_loss = np.mean(train_loss)\n",
    "        print(f'Epoch [{epoch}], Train Loss : [{_train_loss:.5f}] Val Loss : [{_val_loss:.5f}] Val AUC : [{_val_score:.5f}]')\n",
    "            \n",
    "        if best_val_score < _val_score:\n",
    "            best_val_score = _val_score\n",
    "            best_model = model\n",
    "    \n",
    "    return best_model\n",
    "\n",
    "def multiLabel_AUC(y_true, y_scores):\n",
    "    auc_scores = []\n",
    "    for i in range(y_true.shape[1]):\n",
    "        auc = roc_auc_score(y_true[:, i], y_scores[:, i])\n",
    "        auc_scores.append(auc)\n",
    "    mean_auc_score = np.mean(auc_scores)\n",
    "    return mean_auc_score\n",
    "    \n",
    "def validation(model, criterion, val_loader, device):\n",
    "    model.eval()\n",
    "    val_loss, all_labels, all_probs = [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for features, labels in tqdm(iter(val_loader)):\n",
    "            features = features.float().to(device)\n",
    "            labels = labels.float().to(device)\n",
    "            \n",
    "            probs = model(features)\n",
    "            \n",
    "            loss = criterion(probs, labels)\n",
    "\n",
    "            val_loss.append(loss.item())\n",
    "\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "            all_probs.append(probs.cpu().numpy())\n",
    "        \n",
    "        _val_loss = np.mean(val_loss)\n",
    "\n",
    "        all_labels = np.concatenate(all_labels, axis=0)\n",
    "        all_probs = np.concatenate(all_probs, axis=0)\n",
    "        \n",
    "        auc_score = multiLabel_AUC(all_labels, all_probs)\n",
    "    \n",
    "    return _val_loss, auc_score\n",
    "\n",
    "model = MLP()\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr = CONFIG.LR)\n",
    "\n",
    "infer_model = train(model, optimizer, train_loader, val_loader, device)\n",
    "\n",
    "# Unlabeled 데이터에 대해 pseudo-labeling\n",
    "unlabeled_path = './unlabeled_data'\n",
    "unlabeled_files = [os.path.join(unlabeled_path, f) for f in os.listdir(unlabeled_path) if f.endswith('.ogg')]\n",
    "\n",
    "def get_mfcc_feature_from_files(file_paths):\n",
    "    features = []\n",
    "    for file_path in tqdm(file_paths):\n",
    "        y, sr = librosa.load(file_path, sr=CONFIG.SR)\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=CONFIG.N_MFCC)\n",
    "        mfcc = np.mean(mfcc.T, axis=0)\n",
    "        features.append(mfcc)\n",
    "    return features\n",
    "\n",
    "unlabeled_features = get_mfcc_feature_from_files(unlabeled_files)\n",
    "unlabeled_features = np.array(unlabeled_features)\n",
    "unlabeled_dataset = CustomDataset(unlabeled_features, torch.zeros((len(unlabeled_features), CONFIG.N_CLASSES)))\n",
    "unlabeled_loader = DataLoader(unlabeled_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=False)\n",
    "\n",
    "def pseudo_labeling(model, loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    pseudo_labels = []\n",
    "    with torch.no_grad():\n",
    "        for features, _ in tqdm(iter(loader)):\n",
    "            features = features.float().to(device)\n",
    "            probs = model(features)\n",
    "            pseudo_labels.append(probs.cpu().detach().numpy())\n",
    "    return np.concatenate(pseudo_labels, axis=0)\n",
    "\n",
    "pseudo_labels = pseudo_labeling(infer_model, unlabeled_loader, device)\n",
    "pseudo_labels = (pseudo_labels > 0.5).astype(int)  # 임계값 0.5를 사용하여 pseudo-label 생성\n",
    "\n",
    "# Pseudo-labeled 데이터를 학습 데이터에 추가\n",
    "pseudo_labeled_dataset = CustomDataset(unlabeled_features, torch.tensor(pseudo_labels).float())\n",
    "train_dataset_combined = torch.utils.data.ConcatDataset([train_dataset, pseudo_labeled_dataset])\n",
    "train_loader_combined = DataLoader(train_dataset_combined, batch_size=CONFIG.BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# 모델을 pseudo-labeled 데이터를 포함하여 재학습\n",
    "infer_model = train(model, optimizer, train_loader_combined, val_loader, device)\n",
    "\n",
    "# 테스트 데이터 예측\n",
    "test = pd.read_csv('./test.csv')\n",
    "test_features, _ = get_mfcc_feature(test, False)\n",
    "test_features = np.array(test_features)\n",
    "test_dataset = CustomDataset(test_features, torch.zeros((len(test_features), CONFIG.N_CLASSES)))\n",
    "test_loader = DataLoader(test_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=False)\n",
    "\n",
    "def inference(model, test_loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for features, _ in tqdm(iter(test_loader)):\n",
    "            features = features.float().to(device)\n",
    "            probs = model(features)\n",
    "            probs  = probs.cpu().detach().numpy()\n",
    "            predictions += probs.tolist()\n",
    "    return predictions\n",
    "\n",
    "preds = inference(infer_model, test_loader, device)\n",
    "\n",
    "submit = pd.read_csv('./sample_submission.csv')\n",
    "submit.iloc[:, 1:] = preds\n",
    "submit.head()\n",
    "\n",
    "submit.to_csv('./pseudo_label_submit.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "class Config:\n",
    "    SR = 32000\n",
    "    N_MFCC = 13\n",
    "    ROOT_FOLDER = './'\n",
    "    N_CLASSES = 2\n",
    "    BATCH_SIZE = 96\n",
    "    N_EPOCHS = 10\n",
    "    LR = 3e-4\n",
    "    SEED = 42\n",
    "\n",
    "CONFIG = Config()\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CONFIG.SEED)\n",
    "\n",
    "# 데이터 로드 및 전처리\n",
    "df = pd.read_csv('./train.csv')\n",
    "\n",
    "def label_encoder(column):\n",
    "    le = LabelEncoder().fit(column)\n",
    "    print(column.name, le.classes_)\n",
    "    return le.transform(column)\n",
    "\n",
    "df['class'] = label_encoder(df['label'])\n",
    "\n",
    "# MFCC 특징 추출\n",
    "def get_mfcc_feature(df, train_mode=True):\n",
    "    features = []\n",
    "    labels = []\n",
    "    for _, row in tqdm(df.iterrows()):\n",
    "        y, sr = librosa.load(row['path'], sr=CONFIG.SR)\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=CONFIG.N_MFCC)\n",
    "        mfcc = np.mean(mfcc.T, axis=0)\n",
    "        features.append(mfcc)\n",
    "\n",
    "        if train_mode:\n",
    "            label = row['class']\n",
    "            labels.append(label)\n",
    "\n",
    "    return features, labels\n",
    "\n",
    "features, labels = get_mfcc_feature(df, True)\n",
    "feature_df = pd.DataFrame({'features': features, 'class': labels})\n",
    "\n",
    "X = np.array(feature_df['features'].tolist())\n",
    "y = np.array(feature_df['class'].tolist())\n",
    "\n",
    "# 데이터 불균형 해결\n",
    "ros = RandomOverSampler(random_state=CONFIG.SEED)\n",
    "X_resampled, y_resampled = ros.fit_resample(X, y)\n",
    "y_resampled = torch.tensor(y_resampled).long()  # 정수형으로 변환\n",
    "y_resampled = torch.nn.functional.one_hot(y_resampled, num_classes=CONFIG.N_CLASSES).float()\n",
    "\n",
    "# 데이터 분할\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=CONFIG.SEED)\n",
    "\n",
    "# PyTorch Dataset 클래스 정의\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, mfcc, label):\n",
    "        self.mfcc = mfcc\n",
    "        self.label = label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mfcc)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.mfcc[index], self.label[index]\n",
    "\n",
    "train_dataset = CustomDataset(X_train, y_train)\n",
    "val_dataset = CustomDataset(X_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# MLP 모델 정의\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim=CONFIG.N_MFCC, hidden_dims=[128, 256, 128], output_dim=CONFIG.N_CLASSES):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dims[0])\n",
    "        self.fc2 = nn.Linear(hidden_dims[0], hidden_dims[1])\n",
    "        self.fc3 = nn.Linear(hidden_dims[1], hidden_dims[2])\n",
    "        self.fc4 = nn.Linear(hidden_dims[2], output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "# 모델 학습 함수 정의\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def train(model, optimizer, train_loader, val_loader, device):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss().to(device)\n",
    "    \n",
    "    best_val_score = 0\n",
    "    best_model = None\n",
    "    \n",
    "    for epoch in range(1, CONFIG.N_EPOCHS+1):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        for features, labels in tqdm(iter(train_loader)):\n",
    "            features = features.float().to(device)\n",
    "            labels = labels.float().to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model(features)\n",
    "            loss = criterion(output, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss.append(loss.item())\n",
    "                    \n",
    "        _val_loss, _val_score = validation(model, criterion, val_loader, device)\n",
    "        _train_loss = np.mean(train_loss)\n",
    "        print(f'Epoch [{epoch}], Train Loss : [{_train_loss:.5f}] Val Loss : [{_val_loss:.5f}] Val AUC : [{_val_score:.5f}]')\n",
    "            \n",
    "        if best_val_score < _val_score:\n",
    "            best_val_score = _val_score\n",
    "            best_model = model\n",
    "    \n",
    "    return best_model\n",
    "\n",
    "def multiLabel_AUC(y_true, y_scores):\n",
    "    auc_scores = []\n",
    "    for i in range(y_true.shape[1]):\n",
    "        auc = roc_auc_score(y_true[:, i], y_scores[:, i])\n",
    "        auc_scores.append(auc)\n",
    "    mean_auc_score = np.mean(auc_scores)\n",
    "    return mean_auc_score\n",
    "    \n",
    "def validation(model, criterion, val_loader, device):\n",
    "    model.eval()\n",
    "    val_loss, all_labels, all_probs = [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for features, labels in tqdm(iter(val_loader)):\n",
    "            features = features.float().to(device)\n",
    "            labels = labels.float().to(device)\n",
    "            \n",
    "            probs = model(features)\n",
    "            \n",
    "            loss = criterion(probs, labels)\n",
    "\n",
    "            val_loss.append(loss.item())\n",
    "\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "            all_probs.append(probs.cpu().numpy())\n",
    "        \n",
    "        _val_loss = np.mean(val_loss)\n",
    "\n",
    "        all_labels = np.concatenate(all_labels, axis=0)\n",
    "        all_probs = np.concatenate(all_probs, axis=0)\n",
    "        \n",
    "        auc_score = multiLabel_AUC(all_labels, all_probs)\n",
    "    \n",
    "    return _val_loss, auc_score\n",
    "\n",
    "model = MLP()\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr = CONFIG.LR)\n",
    "\n",
    "infer_model = train(model, optimizer, train_loader, val_loader, device)\n",
    "\n",
    "# Unlabeled 데이터에 대해 pseudo-labeling\n",
    "unlabeled_path = './unlabeled_data'\n",
    "unlabeled_files = [os.path.join(unlabeled_path, f) for f in os.listdir(unlabeled_path) if f.endswith('.ogg')]\n",
    "\n",
    "def get_mfcc_feature_from_files(file_paths):\n",
    "    features = []\n",
    "    for file_path in tqdm(file_paths):\n",
    "        y, sr = librosa.load(file_path, sr=CONFIG.SR)\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=CONFIG.N_MFCC)\n",
    "        mfcc = np.mean(mfcc.T, axis=0)\n",
    "        features.append(mfcc)\n",
    "    return features\n",
    "\n",
    "unlabeled_features = get_mfcc_feature_from_files(unlabeled_files)\n",
    "unlabeled_features = np.array(unlabeled_features)\n",
    "unlabeled_dataset = CustomDataset(unlabeled_features, torch.zeros((len(unlabeled_features), CONFIG.N_CLASSES)))\n",
    "unlabeled_loader = DataLoader(unlabeled_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=False)\n",
    "\n",
    "def pseudo_labeling(model, loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    pseudo_labels = []\n",
    "    with torch.no_grad():\n",
    "        for features, _ in tqdm(iter(loader)):\n",
    "            features = features.float().to(device)\n",
    "            probs = model(features)\n",
    "            pseudo_labels.append(probs.cpu().detach().numpy())\n",
    "    return np.concatenate(pseudo_labels, axis=0)\n",
    "\n",
    "pseudo_labels = pseudo_labeling(infer_model, unlabeled_loader, device)\n",
    "pseudo_labels = (pseudo_labels > 0.7).astype(int)  # 임계값 0.7를 사용하여 pseudo-label 생성\n",
    "\n",
    "# Pseudo-labeled 데이터를 학습 데이터에 추가\n",
    "pseudo_labeled_dataset = CustomDataset(unlabeled_features, torch.tensor(pseudo_labels).float())\n",
    "train_dataset_combined = torch.utils.data.ConcatDataset([train_dataset, pseudo_labeled_dataset])\n",
    "train_loader_combined = DataLoader(train_dataset_combined, batch_size=CONFIG.BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# 모델을 pseudo-labeled 데이터를 포함하여 재학습\n",
    "infer_model = train(model, optimizer, train_loader_combined, val_loader, device)\n",
    "\n",
    "# 테스트 데이터 예측\n",
    "test = pd.read_csv('./test.csv')\n",
    "test_features, _ = get_mfcc_feature(test, False)\n",
    "test_features = np.array(test_features)\n",
    "test_dataset = CustomDataset(test_features, torch.zeros((len(test_features), CONFIG.N_CLASSES)))\n",
    "test_loader = DataLoader(test_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=False)\n",
    "\n",
    "def inference(model, test_loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for features, _ in tqdm(iter(test_loader)):\n",
    "            features = features.float().to(device)\n",
    "            probs = model(features)\n",
    "            probs  = probs.cpu().detach().numpy()\n",
    "            predictions += probs.tolist()\n",
    "    return predictions\n",
    "\n",
    "preds = inference(infer_model, test_loader, device)\n",
    "\n",
    "submit = pd.read_csv('./sample_submission.csv')\n",
    "submit.iloc[:, 1:] = preds\n",
    "submit.head()\n",
    "\n",
    "submit.to_csv('./pseudo_label_submit2.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import librosa\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "class Config:\n",
    "    SR = 32000\n",
    "    N_MFCC = 13\n",
    "    ROOT_FOLDER = './'\n",
    "    N_CLASSES = 2\n",
    "    BATCH_SIZE = 64  # Reduce batch size for better GPU memory utilization\n",
    "    N_EPOCHS = 20  # Increase the number of epochs\n",
    "    LR = 1e-4  # Reduce learning rate for finer adjustments\n",
    "    SEED = 42\n",
    "\n",
    "CONFIG = Config()\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CONFIG.SEED)\n",
    "\n",
    "# 데이터 로드 및 전처리\n",
    "df = pd.read_csv('./train.csv')\n",
    "\n",
    "def label_encoder(column):\n",
    "    le = LabelEncoder().fit(column)\n",
    "    print(column.name, le.classes_)\n",
    "    return le.transform(column)\n",
    "\n",
    "df['class'] = label_encoder(df['label'])\n",
    "\n",
    "# MFCC 특징 추출\n",
    "def get_mfcc_feature(df, train_mode=True):\n",
    "    features = []\n",
    "    labels = []\n",
    "    for _, row in tqdm(df.iterrows()):\n",
    "        y, sr = librosa.load(row['path'], sr=CONFIG.SR)\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=CONFIG.N_MFCC)\n",
    "        mfcc = np.mean(mfcc.T, axis=0)\n",
    "        features.append(mfcc)\n",
    "\n",
    "        if train_mode:\n",
    "            label = row['class']\n",
    "            labels.append(label)\n",
    "\n",
    "    return features, labels\n",
    "\n",
    "features, labels = get_mfcc_feature(df, True)\n",
    "feature_df = pd.DataFrame({'features': features, 'class': labels})\n",
    "\n",
    "X = np.array(feature_df['features'].tolist())\n",
    "y = np.array(feature_df['class'].tolist())\n",
    "\n",
    "# 이상치 제거\n",
    "iso = IsolationForest(contamination=0.1, random_state=CONFIG.SEED)\n",
    "yhat = iso.fit_predict(X)\n",
    "mask = yhat != -1\n",
    "X, y = X[mask, :], y[mask]\n",
    "\n",
    "# 데이터 불균형 해결 (SMOTE 사용)\n",
    "smote = SMOTE(random_state=CONFIG.SEED)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "y_resampled = torch.tensor(y_resampled).long()  # 정수형으로 변환\n",
    "y_resampled = torch.nn.functional.one_hot(y_resampled, num_classes=CONFIG.N_CLASSES).float()\n",
    "\n",
    "# 데이터 분할\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=CONFIG.SEED)\n",
    "\n",
    "# PyTorch Dataset 클래스 정의\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, mfcc, label):\n",
    "        self.mfcc = mfcc\n",
    "        self.label = label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mfcc)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.mfcc[index], self.label[index]\n",
    "\n",
    "train_dataset = CustomDataset(X_train, y_train)\n",
    "val_dataset = CustomDataset(X_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# MLP 모델 정의\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim=CONFIG.N_MFCC, hidden_dims=[256, 512, 256], output_dim=CONFIG.N_CLASSES):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dims[0])\n",
    "        self.fc2 = nn.Linear(hidden_dims[0], hidden_dims[1])\n",
    "        self.fc3 = nn.Linear(hidden_dims[1], hidden_dims[2])\n",
    "        self.fc4 = nn.Linear(hidden_dims[2], output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "# 모델 학습 함수 정의\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def train(model, optimizer, train_loader, val_loader, device):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss().to(device)\n",
    "    \n",
    "    best_val_score = 0\n",
    "    best_model = None\n",
    "    \n",
    "    for epoch in range(1, CONFIG.N_EPOCHS+1):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        for features, labels in tqdm(iter(train_loader)):\n",
    "            features = features.float().to(device)\n",
    "            labels = labels.float().to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model(features)\n",
    "            loss = criterion(output, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss.append(loss.item())\n",
    "                    \n",
    "        _val_loss, _val_score = validation(model, criterion, val_loader, device)\n",
    "        _train_loss = np.mean(train_loss)\n",
    "        print(f'Epoch [{epoch}], Train Loss : [{_train_loss:.5f}] Val Loss : [{_val_loss:.5f}] Val AUC : [{_val_score:.5f}]')\n",
    "            \n",
    "        if best_val_score < _val_score:\n",
    "            best_val_score = _val_score\n",
    "            best_model = model\n",
    "    \n",
    "    return best_model\n",
    "\n",
    "def multiLabel_AUC(y_true, y_scores):\n",
    "    auc_scores = []\n",
    "    for i in range(y_true.shape[1]):\n",
    "        auc = roc_auc_score(y_true[:, i], y_scores[:, i])\n",
    "        auc_scores.append(auc)\n",
    "    mean_auc_score = np.mean(auc_scores)\n",
    "    return mean_auc_score\n",
    "    \n",
    "def validation(model, criterion, val_loader, device):\n",
    "    model.eval()\n",
    "    val_loss, all_labels, all_probs = [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for features, labels in tqdm(iter(val_loader)):\n",
    "            features = features.float().to(device)\n",
    "            labels = labels.float().to(device)\n",
    "            \n",
    "            probs = model(features)\n",
    "            \n",
    "            loss = criterion(probs, labels)\n",
    "\n",
    "            val_loss.append(loss.item())\n",
    "\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "            all_probs.append(probs.cpu().numpy())\n",
    "        \n",
    "        _val_loss = np.mean(val_loss)\n",
    "\n",
    "        all_labels = np.concatenate(all_labels, axis=0)\n",
    "        all_probs = np.concatenate(all_probs, axis=0)\n",
    "        \n",
    "        auc_score = multiLabel_AUC(all_labels, all_probs)\n",
    "    \n",
    "    return _val_loss, auc_score\n",
    "\n",
    "model = MLP()\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr = CONFIG.LR)\n",
    "\n",
    "infer_model = train(model, optimizer, train_loader, val_loader, device)\n",
    "\n",
    "# Unlabeled 데이터에 대해 pseudo-labeling\n",
    "unlabeled_path = './unlabeled_data'\n",
    "unlabeled_files = [os.path.join(unlabeled_path, f) for f in os.listdir(unlabeled_path) if f.endswith('.ogg')]\n",
    "\n",
    "def get_mfcc_feature_from_files(file_paths):\n",
    "    features = []\n",
    "    for file_path in tqdm(file_paths):\n",
    "        y, sr = librosa.load(file_path, sr=CONFIG.SR)\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=CONFIG.N_MFCC)\n",
    "        mfcc = np.mean(mfcc.T, axis=0)\n",
    "        features.append(mfcc)\n",
    "    return features\n",
    "\n",
    "unlabeled_features = get_mfcc_feature_from_files(unlabeled_files)\n",
    "unlabeled_features = np.array(unlabeled_features)\n",
    "unlabeled_dataset = CustomDataset(unlabeled_features, torch.zeros((len(unlabeled_features), CONFIG.N_CLASSES)))\n",
    "unlabeled_loader = DataLoader(unlabeled_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=False)\n",
    "\n",
    "def pseudo_labeling(model, loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    pseudo_labels = []\n",
    "    with torch.no_grad():\n",
    "        for features, _ in tqdm(iter(loader)):\n",
    "            features = features.float().to(device)\n",
    "            probs = model(features)\n",
    "            pseudo_labels.append(probs.cpu().detach().numpy())\n",
    "    return np.concatenate(pseudo_labels, axis=0)\n",
    "\n",
    "pseudo_labels = pseudo_labeling(infer_model, unlabeled_loader, device)\n",
    "pseudo_labels = (pseudo_labels > 0.5).astype(int)  # 임계값 0.5를 사용하여 pseudo-label 생성\n",
    "\n",
    "# Pseudo-labeled 데이터를 학습 데이터에 추가\n",
    "pseudo_labeled_dataset = CustomDataset(unlabeled_features, torch.tensor(pseudo_labels).float())\n",
    "train_dataset_combined = torch.utils.data.ConcatDataset([train_dataset, pseudo_labeled_dataset])\n",
    "train_loader_combined = DataLoader(train_dataset_combined, batch_size=CONFIG.BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# 모델을 pseudo-labeled 데이터를 포함하여 재학습\n",
    "infer_model = train(model, optimizer, train_loader_combined, val_loader, device)\n",
    "\n",
    "# 테스트 데이터 예측\n",
    "test = pd.read_csv('./test.csv')\n",
    "test_features, _ = get_mfcc_feature(test, False)\n",
    "test_features = np.array(test_features)\n",
    "test_dataset = CustomDataset(test_features, torch.zeros((len(test_features), CONFIG.N_CLASSES)))\n",
    "test_loader = DataLoader(test_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=False)\n",
    "\n",
    "def inference(model, test_loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for features, _ in tqdm(iter(test_loader)):\n",
    "            features = features.float().to(device)\n",
    "            probs = model(features)\n",
    "            probs  = probs.cpu().detach().numpy()\n",
    "            predictions += probs.tolist()\n",
    "    return predictions\n",
    "\n",
    "preds = inference(infer_model, test_loader, device)\n",
    "\n",
    "submit = pd.read_csv('./sample_submission.csv')\n",
    "submit.iloc[:, 1:] = preds\n",
    "submit.head()\n",
    "\n",
    "submit.to_csv('./pseudo_label_submit3.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import librosa\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "class Config:\n",
    "    SR = 32000\n",
    "    N_MFCC = 13\n",
    "    ROOT_FOLDER = './'\n",
    "    N_CLASSES = 2\n",
    "    BATCH_SIZE = 64  # Reduce batch size for better GPU memory utilization\n",
    "    N_EPOCHS = 50  # Increase the number of epochs\n",
    "    LR = 1e-4  # Reduce learning rate for finer adjustments\n",
    "    SEED = 42\n",
    "\n",
    "CONFIG = Config()\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CONFIG.SEED)\n",
    "\n",
    "# 데이터 로드 및 전처리\n",
    "df = pd.read_csv('./train.csv')\n",
    "\n",
    "def label_encoder(column):\n",
    "    le = LabelEncoder().fit(column)\n",
    "    print(column.name, le.classes_)\n",
    "    return le.transform(column)\n",
    "\n",
    "df['class'] = label_encoder(df['label'])\n",
    "\n",
    "# MFCC 특징 추출\n",
    "def get_mfcc_feature(df, train_mode=True):\n",
    "    features = []\n",
    "    labels = []\n",
    "    for _, row in tqdm(df.iterrows()):\n",
    "        y, sr = librosa.load(row['path'], sr=CONFIG.SR)\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=CONFIG.N_MFCC)\n",
    "        mfcc = np.mean(mfcc.T, axis=0)\n",
    "        features.append(mfcc)\n",
    "\n",
    "        if train_mode:\n",
    "            label = row['class']\n",
    "            labels.append(label)\n",
    "\n",
    "    return features, labels\n",
    "\n",
    "features, labels = get_mfcc_feature(df, True)\n",
    "feature_df = pd.DataFrame({'features': features, 'class': labels})\n",
    "\n",
    "X = np.array(feature_df['features'].tolist())\n",
    "y = np.array(feature_df['class'].tolist())\n",
    "\n",
    "# 이상치 제거\n",
    "iso = IsolationForest(contamination=0.1, random_state=CONFIG.SEED)\n",
    "yhat = iso.fit_predict(X)\n",
    "mask = yhat != -1\n",
    "X, y = X[mask, :], y[mask]\n",
    "\n",
    "# 데이터 불균형 해결 (SMOTE 사용)\n",
    "smote = SMOTE(random_state=CONFIG.SEED)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "y_resampled = torch.tensor(y_resampled).long()  # 정수형으로 변환\n",
    "y_resampled = torch.nn.functional.one_hot(y_resampled, num_classes=CONFIG.N_CLASSES).float()\n",
    "\n",
    "# 데이터 분할\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=CONFIG.SEED)\n",
    "\n",
    "# PyTorch Dataset 클래스 정의\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, mfcc, label):\n",
    "        self.mfcc = mfcc\n",
    "        self.label = label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mfcc)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.mfcc[index], self.label[index]\n",
    "\n",
    "train_dataset = CustomDataset(X_train, y_train)\n",
    "val_dataset = CustomDataset(X_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# MLP 모델 정의\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim=CONFIG.N_MFCC, hidden_dims=[256, 512, 256], output_dim=CONFIG.N_CLASSES):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dims[0])\n",
    "        self.fc2 = nn.Linear(hidden_dims[0], hidden_dims[1])\n",
    "        self.fc3 = nn.Linear(hidden_dims[1], hidden_dims[2])\n",
    "        self.fc4 = nn.Linear(hidden_dims[2], output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "# 모델 학습 함수 정의\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def train(model, optimizer, train_loader, val_loader, device):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss().to(device)\n",
    "    \n",
    "    best_val_score = 0\n",
    "    best_model = None\n",
    "    \n",
    "    for epoch in range(1, CONFIG.N_EPOCHS+1):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        for features, labels in tqdm(iter(train_loader)):\n",
    "            features = features.float().to(device)\n",
    "            labels = labels.float().to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model(features)\n",
    "            loss = criterion(output, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss.append(loss.item())\n",
    "                    \n",
    "        _val_loss, _val_score = validation(model, criterion, val_loader, device)\n",
    "        _train_loss = np.mean(train_loss)\n",
    "        print(f'Epoch [{epoch}], Train Loss : [{_train_loss:.5f}] Val Loss : [{_val_loss:.5f}] Val AUC : [{_val_score:.5f}]')\n",
    "            \n",
    "        if best_val_score < _val_score:\n",
    "            best_val_score = _val_score\n",
    "            best_model = model\n",
    "    \n",
    "    return best_model\n",
    "\n",
    "def multiLabel_AUC(y_true, y_scores):\n",
    "    auc_scores = []\n",
    "    for i in range(y_true.shape[1]):\n",
    "        auc = roc_auc_score(y_true[:, i], y_scores[:, i])\n",
    "        auc_scores.append(auc)\n",
    "    mean_auc_score = np.mean(auc_scores)\n",
    "    return mean_auc_score\n",
    "    \n",
    "def validation(model, criterion, val_loader, device):\n",
    "    model.eval()\n",
    "    val_loss, all_labels, all_probs = [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for features, labels in tqdm(iter(val_loader)):\n",
    "            features = features.float().to(device)\n",
    "            labels = labels.float().to(device)\n",
    "            \n",
    "            probs = model(features)\n",
    "            \n",
    "            loss = criterion(probs, labels)\n",
    "\n",
    "            val_loss.append(loss.item())\n",
    "\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "            all_probs.append(probs.cpu().numpy())\n",
    "        \n",
    "        _val_loss = np.mean(val_loss)\n",
    "\n",
    "        all_labels = np.concatenate(all_labels, axis=0)\n",
    "        all_probs = np.concatenate(all_probs, axis=0)\n",
    "        \n",
    "        auc_score = multiLabel_AUC(all_labels, all_probs)\n",
    "    \n",
    "    return _val_loss, auc_score\n",
    "\n",
    "model = MLP()\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr = CONFIG.LR)\n",
    "\n",
    "infer_model = train(model, optimizer, train_loader, val_loader, device)\n",
    "\n",
    "# Unlabeled 데이터에 대해 pseudo-labeling\n",
    "unlabeled_path = './unlabeled_data'\n",
    "unlabeled_files = [os.path.join(unlabeled_path, f) for f in os.listdir(unlabeled_path) if f.endswith('.ogg')]\n",
    "\n",
    "def get_mfcc_feature_from_files(file_paths):\n",
    "    features = []\n",
    "    for file_path in tqdm(file_paths):\n",
    "        y, sr = librosa.load(file_path, sr=CONFIG.SR)\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=CONFIG.N_MFCC)\n",
    "        mfcc = np.mean(mfcc.T, axis=0)\n",
    "        features.append(mfcc)\n",
    "    return features\n",
    "\n",
    "unlabeled_features = get_mfcc_feature_from_files(unlabeled_files)\n",
    "unlabeled_features = np.array(unlabeled_features)\n",
    "unlabeled_dataset = CustomDataset(unlabeled_features, torch.zeros((len(unlabeled_features), CONFIG.N_CLASSES)))\n",
    "unlabeled_loader = DataLoader(unlabeled_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=False)\n",
    "\n",
    "def pseudo_labeling(model, loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    pseudo_labels = []\n",
    "    with torch.no_grad():\n",
    "        for features, _ in tqdm(iter(loader)):\n",
    "            features = features.float().to(device)\n",
    "            probs = model(features)\n",
    "            pseudo_labels.append(probs.cpu().detach().numpy())\n",
    "    return np.concatenate(pseudo_labels, axis=0)\n",
    "\n",
    "pseudo_labels = pseudo_labeling(infer_model, unlabeled_loader, device)\n",
    "pseudo_labels = (pseudo_labels > 0.5).astype(int)  # 임계값 0.5를 사용하여 pseudo-label 생성\n",
    "\n",
    "# Pseudo-labeled 데이터를 학습 데이터에 추가\n",
    "pseudo_labeled_dataset = CustomDataset(unlabeled_features, torch.tensor(pseudo_labels).float())\n",
    "train_dataset_combined = torch.utils.data.ConcatDataset([train_dataset, pseudo_labeled_dataset])\n",
    "train_loader_combined = DataLoader(train_dataset_combined, batch_size=CONFIG.BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# 모델을 pseudo-labeled 데이터를 포함하여 재학습\n",
    "infer_model = train(model, optimizer, train_loader_combined, val_loader, device)\n",
    "\n",
    "# 테스트 데이터 예측\n",
    "test = pd.read_csv('./test.csv')\n",
    "test_features, _ = get_mfcc_feature(test, False)\n",
    "test_features = np.array(test_features)\n",
    "test_dataset = CustomDataset(test_features, torch.zeros((len(test_features), CONFIG.N_CLASSES)))\n",
    "test_loader = DataLoader(test_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=False)\n",
    "\n",
    "def inference(model, test_loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for features, _ in tqdm(iter(test_loader)):\n",
    "            features = features.float().to(device)\n",
    "            probs = model(features)\n",
    "            probs  = probs.cpu().detach().numpy()\n",
    "            predictions += probs.tolist()\n",
    "    return predictions\n",
    "\n",
    "preds = inference(infer_model, test_loader, device)\n",
    "\n",
    "submit = pd.read_csv('./sample_submission.csv')\n",
    "submit.iloc[:, 1:] = preds\n",
    "submit.head()\n",
    "\n",
    "submit.to_csv('./pseudo_label_submit4.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import librosa\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import warnings\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "class Config:\n",
    "    SR = 32000\n",
    "    N_MFCC = 13\n",
    "    ROOT_FOLDER = './'\n",
    "    N_CLASSES = 2\n",
    "    BATCH_SIZE = 64\n",
    "    N_EPOCHS = 50\n",
    "    LR = 1e-4\n",
    "    SEED = 42\n",
    "    DROPOUT_RATE = 0.5\n",
    "    WEIGHT_DECAY = 1e-4\n",
    "    HIDDEN_DIMS = [256, 512, 256]\n",
    "    LR_SCHEDULER_STEP_SIZE = 10\n",
    "    LR_SCHEDULER_GAMMA = 0.5\n",
    "\n",
    "CONFIG = Config()\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CONFIG.SEED)\n",
    "\n",
    "# 데이터 로드 및 전처리\n",
    "df = pd.read_csv('./train.csv')\n",
    "\n",
    "def label_encoder(column):\n",
    "    le = LabelEncoder().fit(column)\n",
    "    print(column.name, le.classes_)\n",
    "    return le.transform(column)\n",
    "\n",
    "df['class'] = label_encoder(df['label'])\n",
    "\n",
    "# MFCC 특징 추출\n",
    "def get_mfcc_feature(df, train_mode=True):\n",
    "    features = []\n",
    "    labels = []\n",
    "    for _, row in tqdm(df.iterrows()):\n",
    "        y, sr = librosa.load(row['path'], sr=CONFIG.SR)\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=CONFIG.N_MFCC)\n",
    "        mfcc = np.mean(mfcc.T, axis=0)\n",
    "        features.append(mfcc)\n",
    "\n",
    "        if train_mode:\n",
    "            label = row['class']\n",
    "            labels.append(label)\n",
    "\n",
    "    return features, labels\n",
    "\n",
    "features, labels = get_mfcc_feature(df, True)\n",
    "feature_df = pd.DataFrame({'features': features, 'class': labels})\n",
    "\n",
    "X = np.array(feature_df['features'].tolist())\n",
    "y = np.array(feature_df['class'].tolist())\n",
    "\n",
    "# 이상치 제거\n",
    "iso = IsolationForest(contamination=0.1, random_state=CONFIG.SEED)\n",
    "yhat = iso.fit_predict(X)\n",
    "mask = yhat != -1\n",
    "X, y = X[mask, :], y[mask]\n",
    "\n",
    "# 데이터 불균형 해결 (SMOTE 사용)\n",
    "smote = SMOTE(random_state=CONFIG.SEED)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "y_resampled = torch.tensor(y_resampled).long()  # 정수형으로 변환\n",
    "y_resampled = torch.nn.functional.one_hot(y_resampled, num_classes=CONFIG.N_CLASSES).float()\n",
    "\n",
    "# 데이터 분할\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=CONFIG.SEED)\n",
    "\n",
    "# PyTorch Dataset 클래스 정의\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, mfcc, label):\n",
    "        self.mfcc = mfcc\n",
    "        self.label = label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mfcc)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.mfcc[index], self.label[index]\n",
    "\n",
    "train_dataset = CustomDataset(X_train, y_train)\n",
    "val_dataset = CustomDataset(X_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# MLP 모델 정의\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim=CONFIG.N_MFCC, hidden_dims=CONFIG.HIDDEN_DIMS, output_dim=CONFIG.N_CLASSES, dropout_rate=CONFIG.DROPOUT_RATE):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dims[0])\n",
    "        self.fc2 = nn.Linear(hidden_dims[0], hidden_dims[1])\n",
    "        self.fc3 = nn.Linear(hidden_dims[1], hidden_dims[2])\n",
    "        self.fc4 = nn.Linear(hidden_dims[2], output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # Xavier initialization\n",
    "        nn.init.xavier_normal_(self.fc1.weight)\n",
    "        nn.init.xavier_normal_(self.fc2.weight)\n",
    "        nn.init.xavier_normal_(self.fc3.weight)\n",
    "        nn.init.xavier_normal_(self.fc4.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "# 모델 학습 함수 정의\n",
    "def train(model, optimizer, train_loader, val_loader, device):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss().to(device)\n",
    "    scheduler = StepLR(optimizer, step_size=CONFIG.LR_SCHEDULER_STEP_SIZE, gamma=CONFIG.LR_SCHEDULER_GAMMA)\n",
    "    \n",
    "    best_val_score = 0\n",
    "    best_model = None\n",
    "    \n",
    "    for epoch in range(1, CONFIG.N_EPOCHS+1):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        for features, labels in tqdm(iter(train_loader)):\n",
    "            features = features.float().to(device)\n",
    "            labels = labels.float().to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model(features)\n",
    "            loss = criterion(output, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss.append(loss.item())\n",
    "                    \n",
    "        _val_loss, _val_score = validation(model, criterion, val_loader, device)\n",
    "        _train_loss = np.mean(train_loss)\n",
    "        scheduler.step()\n",
    "        \n",
    "        print(f'Epoch [{epoch}], Train Loss : [{_train_loss:.5f}] Val Loss : [{_val_loss:.5f}] Val AUC : [{_val_score:.5f}]')\n",
    "            \n",
    "        if best_val_score < _val_score:\n",
    "            best_val_score = _val_score\n",
    "            best_model = model\n",
    "    \n",
    "    return best_model\n",
    "\n",
    "def multiLabel_AUC(y_true, y_scores):\n",
    "    auc_scores = []\n",
    "    for i in range(y_true.shape[1]):\n",
    "        auc = roc_auc_score(y_true[:, i], y_scores[:, i])\n",
    "        auc_scores.append(auc)\n",
    "    mean_auc_score = np.mean(auc_scores)\n",
    "    return mean_auc_score\n",
    "    \n",
    "def validation(model, criterion, val_loader, device):\n",
    "    model.eval()\n",
    "    val_loss, all_labels, all_probs = [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for features, labels in tqdm(iter(val_loader)):\n",
    "            features = features.float().to(device)\n",
    "            labels = labels.float().to(device)\n",
    "            \n",
    "            probs = model(features)\n",
    "            \n",
    "            loss = criterion(probs, labels)\n",
    "\n",
    "            val_loss.append(loss.item())\n",
    "\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "            all_probs.append(probs.cpu().numpy())\n",
    "        \n",
    "        _val_loss = np.mean(val_loss)\n",
    "\n",
    "        all_labels = np.concatenate(all_labels, axis=0)\n",
    "        all_probs = np.concatenate(all_probs, axis=0)\n",
    "        \n",
    "        auc_score = multiLabel_AUC(all_labels, all_probs)\n",
    "    \n",
    "    return _val_loss, auc_score\n",
    "\n",
    "model = MLP()\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr = CONFIG.LR, weight_decay=CONFIG.WEIGHT_DECAY)\n",
    "\n",
    "infer_model = train(model, optimizer, train_loader, val_loader, device)\n",
    "\n",
    "# Unlabeled 데이터에 대해 pseudo-labeling\n",
    "unlabeled_path = './unlabeled_data'\n",
    "unlabeled_files = [os.path.join(unlabeled_path, f) for f in os.listdir(unlabeled_path) if f.endswith('.ogg')]\n",
    "\n",
    "def get_mfcc_feature_from_files(file_paths):\n",
    "    features = []\n",
    "    for file_path in tqdm(file_paths):\n",
    "        y, sr = librosa.load(file_path, sr=CONFIG.SR)\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=CONFIG.N_MFCC)\n",
    "        mfcc = np.mean(mfcc.T, axis=0)\n",
    "        features.append(mfcc)\n",
    "    return features\n",
    "\n",
    "unlabeled_features = get_mfcc_feature_from_files(unlabeled_files)\n",
    "unlabeled_features = np.array(unlabeled_features)\n",
    "unlabeled_dataset = CustomDataset(unlabeled_features, torch.zeros((len(unlabeled_features), CONFIG.N_CLASSES)))\n",
    "unlabeled_loader = DataLoader(unlabeled_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=False)\n",
    "\n",
    "def pseudo_labeling(model, loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    pseudo_labels = []\n",
    "    with torch.no_grad():\n",
    "        for features, _ in tqdm(iter(loader)):\n",
    "            features = features.float().to(device)\n",
    "            probs = model(features)\n",
    "            pseudo_labels.append(probs.cpu().detach().numpy())\n",
    "    return np.concatenate(pseudo_labels, axis=0)\n",
    "\n",
    "pseudo_labels = pseudo_labeling(infer_model, unlabeled_loader, device)\n",
    "pseudo_labels = (pseudo_labels > 0.5).astype(int)  # 임계값 0.5를 사용하여 pseudo-label 생성\n",
    "\n",
    "# Pseudo-labeled 데이터를 학습 데이터에 추가\n",
    "pseudo_labeled_dataset = CustomDataset(unlabeled_features, torch.tensor(pseudo_labels).float())\n",
    "train_dataset_combined = torch.utils.data.ConcatDataset([train_dataset, pseudo_labeled_dataset])\n",
    "train_loader_combined = DataLoader(train_dataset_combined, batch_size=CONFIG.BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# 모델을 pseudo-labeled 데이터를 포함하여 재학습\n",
    "infer_model = train(model, optimizer, train_loader_combined, val_loader, device)\n",
    "\n",
    "# 테스트 데이터 예측\n",
    "test = pd.read_csv('./test.csv')\n",
    "test_features, _ = get_mfcc_feature(test, False)\n",
    "test_features = np.array(test_features)\n",
    "test_dataset = CustomDataset(test_features, torch.zeros((len(test_features), CONFIG.N_CLASSES)))\n",
    "test_loader = DataLoader(test_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=False)\n",
    "\n",
    "def inference(model, test_loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for features, _ in tqdm(iter(test_loader)):\n",
    "            features = features.float().to(device)\n",
    "            probs = model(features)\n",
    "            probs  = probs.cpu().detach().numpy()\n",
    "            predictions += probs.tolist()\n",
    "    return predictions\n",
    "\n",
    "preds = inference(infer_model, test_loader, device)\n",
    "\n",
    "submit = pd.read_csv('./sample_submission.csv')\n",
    "submit.iloc[:, 1:] = preds\n",
    "submit.head()\n",
    "\n",
    "submit.to_csv('./pseudo_label_submit5.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import librosa\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import warnings\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import optuna\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "class Config:\n",
    "    SR = 32000\n",
    "    N_MFCC = 13\n",
    "    ROOT_FOLDER = './'\n",
    "    N_CLASSES = 2\n",
    "    BATCH_SIZE = 64\n",
    "    N_EPOCHS = 50\n",
    "    LR = 1e-4\n",
    "    SEED = 42\n",
    "    DROPOUT_RATE = 0.5\n",
    "    WEIGHT_DECAY = 1e-4\n",
    "    HIDDEN_DIMS = [256, 512, 256]\n",
    "    LR_SCHEDULER_STEP_SIZE = 10\n",
    "    LR_SCHEDULER_GAMMA = 0.5\n",
    "\n",
    "CONFIG = Config()\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CONFIG.SEED)\n",
    "\n",
    "# 데이터 로드 및 전처리\n",
    "df = pd.read_csv('./train.csv')\n",
    "\n",
    "def label_encoder(column):\n",
    "    le = LabelEncoder().fit(column)\n",
    "    print(column.name, le.classes_)\n",
    "    return le.transform(column)\n",
    "\n",
    "df['class'] = label_encoder(df['label'])\n",
    "\n",
    "# MFCC 특징 추출\n",
    "def get_mfcc_feature(df, train_mode=True):\n",
    "    features = []\n",
    "    labels = []\n",
    "    for _, row in tqdm(df.iterrows()):\n",
    "        y, sr = librosa.load(row['path'], sr=CONFIG.SR)\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=CONFIG.N_MFCC)\n",
    "        mfcc = np.mean(mfcc.T, axis=0)\n",
    "        features.append(mfcc)\n",
    "\n",
    "        if train_mode:\n",
    "            label = row['class']\n",
    "            labels.append(label)\n",
    "\n",
    "    return features, labels\n",
    "\n",
    "features, labels = get_mfcc_feature(df, True)\n",
    "feature_df = pd.DataFrame({'features': features, 'class': labels})\n",
    "\n",
    "X = np.array(feature_df['features'].tolist())\n",
    "y = np.array(feature_df['class'].tolist())\n",
    "\n",
    "# 이상치 제거\n",
    "iso = IsolationForest(contamination=0.1, random_state=CONFIG.SEED)\n",
    "yhat = iso.fit_predict(X)\n",
    "mask = yhat != -1\n",
    "X, y = X[mask, :], y[mask]\n",
    "\n",
    "# 데이터 불균형 해결 (SMOTE 사용)\n",
    "smote = SMOTE(random_state=CONFIG.SEED)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "y_resampled = torch.tensor(y_resampled).long()  # 정수형으로 변환\n",
    "y_resampled = torch.nn.functional.one_hot(y_resampled, num_classes=CONFIG.N_CLASSES).float()\n",
    "\n",
    "# 데이터 분할\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=CONFIG.SEED)\n",
    "\n",
    "# PyTorch Dataset 클래스 정의\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, mfcc, label):\n",
    "        self.mfcc = mfcc\n",
    "        self.label = label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mfcc)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.mfcc[index], self.label[index]\n",
    "\n",
    "train_dataset = CustomDataset(X_train, y_train)\n",
    "val_dataset = CustomDataset(X_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# MLP 모델 정의\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim=CONFIG.N_MFCC, hidden_dims=CONFIG.HIDDEN_DIMS, output_dim=CONFIG.N_CLASSES, dropout_rate=CONFIG.DROPOUT_RATE):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dims[0])\n",
    "        self.fc2 = nn.Linear(hidden_dims[0], hidden_dims[1])\n",
    "        self.fc3 = nn.Linear(hidden_dims[1], hidden_dims[2])\n",
    "        self.fc4 = nn.Linear(hidden_dims[2], output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # Xavier initialization\n",
    "        nn.init.xavier_normal_(self.fc1.weight)\n",
    "        nn.init.xavier_normal_(self.fc2.weight)\n",
    "        nn.init.xavier_normal_(self.fc3.weight)\n",
    "        nn.init.xavier_normal_(self.fc4.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "# 모델 학습 함수 정의\n",
    "def train(model, optimizer, train_loader, val_loader, device):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss().to(device)\n",
    "    scheduler = StepLR(optimizer, step_size=CONFIG.LR_SCHEDULER_STEP_SIZE, gamma=CONFIG.LR_SCHEDULER_GAMMA)\n",
    "    \n",
    "    best_val_score = 0\n",
    "    best_model = None\n",
    "    \n",
    "    for epoch in range(1, CONFIG.N_EPOCHS+1):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        for features, labels in tqdm(iter(train_loader)):\n",
    "            features = features.float().to(device)\n",
    "            labels = labels.float().to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model(features)\n",
    "            loss = criterion(output, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss.append(loss.item())\n",
    "                    \n",
    "        _val_loss, _val_score = validation(model, criterion, val_loader, device)\n",
    "        _train_loss = np.mean(train_loss)\n",
    "        scheduler.step()\n",
    "        \n",
    "        print(f'Epoch [{epoch}], Train Loss : [{_train_loss:.5f}] Val Loss : [{_val_loss:.5f}] Val AUC : [{_val_score:.5f}]')\n",
    "            \n",
    "        if best_val_score < _val_score:\n",
    "            best_val_score = _val_score\n",
    "            best_model = model\n",
    "    \n",
    "    return best_model\n",
    "\n",
    "def multiLabel_AUC(y_true, y_scores):\n",
    "    auc_scores = []\n",
    "    for i in range(y_true.shape[1]):\n",
    "        auc = roc_auc_score(y_true[:, i], y_scores[:, i])\n",
    "        auc_scores.append(auc)\n",
    "    mean_auc_score = np.mean(auc_scores)\n",
    "    return mean_auc_score\n",
    "    \n",
    "def validation(model, criterion, val_loader, device):\n",
    "    model.eval()\n",
    "    val_loss, all_labels, all_probs = [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for features, labels in tqdm(iter(val_loader)):\n",
    "            features = features.float().to(device)\n",
    "            labels = labels.float().to(device)\n",
    "            \n",
    "            probs = model(features)\n",
    "            \n",
    "            loss = criterion(probs, labels)\n",
    "\n",
    "            val_loss.append(loss.item())\n",
    "\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "            all_probs.append(probs.cpu().numpy())\n",
    "        \n",
    "        _val_loss = np.mean(val_loss)\n",
    "\n",
    "        all_labels = np.concatenate(all_labels, axis=0)\n",
    "        all_probs = np.concatenate(all_probs, axis=0)\n",
    "        \n",
    "        auc_score = multiLabel_AUC(all_labels, all_probs)\n",
    "    \n",
    "    return _val_loss, auc_score\n",
    "\n",
    "def get_mfcc_feature_from_files(file_paths):\n",
    "    features = []\n",
    "    for file_path in tqdm(file_paths):\n",
    "        y, sr = librosa.load(file_path, sr=CONFIG.SR)\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=CONFIG.N_MFCC)\n",
    "        mfcc = np.mean(mfcc.T, axis=0)\n",
    "        features.append(mfcc)\n",
    "    return features\n",
    "\n",
    "def pseudo_labeling(model, loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    pseudo_labels = []\n",
    "    with torch.no_grad():\n",
    "        for features, _ in tqdm(iter(loader)):\n",
    "            features = features.float().to(device)\n",
    "            probs = model(features)\n",
    "            pseudo_labels.append(probs.cpu().detach().numpy())\n",
    "    return np.concatenate(pseudo_labels, axis=0)\n",
    "\n",
    "def inference(model, test_loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for features, _ in tqdm(iter(test_loader)):\n",
    "            features = features.float().to(device)\n",
    "            probs = model(features)\n",
    "            probs  = probs.cpu().detach().numpy()\n",
    "            predictions += probs.tolist()\n",
    "    return predictions\n",
    "\n",
    "def objective(trial):\n",
    "    CONFIG.BATCH_SIZE = trial.suggest_int('batch_size', 32, 128)\n",
    "    CONFIG.LR = trial.suggest_loguniform('lr', 1e-5, 1e-3)\n",
    "    CONFIG.DROPOUT_RATE = trial.suggest_uniform('dropout_rate', 0.2, 0.7)\n",
    "    CONFIG.WEIGHT_DECAY = trial.suggest_loguniform('weight_decay', 1e-5, 1e-3)\n",
    "    CONFIG.HIDDEN_DIMS = [trial.suggest_int(f'hidden_dim_{i}', 128, 512) for i in range(3)]\n",
    "    CONFIG.LR_SCHEDULER_STEP_SIZE = trial.suggest_int('lr_scheduler_step_size', 5, 20)\n",
    "    CONFIG.LR_SCHEDULER_GAMMA = trial.suggest_uniform('lr_scheduler_gamma', 0.1, 0.9)\n",
    "\n",
    "    model = MLP()\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=CONFIG.LR, weight_decay=CONFIG.WEIGHT_DECAY)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    infer_model = train(model, optimizer, train_loader, val_loader, device)\n",
    "\n",
    "    _val_loss, _val_score = validation(infer_model, nn.BCELoss().to(device), val_loader, device)\n",
    "    \n",
    "    return _val_score\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "print(\"Number of finished trials: \", len(study.trials))\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: \", trial.value)\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n",
    "\n",
    "# 최적의 하이퍼파라미터 설정\n",
    "CONFIG.BATCH_SIZE = trial.params['batch_size']\n",
    "CONFIG.LR = trial.params['lr']\n",
    "CONFIG.DROPOUT_RATE = trial.params['dropout_rate']\n",
    "CONFIG.WEIGHT_DECAY = trial.params['weight_decay']\n",
    "CONFIG.HIDDEN_DIMS = [trial.params[f'hidden_dim_{i}'] for i in range(3)]\n",
    "CONFIG.LR_SCHEDULER_STEP_SIZE = trial.params['lr_scheduler_step_size']\n",
    "CONFIG.LR_SCHEDULER_GAMMA = trial.params['lr_scheduler_gamma']\n",
    "\n",
    "# 최적의 하이퍼파라미터로 최종 모델 학습 및 테스트 예측 수행\n",
    "model = MLP()\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=CONFIG.LR, weight_decay=CONFIG.WEIGHT_DECAY)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=False)\n",
    "\n",
    "infer_model = train(model, optimizer, train_loader, val_loader, device)\n",
    "\n",
    "# Unlabeled 데이터에 대해 pseudo-labeling\n",
    "unlabeled_path = './unlabeled_data'\n",
    "unlabeled_files = [os.path.join(unlabeled_path, f) for f in os.listdir(unlabeled_path) if f.endswith('.ogg')]\n",
    "\n",
    "unlabeled_features = get_mfcc_feature_from_files(unlabeled_files)\n",
    "unlabeled_features = np.array(unlabeled_features)\n",
    "unlabeled_dataset = CustomDataset(unlabeled_features, torch.zeros((len(unlabeled_features), CONFIG.N_CLASSES)))\n",
    "unlabeled_loader = DataLoader(unlabeled_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=False)\n",
    "\n",
    "pseudo_labels = pseudo_labeling(infer_model, unlabeled_loader, device)\n",
    "pseudo_labels = (pseudo_labels > 0.5).astype(int)\n",
    "\n",
    "# Pseudo-labeled 데이터를 학습 데이터에 추가\n",
    "pseudo_labeled_dataset = CustomDataset(unlabeled_features, torch.tensor(pseudo_labels).float())\n",
    "train_dataset_combined = torch.utils.data.ConcatDataset([train_dataset, pseudo_labeled_dataset])\n",
    "train_loader_combined = DataLoader(train_dataset_combined, batch_size=CONFIG.BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# 모델을 pseudo-labeled 데이터를 포함하여 재학습\n",
    "infer_model = train(model, optimizer, train_loader_combined, val_loader, device)\n",
    "\n",
    "# 테스트 데이터 예측\n",
    "test = pd.read_csv('./test.csv')\n",
    "test_features, _ = get_mfcc_feature(test, False)\n",
    "test_features = np.array(test_features)\n",
    "test_dataset = CustomDataset(test_features, torch.zeros((len(test_features), CONFIG.N_CLASSES)))\n",
    "test_loader = DataLoader(test_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=False)\n",
    "\n",
    "preds = inference(infer_model, test_loader, device)\n",
    "\n",
    "submit = pd.read_csv('./sample_submission.csv')\n",
    "submit.iloc[:, 1:] = preds\n",
    "submit.head()\n",
    "\n",
    "submit.to_csv('./pseudo_label_submit_optuna.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "class Config:\n",
    "    SR = 32000\n",
    "    N_MFCC = 13\n",
    "    ROOT_FOLDER = './'\n",
    "    N_CLASSES = 2\n",
    "    BATCH_SIZE = 96\n",
    "    N_EPOCHS = 5\n",
    "    LR = 3e-4\n",
    "    SEED = 42\n",
    "    DROPOUT_RATE = 0.5\n",
    "    HIDDEN_DIMS = [128, 256, 128]\n",
    "    INITIALIZATION = 'xavier'\n",
    "    ACTIVATION_FUNCTION = 'relu'\n",
    "\n",
    "CONFIG = Config()\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CONFIG.SEED)\n",
    "\n",
    "# 데이터 로드 및 전처리\n",
    "df = pd.read_csv('./train.csv')\n",
    "\n",
    "def label_encoder(column):\n",
    "    le = LabelEncoder().fit(column)\n",
    "    print(column.name, le.classes_)\n",
    "    return le.transform(column)\n",
    "\n",
    "df['class'] = label_encoder(df['label'])\n",
    "\n",
    "# MFCC 특징 추출\n",
    "def get_mfcc_feature(df, train_mode=True):\n",
    "    features = []\n",
    "    labels = []\n",
    "    for _, row in tqdm(df.iterrows()):\n",
    "        y, sr = librosa.load(row['path'], sr=CONFIG.SR)\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=CONFIG.N_MFCC)\n",
    "        mfcc = np.mean(mfcc.T, axis=0)\n",
    "        features.append(mfcc)\n",
    "\n",
    "        if train_mode:\n",
    "            label = row['class']\n",
    "            labels.append(label)\n",
    "\n",
    "    return features, labels\n",
    "\n",
    "features, labels = get_mfcc_feature(df, True)\n",
    "feature_df = pd.DataFrame({'features': features, 'class': labels})\n",
    "\n",
    "X = np.array(feature_df['features'].tolist())\n",
    "y = np.array(feature_df['class'].tolist())\n",
    "\n",
    "# 데이터 불균형 해결\n",
    "ros = RandomOverSampler(random_state=CONFIG.SEED)\n",
    "X_resampled, y_resampled = ros.fit_resample(X, y)\n",
    "y_resampled = torch.tensor(y_resampled).long()  # 정수형으로 변환\n",
    "y_resampled = torch.nn.functional.one_hot(y_resampled, num_classes=CONFIG.N_CLASSES).float()\n",
    "\n",
    "# 데이터 분할\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=CONFIG.SEED)\n",
    "\n",
    "# PyTorch Dataset 클래스 정의\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, mfcc, label):\n",
    "        self.mfcc = mfcc\n",
    "        self.label = label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mfcc)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.mfcc[index], self.label[index]\n",
    "\n",
    "train_dataset = CustomDataset(X_train, y_train)\n",
    "val_dataset = CustomDataset(X_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# MLP 모델 정의\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim=CONFIG.N_MFCC, hidden_dims=[128, 256, 128], output_dim=CONFIG.N_CLASSES):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dims[0])\n",
    "        self.fc2 = nn.Linear(hidden_dims[0], hidden_dims[1])\n",
    "        self.fc3 = nn.Linear(hidden_dims[1], hidden_dims[2])\n",
    "        self.fc4 = nn.Linear(hidden_dims[2], output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "# 모델 학습 함수 정의\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def train(model, optimizer, train_loader, val_loader, device):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss().to(device)\n",
    "    \n",
    "    best_val_score = 0\n",
    "    best_model = None\n",
    "    \n",
    "    for epoch in range(1, CONFIG.N_EPOCHS+1):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        for features, labels in tqdm(iter(train_loader)):\n",
    "            features = features.float().to(device)\n",
    "            labels = labels.float().to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model(features)\n",
    "            loss = criterion(output, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss.append(loss.item())\n",
    "                    \n",
    "        _val_loss, _val_score = validation(model, criterion, val_loader, device)\n",
    "        _train_loss = np.mean(train_loss)\n",
    "        print(f'Epoch [{epoch}], Train Loss : [{_train_loss:.5f}] Val Loss : [{_val_loss:.5f}] Val AUC : [{_val_score:.5f}]')\n",
    "            \n",
    "        if best_val_score < _val_score:\n",
    "            best_val_score = _val_score\n",
    "            best_model = model\n",
    "    \n",
    "    return best_model\n",
    "\n",
    "def multiLabel_AUC(y_true, y_scores):\n",
    "    auc_scores = []\n",
    "    for i in range(y_true.shape[1]):\n",
    "        auc = roc_auc_score(y_true[:, i], y_scores[:, i])\n",
    "        auc_scores.append(auc)\n",
    "    mean_auc_score = np.mean(auc_scores)\n",
    "    return mean_auc_score\n",
    "    \n",
    "def validation(model, criterion, val_loader, device):\n",
    "    model.eval()\n",
    "    val_loss, all_labels, all_probs = [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for features, labels in tqdm(iter(val_loader)):\n",
    "            features = features.float().to(device)\n",
    "            labels = labels.float().to(device)\n",
    "            \n",
    "            probs = model(features)\n",
    "            \n",
    "            loss = criterion(probs, labels)\n",
    "\n",
    "            val_loss.append(loss.item())\n",
    "\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "            all_probs.append(probs.cpu().numpy())\n",
    "        \n",
    "        _val_loss = np.mean(val_loss)\n",
    "\n",
    "        all_labels = np.concatenate(all_labels, axis=0)\n",
    "        all_probs = np.concatenate(all_probs, axis=0)\n",
    "        \n",
    "        auc_score = multiLabel_AUC(all_labels, all_probs)\n",
    "    \n",
    "    return _val_loss, auc_score\n",
    "\n",
    "model = MLP()\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr = CONFIG.LR)\n",
    "\n",
    "infer_model = train(model, optimizer, train_loader, val_loader, device)\n",
    "\n",
    "# Unlabeled 데이터에 대해 pseudo-labeling\n",
    "unlabeled_path = './unlabeled_data'\n",
    "unlabeled_files = [os.path.join(unlabeled_path, f) for f in os.listdir(unlabeled_path) if f.endswith('.ogg')]\n",
    "\n",
    "def get_mfcc_feature_from_files(file_paths):\n",
    "    features = []\n",
    "    for file_path in tqdm(file_paths):\n",
    "        y, sr = librosa.load(file_path, sr=CONFIG.SR)\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=CONFIG.N_MFCC)\n",
    "        mfcc = np.mean(mfcc.T, axis=0)\n",
    "        features.append(mfcc)\n",
    "    return features\n",
    "\n",
    "unlabeled_features = get_mfcc_feature_from_files(unlabeled_files)\n",
    "unlabeled_features = np.array(unlabeled_features)\n",
    "unlabeled_dataset = CustomDataset(unlabeled_features, torch.zeros((len(unlabeled_features), CONFIG.N_CLASSES)))\n",
    "unlabeled_loader = DataLoader(unlabeled_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=False)\n",
    "\n",
    "def pseudo_labeling(model, loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    pseudo_labels = []\n",
    "    with torch.no_grad():\n",
    "        for features, _ in tqdm(iter(loader)):\n",
    "            features = features.float().to(device)\n",
    "            probs = model(features)\n",
    "            pseudo_labels.append(probs.cpu().detach().numpy())\n",
    "    return np.concatenate(pseudo_labels, axis=0)\n",
    "\n",
    "pseudo_labels = pseudo_labeling(infer_model, unlabeled_loader, device)\n",
    "pseudo_labels = (pseudo_labels > 0.5).astype(int)  # 임계값 0.5를 사용하여 pseudo-label 생성\n",
    "\n",
    "# Pseudo-labeled 데이터를 학습 데이터에 추가\n",
    "pseudo_labeled_dataset = CustomDataset(unlabeled_features, torch.tensor(pseudo_labels).float())\n",
    "train_dataset_combined = torch.utils.data.ConcatDataset([train_dataset, pseudo_labeled_dataset])\n",
    "train_loader_combined = DataLoader(train_dataset_combined, batch_size=CONFIG.BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# 모델을 pseudo-labeled 데이터를 포함하여 재학습\n",
    "infer_model = train(model, optimizer, train_loader_combined, val_loader, device)\n",
    "\n",
    "# 테스트 데이터 예측\n",
    "test = pd.read_csv('./test.csv')\n",
    "test_features, _ = get_mfcc_feature(test, False)\n",
    "test_features = np.array(test_features)\n",
    "test_dataset = CustomDataset(test_features, torch.zeros((len(test_features), CONFIG.N_CLASSES)))\n",
    "test_loader = DataLoader(test_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=False)\n",
    "\n",
    "def inference(model, test_loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for features, _ in tqdm(iter(test_loader)):\n",
    "            features = features.float().to(device)\n",
    "            probs = model(features)\n",
    "            probs  = probs.cpu().detach().numpy()\n",
    "            predictions += probs.tolist()\n",
    "    return predictions\n",
    "\n",
    "preds = inference(infer_model, test_loader, device)\n",
    "\n",
    "submit = pd.read_csv('./sample_submission.csv')\n",
    "submit.iloc[:, 1:] = preds\n",
    "submit.head()\n",
    "\n",
    "submit.to_csv('./pseudo_label_submit6.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "class Config:\n",
    "    SR = 32000\n",
    "    N_MFCC = 13\n",
    "    ROOT_FOLDER = './'\n",
    "    N_CLASSES = 2\n",
    "    BATCH_SIZE = 96\n",
    "    N_EPOCHS = 5\n",
    "    LR = 3e-4\n",
    "    SEED = 42\n",
    "    DROPOUT_RATE = 0.5\n",
    "    HIDDEN_DIMS = [128, 256, 128]\n",
    "    INITIALIZATION = 'xavier'\n",
    "    ACTIVATION_FUNCTION = 'relu'\n",
    "\n",
    "CONFIG = Config()\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CONFIG.SEED)\n",
    "\n",
    "# 데이터 로드 및 전처리\n",
    "df = pd.read_csv('./train.csv')\n",
    "\n",
    "def label_encoder(column):\n",
    "    le = LabelEncoder().fit(column)\n",
    "    print(column.name, le.classes_)\n",
    "    return le.transform(column)\n",
    "\n",
    "df['class'] = label_encoder(df['label'])\n",
    "\n",
    "# MFCC 특징 추출\n",
    "def get_mfcc_feature(df, train_mode=True):\n",
    "    features = []\n",
    "    labels = []\n",
    "    for _, row in tqdm(df.iterrows()):\n",
    "        y, sr = librosa.load(row['path'], sr=CONFIG.SR)\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=CONFIG.N_MFCC)\n",
    "        mfcc = np.mean(mfcc.T, axis=0)\n",
    "        features.append(mfcc)\n",
    "\n",
    "        if train_mode:\n",
    "            label = row['class']\n",
    "            labels.append(label)\n",
    "\n",
    "    return features, labels\n",
    "\n",
    "features, labels = get_mfcc_feature(df, True)\n",
    "feature_df = pd.DataFrame({'features': features, 'class': labels})\n",
    "\n",
    "X = np.array(feature_df['features'].tolist())\n",
    "y = np.array(feature_df['class'].tolist())\n",
    "\n",
    "# 데이터 불균형 해결\n",
    "ros = RandomOverSampler(random_state=CONFIG.SEED)\n",
    "X_resampled, y_resampled = ros.fit_resample(X, y)\n",
    "y_resampled = torch.tensor(y_resampled).long()  # 정수형으로 변환\n",
    "y_resampled = torch.nn.functional.one_hot(y_resampled, num_classes=CONFIG.N_CLASSES).float()\n",
    "\n",
    "# 데이터 분할\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=CONFIG.SEED)\n",
    "\n",
    "# PyTorch Dataset 클래스 정의\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, mfcc, label):\n",
    "        self.mfcc = mfcc\n",
    "        self.label = label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mfcc)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.mfcc[index], self.label[index]\n",
    "\n",
    "train_dataset = CustomDataset(X_train, y_train)\n",
    "val_dataset = CustomDataset(X_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# MLP 모델 정의\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim=CONFIG.N_MFCC, hidden_dims=[128, 256, 128], output_dim=CONFIG.N_CLASSES):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dims[0])\n",
    "        self.fc2 = nn.Linear(hidden_dims[0], hidden_dims[1])\n",
    "        self.fc3 = nn.Linear(hidden_dims[1], hidden_dims[2])\n",
    "        self.fc4 = nn.Linear(hidden_dims[2], output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "# 모델 학습 함수 정의\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def train(model, optimizer, train_loader, val_loader, device):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss().to(device)\n",
    "    \n",
    "    best_val_score = 0\n",
    "    best_model = None\n",
    "    \n",
    "    for epoch in range(1, CONFIG.N_EPOCHS+1):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        for features, labels in tqdm(iter(train_loader)):\n",
    "            features = features.float().to(device)\n",
    "            labels = labels.float().to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model(features)\n",
    "            loss = criterion(output, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss.append(loss.item())\n",
    "                    \n",
    "        _val_loss, _val_score = validation(model, criterion, val_loader, device)\n",
    "        _train_loss = np.mean(train_loss)\n",
    "        print(f'Epoch [{epoch}], Train Loss : [{_train_loss:.5f}] Val Loss : [{_val_loss:.5f}] Val AUC : [{_val_score:.5f}]')\n",
    "            \n",
    "        if best_val_score < _val_score:\n",
    "            best_val_score = _val_score\n",
    "            best_model = model\n",
    "    \n",
    "    return best_model\n",
    "\n",
    "def multiLabel_AUC(y_true, y_scores):\n",
    "    auc_scores = []\n",
    "    for i in range(y_true.shape[1]):\n",
    "        auc = roc_auc_score(y_true[:, i], y_scores[:, i])\n",
    "        auc_scores.append(auc)\n",
    "    mean_auc_score = np.mean(auc_scores)\n",
    "    return mean_auc_score\n",
    "    \n",
    "def validation(model, criterion, val_loader, device):\n",
    "    model.eval()\n",
    "    val_loss, all_labels, all_probs = [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for features, labels in tqdm(iter(val_loader)):\n",
    "            features = features.float().to(device)\n",
    "            labels = labels.float().to(device)\n",
    "            \n",
    "            probs = model(features)\n",
    "            \n",
    "            loss = criterion(probs, labels)\n",
    "\n",
    "            val_loss.append(loss.item())\n",
    "\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "            all_probs.append(probs.cpu().numpy())\n",
    "        \n",
    "        _val_loss = np.mean(val_loss)\n",
    "\n",
    "        all_labels = np.concatenate(all_labels, axis=0)\n",
    "        all_probs = np.concatenate(all_probs, axis=0)\n",
    "        \n",
    "        auc_score = multiLabel_AUC(all_labels, all_probs)\n",
    "    \n",
    "    return _val_loss, auc_score\n",
    "\n",
    "model = MLP()\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr = CONFIG.LR)\n",
    "\n",
    "infer_model = train(model, optimizer, train_loader, val_loader, device)\n",
    "\n",
    "# Unlabeled 데이터에 대해 pseudo-labeling\n",
    "unlabeled_path = './unlabeled_data'\n",
    "unlabeled_files = [os.path.join(unlabeled_path, f) for f in os.listdir(unlabeled_path) if f.endswith('.ogg')]\n",
    "\n",
    "def get_mfcc_feature_from_files(file_paths):\n",
    "    features = []\n",
    "    for file_path in tqdm(file_paths):\n",
    "        y, sr = librosa.load(file_path, sr=CONFIG.SR)\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=CONFIG.N_MFCC)\n",
    "        mfcc = np.mean(mfcc.T, axis=0)\n",
    "        features.append(mfcc)\n",
    "    return features\n",
    "\n",
    "unlabeled_features = get_mfcc_feature_from_files(unlabeled_files)\n",
    "unlabeled_features = np.array(unlabeled_features)\n",
    "unlabeled_dataset = CustomDataset(unlabeled_features, torch.zeros((len(unlabeled_features), CONFIG.N_CLASSES)))\n",
    "unlabeled_loader = DataLoader(unlabeled_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=False)\n",
    "\n",
    "def pseudo_labeling(model, loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    pseudo_labels = []\n",
    "    with torch.no_grad():\n",
    "        for features, _ in tqdm(iter(loader)):\n",
    "            features = features.float().to(device)\n",
    "            probs = model(features)\n",
    "            pseudo_labels.append(probs.cpu().detach().numpy())\n",
    "    return np.concatenate(pseudo_labels, axis=0)\n",
    "\n",
    "pseudo_labels = pseudo_labeling(infer_model, unlabeled_loader, device)\n",
    "pseudo_labels = (pseudo_labels > 0.7).astype(int)  # 임계값 0.5를 사용하여 pseudo-label 생성\n",
    "\n",
    "# Pseudo-labeled 데이터를 학습 데이터에 추가\n",
    "pseudo_labeled_dataset = CustomDataset(unlabeled_features, torch.tensor(pseudo_labels).float())\n",
    "train_dataset_combined = torch.utils.data.ConcatDataset([train_dataset, pseudo_labeled_dataset])\n",
    "train_loader_combined = DataLoader(train_dataset_combined, batch_size=CONFIG.BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# 모델을 pseudo-labeled 데이터를 포함하여 재학습\n",
    "infer_model = train(model, optimizer, train_loader_combined, val_loader, device)\n",
    "\n",
    "# 테스트 데이터 예측\n",
    "test = pd.read_csv('./test.csv')\n",
    "test_features, _ = get_mfcc_feature(test, False)\n",
    "test_features = np.array(test_features)\n",
    "test_dataset = CustomDataset(test_features, torch.zeros((len(test_features), CONFIG.N_CLASSES)))\n",
    "test_loader = DataLoader(test_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=False)\n",
    "\n",
    "def inference(model, test_loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for features, _ in tqdm(iter(test_loader)):\n",
    "            features = features.float().to(device)\n",
    "            probs = model(features)\n",
    "            probs  = probs.cpu().detach().numpy()\n",
    "            predictions += probs.tolist()\n",
    "    return predictions\n",
    "\n",
    "preds = inference(infer_model, test_loader, device)\n",
    "\n",
    "submit = pd.read_csv('./sample_submission.csv')\n",
    "submit.iloc[:, 1:] = preds\n",
    "submit.head()\n",
    "\n",
    "submit.to_csv('./pseudo_label_submit7.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "class Config:\n",
    "    SR = 32000\n",
    "    N_MFCC = 13\n",
    "    ROOT_FOLDER = './'\n",
    "    N_CLASSES = 2\n",
    "    BATCH_SIZE = 96\n",
    "    N_EPOCHS = 10\n",
    "    LR = 3e-4\n",
    "    SEED = 42\n",
    "\n",
    "CONFIG = Config()\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CONFIG.SEED)\n",
    "\n",
    "# 데이터 로드 및 전처리\n",
    "df = pd.read_csv('./train.csv')\n",
    "\n",
    "def label_encoder(column):\n",
    "    le = LabelEncoder().fit(column)\n",
    "    print(column.name, le.classes_)\n",
    "    return le.transform(column)\n",
    "\n",
    "df['class'] = label_encoder(df['label'])\n",
    "\n",
    "# MFCC 특징 추출\n",
    "def get_mfcc_feature(df, train_mode=True):\n",
    "    features = []\n",
    "    labels = []\n",
    "    for _, row in tqdm(df.iterrows()):\n",
    "        y, sr = librosa.load(row['path'], sr=CONFIG.SR)\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=CONFIG.N_MFCC)\n",
    "        mfcc = np.mean(mfcc.T, axis=0)\n",
    "        features.append(mfcc)\n",
    "\n",
    "        if train_mode:\n",
    "            label = row['class']\n",
    "            labels.append(label)\n",
    "\n",
    "    return features, labels\n",
    "\n",
    "features, labels = get_mfcc_feature(df, True)\n",
    "feature_df = pd.DataFrame({'features': features, 'class': labels})\n",
    "\n",
    "X = np.array(feature_df['features'].tolist())\n",
    "y = np.array(feature_df['class'].tolist())\n",
    "\n",
    "# 데이터 불균형 해결\n",
    "ros = RandomOverSampler(random_state=CONFIG.SEED)\n",
    "X_resampled, y_resampled = ros.fit_resample(X, y)\n",
    "y_resampled = torch.tensor(y_resampled).long()  # 정수형으로 변환\n",
    "y_resampled = torch.nn.functional.one_hot(y_resampled, num_classes=CONFIG.N_CLASSES).float()\n",
    "\n",
    "# 데이터 분할\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=CONFIG.SEED)\n",
    "\n",
    "# PyTorch Dataset 클래스 정의\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, mfcc, label):\n",
    "        self.mfcc = mfcc\n",
    "        self.label = label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mfcc)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.mfcc[index], self.label[index]\n",
    "\n",
    "train_dataset = CustomDataset(X_train, y_train)\n",
    "val_dataset = CustomDataset(X_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# MLP 모델 정의\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim=CONFIG.N_MFCC, hidden_dims=[128, 256, 128], output_dim=CONFIG.N_CLASSES):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dims[0])\n",
    "        self.fc2 = nn.Linear(hidden_dims[0], hidden_dims[1])\n",
    "        self.fc3 = nn.Linear(hidden_dims[1], hidden_dims[2])\n",
    "        self.fc4 = nn.Linear(hidden_dims[2], output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "# 모델 학습 함수 정의\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def train(model, optimizer, train_loader, val_loader, device):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss().to(device)\n",
    "    \n",
    "    best_val_score = 0\n",
    "    best_model = None\n",
    "    \n",
    "    for epoch in range(1, CONFIG.N_EPOCHS+1):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        for features, labels in tqdm(iter(train_loader)):\n",
    "            features = features.float().to(device)\n",
    "            labels = labels.float().to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model(features)\n",
    "            loss = criterion(output, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss.append(loss.item())\n",
    "                    \n",
    "        _val_loss, _val_score = validation(model, criterion, val_loader, device)\n",
    "        _train_loss = np.mean(train_loss)\n",
    "        print(f'Epoch [{epoch}], Train Loss : [{_train_loss:.5f}] Val Loss : [{_val_loss:.5f}] Val AUC : [{_val_score:.5f}]')\n",
    "            \n",
    "        if best_val_score < _val_score:\n",
    "            best_val_score = _val_score\n",
    "            best_model = model\n",
    "    \n",
    "    return best_model\n",
    "\n",
    "def multiLabel_AUC(y_true, y_scores):\n",
    "    auc_scores = []\n",
    "    for i in range(y_true.shape[1]):\n",
    "        auc = roc_auc_score(y_true[:, i], y_scores[:, i])\n",
    "        auc_scores.append(auc)\n",
    "    mean_auc_score = np.mean(auc_scores)\n",
    "    return mean_auc_score\n",
    "    \n",
    "def validation(model, criterion, val_loader, device):\n",
    "    model.eval()\n",
    "    val_loss, all_labels, all_probs = [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for features, labels in tqdm(iter(val_loader)):\n",
    "            features = features.float().to(device)\n",
    "            labels = labels.float().to(device)\n",
    "            \n",
    "            probs = model(features)\n",
    "            \n",
    "            loss = criterion(probs, labels)\n",
    "\n",
    "            val_loss.append(loss.item())\n",
    "\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "            all_probs.append(probs.cpu().numpy())\n",
    "        \n",
    "        _val_loss = np.mean(val_loss)\n",
    "\n",
    "        all_labels = np.concatenate(all_labels, axis=0)\n",
    "        all_probs = np.concatenate(all_probs, axis=0)\n",
    "        \n",
    "        auc_score = multiLabel_AUC(all_labels, all_probs)\n",
    "    \n",
    "    return _val_loss, auc_score\n",
    "\n",
    "model = MLP()\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr = CONFIG.LR)\n",
    "\n",
    "infer_model = train(model, optimizer, train_loader, val_loader, device)\n",
    "\n",
    "# Unlabeled 데이터에 대해 pseudo-labeling\n",
    "unlabeled_path = './unlabeled_data'\n",
    "unlabeled_files = [os.path.join(unlabeled_path, f) for f in os.listdir(unlabeled_path) if f.endswith('.ogg')]\n",
    "\n",
    "def get_mfcc_feature_from_files(file_paths):\n",
    "    features = []\n",
    "    for file_path in tqdm(file_paths):\n",
    "        y, sr = librosa.load(file_path, sr=CONFIG.SR)\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=CONFIG.N_MFCC)\n",
    "        mfcc = np.mean(mfcc.T, axis=0)\n",
    "        features.append(mfcc)\n",
    "    return features\n",
    "\n",
    "unlabeled_features = get_mfcc_feature_from_files(unlabeled_files)\n",
    "unlabeled_features = np.array(unlabeled_features)\n",
    "unlabeled_dataset = CustomDataset(unlabeled_features, torch.zeros((len(unlabeled_features), CONFIG.N_CLASSES)))\n",
    "unlabeled_loader = DataLoader(unlabeled_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=False)\n",
    "\n",
    "def pseudo_labeling(model, loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    pseudo_labels = []\n",
    "    with torch.no_grad():\n",
    "        for features, _ in tqdm(iter(loader)):\n",
    "            features = features.float().to(device)\n",
    "            probs = model(features)\n",
    "            pseudo_labels.append(probs.cpu().detach().numpy())\n",
    "    return np.concatenate(pseudo_labels, axis=0)\n",
    "\n",
    "pseudo_labels = pseudo_labeling(infer_model, unlabeled_loader, device)\n",
    "pseudo_labels = (pseudo_labels > 0.8).astype(int)  # 임계값 0.7를 사용하여 pseudo-label 생성\n",
    "\n",
    "# Pseudo-labeled 데이터를 학습 데이터에 추가\n",
    "pseudo_labeled_dataset = CustomDataset(unlabeled_features, torch.tensor(pseudo_labels).float())\n",
    "train_dataset_combined = torch.utils.data.ConcatDataset([train_dataset, pseudo_labeled_dataset])\n",
    "train_loader_combined = DataLoader(train_dataset_combined, batch_size=CONFIG.BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# 모델을 pseudo-labeled 데이터를 포함하여 재학습\n",
    "infer_model = train(model, optimizer, train_loader_combined, val_loader, device)\n",
    "\n",
    "# 테스트 데이터 예측\n",
    "test = pd.read_csv('./test.csv')\n",
    "test_features, _ = get_mfcc_feature(test, False)\n",
    "test_features = np.array(test_features)\n",
    "test_dataset = CustomDataset(test_features, torch.zeros((len(test_features), CONFIG.N_CLASSES)))\n",
    "test_loader = DataLoader(test_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=False)\n",
    "\n",
    "def inference(model, test_loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for features, _ in tqdm(iter(test_loader)):\n",
    "            features = features.float().to(device)\n",
    "            probs = model(features)\n",
    "            probs  = probs.cpu().detach().numpy()\n",
    "            predictions += probs.tolist()\n",
    "    return predictions\n",
    "\n",
    "preds = inference(infer_model, test_loader, device)\n",
    "\n",
    "submit = pd.read_csv('./sample_submission.csv')\n",
    "submit.iloc[:, 1:] = preds\n",
    "submit.head()\n",
    "\n",
    "submit.to_csv('./pseudo_label_submit8.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import librosa\n",
    "import librosa.effects as le\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "class Config:\n",
    "    SR = 32000\n",
    "    N_MFCC = 13\n",
    "    ROOT_FOLDER = './'\n",
    "    N_CLASSES = 2\n",
    "    BATCH_SIZE = 96\n",
    "    N_EPOCHS = 10\n",
    "    LR = 3e-4\n",
    "    SEED = 42\n",
    "\n",
    "CONFIG = Config()\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CONFIG.SEED)\n",
    "\n",
    "# 데이터 로드 및 전처리\n",
    "df = pd.read_csv('./train.csv')\n",
    "\n",
    "def label_encoder(column):\n",
    "    le = LabelEncoder().fit(column)\n",
    "    print(column.name, le.classes_)\n",
    "    return le.transform(column)\n",
    "\n",
    "df['class'] = label_encoder(df['label'])\n",
    "\n",
    "# MFCC 특징 추출\n",
    "def get_mfcc_feature(df, train_mode=True):\n",
    "    features = []\n",
    "    labels = []\n",
    "    for _, row in tqdm(df.iterrows()):\n",
    "        y, sr = librosa.load(row['path'], sr=CONFIG.SR)\n",
    "        \n",
    "        # 데이터 증강\n",
    "        if train_mode:\n",
    "            y = le.time_stretch(y, rate=np.random.uniform(0.8, 1.2))\n",
    "            y = le.pitch_shift(y, sr=sr, n_steps=np.random.uniform(-2, 2))\n",
    "        \n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=CONFIG.N_MFCC)\n",
    "        zcr = librosa.feature.zero_crossing_rate(y)\n",
    "        spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
    "        chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "        \n",
    "        features.append(np.concatenate([\n",
    "            np.mean(mfcc.T, axis=0),\n",
    "            [np.mean(zcr)],  # 1차원 배열로 변환\n",
    "            [np.mean(spectral_centroid)],  # 1차원 배열로 변환\n",
    "            np.mean(chroma, axis=1)\n",
    "        ]))\n",
    "\n",
    "        if train_mode:\n",
    "            label = row['class']\n",
    "            labels.append(label)\n",
    "\n",
    "    return features, labels\n",
    "\n",
    "features, labels = get_mfcc_feature(df, True)\n",
    "feature_df = pd.DataFrame({'features': features, 'class': labels})\n",
    "\n",
    "X = np.array(feature_df['features'].tolist())\n",
    "y = np.array(feature_df['class'].tolist())\n",
    "\n",
    "# 데이터 불균형 해결\n",
    "ros = RandomOverSampler(random_state=CONFIG.SEED)\n",
    "X_resampled, y_resampled = ros.fit_resample(X, y)\n",
    "y_resampled = torch.tensor(y_resampled).long()  # 정수형으로 변환\n",
    "y_resampled = torch.nn.functional.one_hot(y_resampled, num_classes=CONFIG.N_CLASSES).float()\n",
    "\n",
    "# 데이터 분할\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=CONFIG.SEED)\n",
    "\n",
    "# PyTorch Dataset 클래스 정의\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, mfcc, label):\n",
    "        self.mfcc = mfcc\n",
    "        self.label = label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mfcc)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.mfcc[index], self.label[index]\n",
    "\n",
    "train_dataset = CustomDataset(X_train, y_train)\n",
    "val_dataset = CustomDataset(X_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# MLP 모델 정의\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim=CONFIG.N_MFCC + 14, hidden_dims=[128, 256, 256, 128], output_dim=CONFIG.N_CLASSES):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dims[0])\n",
    "        self.fc2 = nn.Linear(hidden_dims[0], hidden_dims[1])\n",
    "        self.fc3 = nn.Linear(hidden_dims[1], hidden_dims[2])\n",
    "        self.fc4 = nn.Linear(hidden_dims[2], hidden_dims[3])\n",
    "        self.fc5 = nn.Linear(hidden_dims[3], output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(hidden_dims[0])\n",
    "        self.batchnorm2 = nn.BatchNorm1d(hidden_dims[1])\n",
    "        self.batchnorm3 = nn.BatchNorm1d(hidden_dims[2])\n",
    "        self.batchnorm4 = nn.BatchNorm1d(hidden_dims[3])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.batchnorm1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.batchnorm2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.batchnorm3(self.fc3(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.batchnorm4(self.fc4(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc5(x)\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "# 모델 학습 함수 정의\n",
    "def train(model, optimizer, scheduler, train_loader, val_loader, device):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss().to(device)\n",
    "    \n",
    "    best_val_score = 0\n",
    "    best_model = None\n",
    "    \n",
    "    for epoch in range(1, CONFIG.N_EPOCHS+1):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        for features, labels in tqdm(iter(train_loader)):\n",
    "            features = features.float().to(device)\n",
    "            labels = labels.float().to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model(features)\n",
    "            loss = criterion(output, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss.append(loss.item())\n",
    "                    \n",
    "        _val_loss, _val_score = validation(model, criterion, val_loader, device)\n",
    "        _train_loss = np.mean(train_loss)\n",
    "        print(f'Epoch [{epoch}], Train Loss : [{_train_loss:.5f}] Val Loss : [{_val_loss:.5f}] Val AUC : [{_val_score:.5f}]')\n",
    "        \n",
    "        scheduler.step(_val_score)\n",
    "            \n",
    "        if best_val_score < _val_score:\n",
    "            best_val_score = _val_score\n",
    "            best_model = model\n",
    "    \n",
    "    return best_model\n",
    "\n",
    "def multiLabel_AUC(y_true, y_scores):\n",
    "    auc_scores = []\n",
    "    for i in range(y_true.shape[1]):\n",
    "        auc = roc_auc_score(y_true[:, i], y_scores[:, i])\n",
    "        auc_scores.append(auc)\n",
    "    mean_auc_score = np.mean(auc_scores)\n",
    "    return mean_auc_score\n",
    "    \n",
    "def validation(model, criterion, val_loader, device):\n",
    "    model.eval()\n",
    "    val_loss, all_labels, all_probs = [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for features, labels in tqdm(iter(val_loader)):\n",
    "            features = features.float().to(device)\n",
    "            labels = labels.float().to(device)\n",
    "            \n",
    "            probs = model(features)\n",
    "            \n",
    "            loss = criterion(probs, labels)\n",
    "\n",
    "            val_loss.append(loss.item())\n",
    "\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "            all_probs.append(probs.cpu().numpy())\n",
    "        \n",
    "        _val_loss = np.mean(val_loss)\n",
    "\n",
    "        all_labels = np.concatenate(all_labels, axis=0)\n",
    "        all_probs = np.concatenate(all_probs, axis=0)\n",
    "        \n",
    "        auc_score = multiLabel_AUC(all_labels, all_probs)\n",
    "    \n",
    "    return _val_loss, auc_score\n",
    "\n",
    "def train_ensemble(n_models=3):\n",
    "    models = []\n",
    "    for _ in range(n_models):\n",
    "        model = MLP()\n",
    "        optimizer = torch.optim.Adam(params=model.parameters(), lr=CONFIG.LR)\n",
    "        scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2, verbose=True)\n",
    "        trained_model = train(model, optimizer, scheduler, train_loader, val_loader, device)\n",
    "        models.append(trained_model)\n",
    "    return models\n",
    "\n",
    "# Unlabeled 데이터에 대해 pseudo-labeling\n",
    "unlabeled_path = './unlabeled_data'\n",
    "unlabeled_files = [os.path.join(unlabeled_path, f) for f in os.listdir(unlabeled_path) if f.endswith('.ogg')]\n",
    "\n",
    "def get_mfcc_feature_from_files(file_paths):\n",
    "    features = []\n",
    "    for file_path in tqdm(file_paths):\n",
    "        y, sr = librosa.load(file_path, sr=CONFIG.SR)\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=CONFIG.N_MFCC)\n",
    "        zcr = librosa.feature.zero_crossing_rate(y)\n",
    "        spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
    "        chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "        \n",
    "        features.append(np.concatenate([\n",
    "            np.mean(mfcc.T, axis=0),\n",
    "            [np.mean(zcr)],  # 1차원 배열로 변환\n",
    "            [np.mean(spectral_centroid)],  # 1차원 배열로 변환\n",
    "            np.mean(chroma, axis=1)\n",
    "        ]))\n",
    "    return features\n",
    "\n",
    "unlabeled_features = get_mfcc_feature_from_files(unlabeled_files)\n",
    "unlabeled_features = np.array(unlabeled_features)\n",
    "unlabeled_dataset = CustomDataset(unlabeled_features, torch.zeros((len(unlabeled_features), CONFIG.N_CLASSES)))\n",
    "unlabeled_loader = DataLoader(unlabeled_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=False)\n",
    "\n",
    "def pseudo_labeling(models, loader, device):\n",
    "    pseudo_labels = []\n",
    "    for model in models:\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        model_pseudo_labels = []\n",
    "        with torch.no_grad():\n",
    "            for features, _ in tqdm(iter(loader)):\n",
    "                features = features.float().to(device)\n",
    "                probs = model(features)\n",
    "                model_pseudo_labels.append(probs.cpu().detach().numpy())\n",
    "        pseudo_labels.append(np.concatenate(model_pseudo_labels, axis=0))\n",
    "    return np.mean(pseudo_labels, axis=0)\n",
    "\n",
    "ensemble_models = train_ensemble()\n",
    "pseudo_labels = pseudo_labeling(ensemble_models, unlabeled_loader, device)\n",
    "pseudo_labels = (pseudo_labels > 0.9).astype(int)  # 임계값 0.8을 사용하여 pseudo-label 생성\n",
    "\n",
    "# Pseudo-labeled 데이터를 학습 데이터에 추가\n",
    "pseudo_labeled_dataset = CustomDataset(unlabeled_features, torch.tensor(pseudo_labels).float())\n",
    "train_dataset_combined = torch.utils.data.ConcatDataset([train_dataset, pseudo_labeled_dataset])\n",
    "train_loader_combined = DataLoader(train_dataset_combined, batch_size=CONFIG.BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# 모델을 pseudo-labeled 데이터를 포함하여 재학습\n",
    "ensemble_models = train_ensemble()\n",
    "\n",
    "# 테스트 데이터 예측\n",
    "test = pd.read_csv('./test.csv')\n",
    "test_features, _ = get_mfcc_feature(test, False)\n",
    "test_features = np.array(test_features)\n",
    "test_dataset = CustomDataset(test_features, torch.zeros((len(test_features), CONFIG.N_CLASSES)))\n",
    "test_loader = DataLoader(test_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=False)\n",
    "\n",
    "def inference(models, test_loader, device):\n",
    "    predictions = []\n",
    "    for model in models:\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        model_preds = []\n",
    "        with torch.no_grad():\n",
    "            for features, _ in tqdm(iter(test_loader)):\n",
    "                features = features.float().to(device)\n",
    "                probs = model(features)\n",
    "                model_preds += probs.cpu().detach().numpy().tolist()\n",
    "        predictions.append(model_preds)\n",
    "    return np.mean(predictions, axis=0)\n",
    "\n",
    "preds = inference(ensemble_models, test_loader, device)\n",
    "\n",
    "submit = pd.read_csv('./sample_submission.csv')\n",
    "submit.iloc[:, 1:] = preds\n",
    "submit.head()\n",
    "\n",
    "submit.to_csv('./pseudo_label_submit9.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "class Config:\n",
    "    SR = 32000\n",
    "    N_MFCC = 13\n",
    "    N_FEATURES = 64\n",
    "    ROOT_FOLDER = './'\n",
    "    N_CLASSES = 2\n",
    "    BATCH_SIZE = 96\n",
    "    N_EPOCHS = 5\n",
    "    LR = 3e-4\n",
    "    SEED = 42\n",
    "    DROPOUT_RATE = 0.5\n",
    "    HIDDEN_DIMS = [128, 256, 128]\n",
    "    INITIALIZATION = 'xavier'\n",
    "    ACTIVATION_FUNCTION = 'relu'\n",
    "\n",
    "CONFIG = Config()\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CONFIG.SEED)\n",
    "\n",
    "# 데이터 로드 및 전처리\n",
    "df = pd.read_csv(os.path.join(CONFIG.ROOT_FOLDER, 'train.csv'))\n",
    "\n",
    "def label_encoder(column):\n",
    "    le = LabelEncoder().fit(column)\n",
    "    print(column.name, le.classes_)\n",
    "    return le.transform(column)\n",
    "\n",
    "df['class'] = label_encoder(df['label'])\n",
    "\n",
    "# 음성 특징 추출 함수\n",
    "def extract_features(file_path):\n",
    "    y, sr = librosa.load(file_path, sr=CONFIG.SR)\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=CONFIG.N_MFCC)\n",
    "    chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "    mel = librosa.feature.melspectrogram(y=y, sr=sr)\n",
    "    contrast = librosa.feature.spectral_contrast(y=y, sr=sr)\n",
    "    tonnetz = librosa.feature.tonnetz(y=librosa.effects.harmonic(y), sr=sr)\n",
    "    \n",
    "    features = np.concatenate((mfcc, chroma, mel, contrast, tonnetz), axis=0)\n",
    "    return features\n",
    "\n",
    "def get_features_and_labels(df, train_mode=True):\n",
    "    features = []\n",
    "    labels = []\n",
    "    for _, row in tqdm(df.iterrows()):\n",
    "        file_path = os.path.join(CONFIG.ROOT_FOLDER, row['path'][2:])  # './train/' 또는 './test/' 제거\n",
    "        if os.path.isfile(file_path):\n",
    "            features.append(extract_features(file_path))\n",
    "            if train_mode:\n",
    "                labels.append(row['class'])\n",
    "    if train_mode:\n",
    "        return np.array(features), np.array(labels)\n",
    "    return np.array(features)\n",
    "\n",
    "# Train 데이터에서 특징 추출\n",
    "X, y = get_features_and_labels(df, True)\n",
    "\n",
    "# 데이터 불균형 해결\n",
    "smote = SMOTE(random_state=CONFIG.SEED)\n",
    "X_resampled, y_resampled = smote.fit_resample(X.reshape(len(X), -1), y)\n",
    "X_resampled = X_resampled.reshape(len(X_resampled), CONFIG.N_FEATURES, -1)\n",
    "y_resampled = torch.tensor(y_resampled).long()  # 정수형으로 변환\n",
    "y_resampled = torch.nn.functional.one_hot(y_resampled, num_classes=CONFIG.N_CLASSES).float()\n",
    "\n",
    "# 데이터 분할\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=CONFIG.SEED)\n",
    "\n",
    "# PyTorch Dataset 클래스 정의\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.features[index], self.labels[index]\n",
    "\n",
    "train_dataset = CustomDataset(X_train, y_train)\n",
    "val_dataset = CustomDataset(X_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# CNN+RNN 모델 정의\n",
    "class CNNRNN(nn.Module):\n",
    "    def __init__(self, input_dim=CONFIG.N_FEATURES, hidden_dims=CONFIG.HIDDEN_DIMS, output_dim=CONFIG.N_CLASSES):\n",
    "        super(CNNRNN, self).__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(CONFIG.DROPOUT_RATE),\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(CONFIG.DROPOUT_RATE)\n",
    "        )\n",
    "        \n",
    "        self.rnn = nn.LSTM(input_size=64*8, hidden_size=hidden_dims[0], num_layers=2, batch_first=True, dropout=CONFIG.DROPOUT_RATE, bidirectional=True)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dims[0]*2, hidden_dims[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(CONFIG.DROPOUT_RATE),\n",
    "            nn.Linear(hidden_dims[1], hidden_dims[2]),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(CONFIG.DROPOUT_RATE),\n",
    "            nn.Linear(hidden_dims[2], output_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = x.unsqueeze(1)  # (batch_size, 1, n_features, time_steps)\n",
    "        x = self.cnn(x)\n",
    "        x = x.view(batch_size, 64*8, -1).permute(0, 2, 1)  # (batch_size, time_steps, 64*8)\n",
    "        x, _ = self.rnn(x)\n",
    "        x = x[:, -1, :]  # 마지막 타임 스텝의 출력\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# 모델 학습 함수 정의\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def train(model, optimizer, train_loader, val_loader, device):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss().to(device)\n",
    "    \n",
    "    best_val_score = 0\n",
    "    best_model = None\n",
    "    \n",
    "    for epoch in range(1, CONFIG.N_EPOCHS+1):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        for features, labels in tqdm(iter(train_loader)):\n",
    "            features = features.float().to(device)\n",
    "            labels = labels.float().to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model(features)\n",
    "            loss = criterion(output, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss.append(loss.item())\n",
    "                    \n",
    "        _val_loss, _val_score = validation(model, criterion, val_loader, device)\n",
    "        _train_loss = np.mean(train_loss)\n",
    "        print(f'Epoch [{epoch}], Train Loss : [{_train_loss:.5f}] Val Loss : [{_val_loss:.5f}] Val AUC : [{_val_score:.5f}]')\n",
    "            \n",
    "        if best_val_score < _val_score:\n",
    "            best_val_score = _val_score\n",
    "            best_model = model\n",
    "    \n",
    "    return best_model\n",
    "\n",
    "def multiLabel_AUC(y_true, y_scores):\n",
    "    auc_scores = []\n",
    "    for i in range(y_true.shape[1]):\n",
    "        auc = roc_auc_score(y_true[:, i], y_scores[:, i])\n",
    "        auc_scores.append(auc)\n",
    "    mean_auc_score = np.mean(auc_scores)\n",
    "    return mean_auc_score\n",
    "    \n",
    "def validation(model, criterion, val_loader, device):\n",
    "    model.eval()\n",
    "    val_loss, all_labels, all_probs = [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for features, labels in tqdm(iter(val_loader)):\n",
    "            features = features.float().to(device)\n",
    "            labels = labels.float().to(device)\n",
    "            \n",
    "            probs = model(features)\n",
    "            \n",
    "            loss = criterion(probs, labels)\n",
    "\n",
    "            val_loss.append(loss.item())\n",
    "\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "            all_probs.append(probs.cpu().numpy())\n",
    "        \n",
    "        _val_loss = np.mean(val_loss)\n",
    "\n",
    "        all_labels = np.concatenate(all_labels, axis=0)\n",
    "        all_probs = np.concatenate(all_probs, axis=0)\n",
    "        \n",
    "        auc_score = multiLabel_AUC(all_labels, all_probs)\n",
    "    \n",
    "    return _val_loss, auc_score\n",
    "\n",
    "model = CNNRNN()\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr = CONFIG.LR)\n",
    "\n",
    "infer_model = train(model, optimizer, train_loader, val_loader, device)\n",
    "\n",
    "# Unlabeled 데이터에 대해 pseudo-labeling\n",
    "unlabeled_path = os.path.join(CONFIG.ROOT_FOLDER, 'unlabeled_data')\n",
    "unlabeled_files = [os.path.join(unlabeled_path, f) for f in os.listdir(unlabeled_path) if f.endswith('.ogg')]\n",
    "\n",
    "unlabeled_features = get_mfcc_feature_from_files(unlabeled_files)\n",
    "unlabeled_features = np.array(unlabeled_features)\n",
    "unlabeled_dataset = CustomDataset(unlabeled_features, torch.zeros((len(unlabeled_features), CONFIG.N_CLASSES)))\n",
    "unlabeled_loader = DataLoader(unlabeled_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=False)\n",
    "\n",
    "def pseudo_labeling(model, loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    pseudo_labels = []\n",
    "    with torch.no_grad():\n",
    "        for features, _ in tqdm(iter(loader)):\n",
    "            features = features.float().to(device)\n",
    "            probs = model(features)\n",
    "            pseudo_labels.append(probs.cpu().detach().numpy())\n",
    "    return np.concatenate(pseudo_labels, axis=0)\n",
    "\n",
    "pseudo_labels = pseudo_labeling(infer_model, unlabeled_loader, device)\n",
    "pseudo_labels = (pseudo_labels > 0.9).astype(int)  # 임계값 0.7를 사용하여 pseudo-label 생성\n",
    "\n",
    "# Pseudo-labeled 데이터를 학습 데이터에 추가\n",
    "pseudo_labeled_dataset = CustomDataset(unlabeled_features, torch.tensor(pseudo_labels).float())\n",
    "train_dataset_combined = torch.utils.data.ConcatDataset([train_dataset, pseudo_labeled_dataset])\n",
    "train_loader_combined = DataLoader(train_dataset_combined, batch_size=CONFIG.BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# 모델을 pseudo-labeled 데이터를 포함하여 재학습\n",
    "infer_model = train(model, optimizer, train_loader_combined, val_loader, device)\n",
    "\n",
    "# 테스트 데이터 예측\n",
    "test_df = pd.read_csv(os.path.join(CONFIG.ROOT_FOLDER, 'test.csv'))\n",
    "test_features = get_features_and_labels(test_df, False)\n",
    "test_dataset = CustomDataset(test_features, torch.zeros((len(test_features), CONFIG.N_CLASSES)))\n",
    "test_loader = DataLoader(test_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=False)\n",
    "\n",
    "def inference(model, test_loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for features, _ in tqdm(iter(test_loader)):\n",
    "            features = features.float().to(device)\n",
    "            probs = model(features)\n",
    "            probs  = probs.cpu().detach().numpy()\n",
    "            predictions += probs.tolist()\n",
    "    return predictions\n",
    "\n",
    "preds = inference(infer_model, test_loader, device)\n",
    "\n",
    "submit = pd.read_csv(os.path.join(CONFIG.ROOT_FOLDER, 'sample_submission.csv'))\n",
    "submit.iloc[:, 1:] = preds\n",
    "submit.head()\n",
    "\n",
    "submit.to_csv('./pseudo_label_submit10.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "class Config:\n",
    "    SR = 32000\n",
    "    N_MFCC = 13\n",
    "    ROOT_FOLDER = './'\n",
    "    N_CLASSES = 2\n",
    "    BATCH_SIZE = 96\n",
    "    N_EPOCHS = 10\n",
    "    LR = 3e-4\n",
    "    SEED = 42\n",
    "    DROPOUT_RATE = 0.5\n",
    "    HIDDEN_DIMS = [128, 256, 128]\n",
    "    INITIALIZATION = 'xavier'\n",
    "    ACTIVATION_FUNCTION = 'relu'\n",
    "\n",
    "CONFIG = Config()\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CONFIG.SEED)\n",
    "\n",
    "# 데이터 로드 및 전처리\n",
    "df = pd.read_csv(os.path.join(CONFIG.ROOT_FOLDER, 'train.csv'))\n",
    "\n",
    "def label_encoder(column):\n",
    "    le = LabelEncoder().fit(column)\n",
    "    print(column.name, le.classes_)\n",
    "    return le.transform(column)\n",
    "\n",
    "df['class'] = label_encoder(df['label'])\n",
    "\n",
    "# 음성 특징 추출 함수\n",
    "def extract_features(file_path):\n",
    "    y, sr = librosa.load(file_path, sr=CONFIG.SR)\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=CONFIG.N_MFCC)\n",
    "    chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "    mel = librosa.feature.melspectrogram(y=y, sr=sr)\n",
    "    contrast = librosa.feature.spectral_contrast(y=y, sr=sr)\n",
    "    tonnetz = librosa.feature.tonnetz(y=librosa.effects.harmonic(y), sr=sr)\n",
    "    \n",
    "    features = np.concatenate((mfcc, chroma, mel, contrast, tonnetz), axis=0)\n",
    "    return features\n",
    "\n",
    "def get_features_and_labels(df, train_mode=True):\n",
    "    features = []\n",
    "    labels = []\n",
    "    for _, row in tqdm(df.iterrows()):\n",
    "        file_path = os.path.join(CONFIG.ROOT_FOLDER, row['path'][2:])  # './train/' 또는 './test/' 제거\n",
    "        if os.path.isfile(file_path):\n",
    "            features.append(extract_features(file_path))\n",
    "            if train_mode:\n",
    "                labels.append(row['class'])\n",
    "    if train_mode:\n",
    "        return np.array(features), np.array(labels)\n",
    "    return np.array(features)\n",
    "\n",
    "# Train 데이터에서 특징 추출\n",
    "X, y = get_features_and_labels(df, True)\n",
    "\n",
    "# 데이터 불균형 해결\n",
    "smote = SMOTE(random_state=CONFIG.SEED)\n",
    "X_resampled, y_resampled = smote.fit_resample(X.reshape(len(X), -1), y)\n",
    "X_resampled = X_resampled.reshape(len(X_resampled), -1, X_resampled.shape[1] // -1)  # Reshape to original feature shape\n",
    "y_resampled = torch.tensor(y_resampled).long()  # 정수형으로 변환\n",
    "y_resampled = torch.nn.functional.one_hot(y_resampled, num_classes=CONFIG.N_CLASSES).float()\n",
    "\n",
    "# 데이터 분할\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=CONFIG.SEED)\n",
    "\n",
    "# PyTorch Dataset 클래스 정의\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.features[index], self.labels[index]\n",
    "\n",
    "train_dataset = CustomDataset(X_train, y_train)\n",
    "val_dataset = CustomDataset(X_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# CNN+RNN 모델 정의\n",
    "class CNNRNN(nn.Module):\n",
    "    def __init__(self, input_dim=CONFIG.N_MFCC, hidden_dims=CONFIG.HIDDEN_DIMS, output_dim=CONFIG.N_CLASSES):\n",
    "        super(CNNRNN, self).__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(CONFIG.DROPOUT_RATE),\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(CONFIG.DROPOUT_RATE)\n",
    "        )\n",
    "        \n",
    "        self.rnn = nn.LSTM(input_size=64*8, hidden_size=hidden_dims[0], num_layers=2, batch_first=True, dropout=CONFIG.DROPOUT_RATE, bidirectional=True)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dims[0]*2, hidden_dims[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(CONFIG.DROPOUT_RATE),\n",
    "            nn.Linear(hidden_dims[1], hidden_dims[2]),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(CONFIG.DROPOUT_RATE),\n",
    "            nn.Linear(hidden_dims[2], output_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = x.unsqueeze(1)  # (batch_size, 1, n_features, time_steps)\n",
    "        x = self.cnn(x)\n",
    "        x = x.view(batch_size, 64*8, -1).permute(0, 2, 1)  # (batch_size, time_steps, 64*8)\n",
    "        x, _ = self.rnn(x)\n",
    "        x = x[:, -1, :]  # 마지막 타임 스텝의 출력\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# 모델 학습 함수 정의\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def train(model, optimizer, train_loader, val_loader, device):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss().to(device)\n",
    "    \n",
    "    best_val_score = 0\n",
    "    best_model = None\n",
    "    \n",
    "    for epoch in range(1, CONFIG.N_EPOCHS+1):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        for features, labels in tqdm(iter(train_loader)):\n",
    "            features = features.float().to(device)\n",
    "            labels = labels.float().to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model(features)\n",
    "            loss = criterion(output, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss.append(loss.item())\n",
    "                    \n",
    "        _val_loss, _val_score = validation(model, criterion, val_loader, device)\n",
    "        _train_loss = np.mean(train_loss)\n",
    "        print(f'Epoch [{epoch}], Train Loss : [{_train_loss:.5f}] Val Loss : [{_val_loss:.5f}] Val AUC : [{_val_score:.5f}]')\n",
    "            \n",
    "        if best_val_score < _val_score:\n",
    "            best_val_score = _val_score\n",
    "            best_model = model\n",
    "    \n",
    "    return best_model\n",
    "\n",
    "def multiLabel_AUC(y_true, y_scores):\n",
    "    auc_scores = []\n",
    "    for i in range(y_true.shape[1]):\n",
    "        auc = roc_auc_score(y_true[:, i], y_scores[:, i])\n",
    "        auc_scores.append(auc)\n",
    "    mean_auc_score = np.mean(auc_scores)\n",
    "    return mean_auc_score\n",
    "    \n",
    "def validation(model, criterion, val_loader, device):\n",
    "    model.eval()\n",
    "    val_loss, all_labels, all_probs = [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for features, labels in tqdm(iter(val_loader)):\n",
    "            features = features.float().to(device)\n",
    "            labels = labels.float().to(device)\n",
    "            \n",
    "            probs = model(features)\n",
    "            \n",
    "            loss = criterion(probs, labels)\n",
    "\n",
    "            val_loss.append(loss.item())\n",
    "\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "            all_probs.append(probs.cpu().numpy())\n",
    "        \n",
    "        _val_loss = np.mean(val_loss)\n",
    "\n",
    "        all_labels = np.concatenate(all_labels, axis=0)\n",
    "        all_probs = np.concatenate(all_probs, axis=0)\n",
    "        \n",
    "        auc_score = multiLabel_AUC(all_labels, all_probs)\n",
    "    \n",
    "    return _val_loss, auc_score\n",
    "\n",
    "model = CNNRNN()\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr = CONFIG.LR)\n",
    "\n",
    "infer_model = train(model, optimizer, train_loader, val_loader, device)\n",
    "\n",
    "# Unlabeled 데이터에 대해 pseudo-labeling\n",
    "unlabeled_path = os.path.join(CONFIG.ROOT_FOLDER, 'unlabeled_data')\n",
    "unlabeled_files = [os.path.join(unlabeled_path, f) for f in os.listdir(unlabeled_path) if f.endswith('.ogg')]\n",
    "\n",
    "def get_features_from_files(file_paths):\n",
    "    features = []\n",
    "    for file_path in tqdm(file_paths):\n",
    "        features.append(extract_features(file_path))\n",
    "    return np.array(features)\n",
    "\n",
    "unlabeled_features = get_features_from_files(unlabeled_files)\n",
    "unlabeled_dataset = CustomDataset(unlabeled_features, torch.zeros((len(unlabeled_features), CONFIG.N_CLASSES)))\n",
    "unlabeled_loader = DataLoader(unlabeled_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=False)\n",
    "\n",
    "def pseudo_labeling(model, loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    pseudo_labels = []\n",
    "    with torch.no_grad():\n",
    "        for features, _ in tqdm(iter(loader)):\n",
    "            features = features.float().to(device)\n",
    "            probs = model(features)\n",
    "            pseudo_labels.append(probs.cpu().detach().numpy())\n",
    "    return np.concatenate(pseudo_labels, axis=0)\n",
    "\n",
    "pseudo_labels = pseudo_labeling(infer_model, unlabeled_loader, device)\n",
    "pseudo_labels = (pseudo_labels > 0.8).astype(int)  # 임계값 0.7를 사용하여 pseudo-label 생성\n",
    "\n",
    "# Pseudo-labeled 데이터를 학습 데이터에 추가\n",
    "pseudo_labeled_dataset = CustomDataset(unlabeled_features, torch.tensor(pseudo_labels).float())\n",
    "train_dataset_combined = torch.utils.data.ConcatDataset([train_dataset, pseudo_labeled_dataset])\n",
    "train_loader_combined = DataLoader(train_dataset_combined, batch_size=CONFIG.BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# 모델을 pseudo-labeled 데이터를 포함하여 재학습\n",
    "infer_model = train(model, optimizer, train_loader_combined, val_loader, device)\n",
    "\n",
    "# 테스트 데이터 예측\n",
    "test_df = pd.read_csv(os.path.join(CONFIG.ROOT_FOLDER, 'test.csv'))\n",
    "test_features = get_features_and_labels(test_df, False)\n",
    "test_dataset = CustomDataset(test_features, torch.zeros((len(test_features), CONFIG.N_CLASSES)))\n",
    "test_loader = DataLoader(test_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=False)\n",
    "\n",
    "def inference(model, test_loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for features, _ in tqdm(iter(test_loader)):\n",
    "            features = features.float().to(device)\n",
    "            probs = model(features)\n",
    "            probs  = probs.cpu().detach().numpy()\n",
    "            predictions += probs.tolist()\n",
    "    return predictions\n",
    "\n",
    "preds = inference(infer_model, test_loader, device)\n",
    "\n",
    "submit = pd.read_csv(os.path.join(CONFIG.ROOT_FOLDER, 'sample_submission.csv'))\n",
    "submit.iloc[:, 1:] = preds\n",
    "submit.head()\n",
    "\n",
    "submit.to_csv('./pseudo_label_submit11.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label ['fake' 'real']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "45787it [12:01:08,  1.64s/it]"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "class Config:\n",
    "    SR = 32000\n",
    "    N_MFCC = 13\n",
    "    N_FEATURES = 64\n",
    "    ROOT_FOLDER = './'\n",
    "    N_CLASSES = 2\n",
    "    BATCH_SIZE = 64\n",
    "    N_EPOCHS = 5\n",
    "    LR = 1e-4\n",
    "    SEED = 42\n",
    "    DROPOUT_RATE = 0.7\n",
    "    HIDDEN_DIMS = [128, 256, 128]\n",
    "    INITIALIZATION = 'xavier'\n",
    "    ACTIVATION_FUNCTION = 'relu'\n",
    "\n",
    "CONFIG = Config()\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CONFIG.SEED)\n",
    "\n",
    "# 데이터 로드 및 전처리\n",
    "df = pd.read_csv(os.path.join(CONFIG.ROOT_FOLDER, 'train.csv'))\n",
    "\n",
    "def label_encoder(column):\n",
    "    le = LabelEncoder().fit(column)\n",
    "    print(column.name, le.classes_)\n",
    "    return le.transform(column)\n",
    "\n",
    "df['class'] = label_encoder(df['label'])\n",
    "\n",
    "# 음성 특징 추출 함수 (시간 스프레딩 및 주파수 스프레딩 추가)\n",
    "def extract_features(file_path):\n",
    "    y, sr = librosa.load(file_path, sr=CONFIG.SR)\n",
    "    \n",
    "    # 시간 스프레딩 (Time Stretching)\n",
    "    stretched_y = librosa.effects.time_stretch(y, rate=0.8)\n",
    "    \n",
    "    mfcc = librosa.feature.mfcc(y=stretched_y, sr=sr, n_mfcc=CONFIG.N_MFCC)\n",
    "    chroma = librosa.feature.chroma_stft(y=stretched_y, sr=sr)\n",
    "    mel = librosa.feature.melspectrogram(y=stretched_y, sr=sr)\n",
    "    contrast = librosa.feature.spectral_contrast(y=stretched_y, sr=sr)\n",
    "    tonnetz = librosa.feature.tonnetz(y=librosa.effects.harmonic(stretched_y), sr=sr)\n",
    "    \n",
    "    # 주파수 스프레딩 (Frequency Masking)\n",
    "    masker = np.random.uniform(low=0.0, high=1.0, size=chroma.shape) < 0.2\n",
    "    chroma[masker] = 0.0\n",
    "    \n",
    "    features = np.concatenate((mfcc, chroma, mel, contrast, tonnetz), axis=0)\n",
    "    return features\n",
    "\n",
    "def get_features_and_labels(df, train_mode=True):\n",
    "    features = []\n",
    "    labels = []\n",
    "    for _, row in tqdm(df.iterrows()):\n",
    "        file_path = os.path.join(CONFIG.ROOT_FOLDER, row['path'][2:])  # './train/' 또는 './test/' 제거\n",
    "        if os.path.isfile(file_path):\n",
    "            features.append(extract_features(file_path))\n",
    "            if train_mode:\n",
    "                labels.append(row['class'])\n",
    "    if train_mode:\n",
    "        return np.array(features), np.array(labels)\n",
    "    return np.array(features)\n",
    "\n",
    "# Train 데이터에서 특징 추출\n",
    "X, y = get_features_and_labels(df, True)\n",
    "\n",
    "# 데이터 불균형 해결\n",
    "smote = SMOTE(random_state=CONFIG.SEED)\n",
    "X_resampled, y_resampled = smote.fit_resample(X.reshape(len(X), -1), y)\n",
    "X_resampled = X_resampled.reshape(len(X_resampled), CONFIG.N_FEATURES, -1)\n",
    "y_resampled = torch.tensor(y_resampled).long()  # 정수형으로 변환\n",
    "y_resampled = torch.nn.functional.one_hot(y_resampled, num_classes=CONFIG.N_CLASSES).float()\n",
    "\n",
    "# 데이터 분할\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=CONFIG.SEED)\n",
    "\n",
    "# PyTorch Dataset 클래스 정의\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.features[index], self.labels[index]\n",
    "\n",
    "train_dataset = CustomDataset(X_train, y_train)\n",
    "val_dataset = CustomDataset(X_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# CNN+RNN 모델 정의 (CNN 레이어 개선)\n",
    "class CNNRNN(nn.Module):\n",
    "    def __init__(self, input_dim=CONFIG.N_FEATURES, hidden_dims=CONFIG.HIDDEN_DIMS, output_dim=CONFIG.N_CLASSES):\n",
    "        super(CNNRNN, self).__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),  # 필터 수를 64로 증가\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(CONFIG.DROPOUT_RATE),\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),  # 필터 수를 128로 증가\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(CONFIG.DROPOUT_RATE)\n",
    "        )\n",
    "        \n",
    "        self.rnn = nn.LSTM(input_size=128*8, hidden_size=hidden_dims[0], num_layers=3, batch_first=True, dropout=CONFIG.DROPOUT_RATE, bidirectional=True)  # LSTM 레이어를 3개로 증가\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dims[0]*2, hidden_dims[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(CONFIG.DROPOUT_RATE),\n",
    "            nn.Linear(hidden_dims[1], hidden_dims[2]),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(CONFIG.DROPOUT_RATE),\n",
    "            nn.Linear(hidden_dims[2], output_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = x.unsqueeze(1)  # (batch_size, 1, n_features, time_steps)\n",
    "        x = self.cnn(x)\n",
    "        x = x.view(batch_size, 128*8, -1).permute(0, 2, 1)  # (batch_size, time_steps, 128*8)\n",
    "        x, _ = self.rnn(x)\n",
    "        x = x[:, -1, :]  # 마지막 타임 스텝의 출력\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# 모델 학습 함수 정의\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def train(model, optimizer, train_loader, val_loader, device):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss().to(device)\n",
    "    \n",
    "    best_val_score = 0\n",
    "    best_model = None\n",
    "    \n",
    "    for epoch in range(1, CONFIG.N_EPOCHS+1):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        for features, labels in tqdm(iter(train_loader)):\n",
    "            features = features.float().to(device)\n",
    "            labels = labels.float().to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model(features)\n",
    "            loss = criterion(output, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss.append(loss.item())\n",
    "                    \n",
    "        _val_loss, _val_score = validation(model, criterion, val_loader, device)\n",
    "        _train_loss = np.mean(train_loss)\n",
    "        print(f'Epoch [{epoch}], Train Loss : [{_train_loss:.5f}] Val Loss : [{_val_loss:.5f}] Val AUC : [{_val_score:.5f}]')\n",
    "            \n",
    "        if best_val_score < _val_score:\n",
    "            best_val_score = _val_score\n",
    "            best_model = model\n",
    "    \n",
    "    return best_model\n",
    "\n",
    "def multiLabel_AUC(y_true, y_scores):\n",
    "    auc_scores = []\n",
    "    for i in range(y_true.shape[1]):\n",
    "        auc = roc_auc_score(y_true[:, i], y_scores[:, i])\n",
    "        auc_scores.append(auc)\n",
    "    mean_auc_score = np.mean(auc_scores)\n",
    "    return mean_auc_score\n",
    "    \n",
    "def validation(model, criterion, val_loader, device):\n",
    "    model.eval()\n",
    "    val_loss, all_labels, all_probs = [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for features, labels in tqdm(iter(val_loader)):\n",
    "            features = features.float().to(device)\n",
    "            labels = labels.float().to(device)\n",
    "            \n",
    "            probs = model(features)\n",
    "            \n",
    "            loss = criterion(probs, labels)\n",
    "\n",
    "            val_loss.append(loss.item())\n",
    "\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "            all_probs.append(probs.cpu().numpy())\n",
    "        \n",
    "        _val_loss = np.mean(val_loss)\n",
    "\n",
    "        all_labels = np.concatenate(all_labels, axis=0)\n",
    "        all_probs = np.concatenate(all_probs, axis=0)\n",
    "        \n",
    "        auc_score = multiLabel_AUC(all_labels, all_probs)\n",
    "    \n",
    "    return _val_loss, auc_score\n",
    "\n",
    "model = CNNRNN()\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=CONFIG.LR)\n",
    "\n",
    "infer_model = train(model, optimizer, train_loader, val_loader, device)\n",
    "\n",
    "# Unlabeled 데이터에 대해 pseudo-labeling\n",
    "unlabeled_path = os.path.join(CONFIG.ROOT_FOLDER, 'unlabeled_data')\n",
    "unlabeled_files = [os.path.join(unlabeled_path, f) for f in os.listdir(unlabeled_path) if f.endswith('.ogg')]\n",
    "\n",
    "unlabeled_features = get_features_and_labels(unlabeled_files, train_mode=False)\n",
    "unlabeled_features = np.array(unlabeled_features)\n",
    "unlabeled_dataset = CustomDataset(unlabeled_features, torch.zeros((len(unlabeled_features), CONFIG.N_CLASSES)))\n",
    "unlabeled_loader = DataLoader(unlabeled_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=False)\n",
    "\n",
    "def pseudo_labeling(model, loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    pseudo_labels = []\n",
    "    with torch.no_grad():\n",
    "        for features, _ in tqdm(iter(loader)):\n",
    "            features = features.float().to(device)\n",
    "            probs = model(features)\n",
    "            pseudo_labels.append(probs.cpu().detach().numpy())\n",
    "    return np.concatenate(pseudo_labels, axis=0)\n",
    "\n",
    "pseudo_labels = pseudo_labeling(infer_model, unlabeled_loader, device)\n",
    "pseudo_labels = (pseudo_labels > 0.9).astype(int)  # 임계값 0.9를 사용하여 pseudo-label 생성\n",
    "\n",
    "# Pseudo-labeled 데이터를 학습 데이터에 추가\n",
    "pseudo_labeled_dataset = CustomDataset(unlabeled_features, torch.tensor(pseudo_labels).float())\n",
    "train_dataset_combined = torch.utils.data.ConcatDataset([train_dataset, pseudo_labeled_dataset])\n",
    "train_loader_combined = DataLoader(train_dataset_combined, batch_size=CONFIG.BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# 모델을 pseudo-labeled 데이터를 포함하여 재학습\n",
    "infer_model = train(model, optimizer, train_loader_combined, val_loader, device)\n",
    "\n",
    "# 테스트 데이터 예측\n",
    "test_df = pd.read_csv(os.path.join(CONFIG.ROOT_FOLDER, 'test.csv'))\n",
    "test_features = get_features_and_labels(test_df, train_mode=False)\n",
    "test_features = np.array(test_features)\n",
    "test_dataset = CustomDataset(test_features, torch.zeros((len(test_features), CONFIG.N_CLASSES)))\n",
    "test_loader = DataLoader(test_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=False)\n",
    "\n",
    "def inference(model, test_loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for features, _ in tqdm(iter(test_loader)):\n",
    "            features = features.float().to(device)\n",
    "            probs = model(features)\n",
    "            probs  = probs.cpu().detach().numpy()\n",
    "            predictions += probs.tolist()\n",
    "    return predictions\n",
    "\n",
    "preds = inference(infer_model, test_loader, device)\n",
    "\n",
    "submit = pd.read_csv(os.path.join(CONFIG.ROOT_FOLDER, 'sample_submission.csv'))\n",
    "submit.iloc[:, 1:] = preds\n",
    "submit.head()\n",
    "\n",
    "submit.to_csv('./pseudo_label_submit12.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
